{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Key Implementation of:\n# Uncertainty-Calibrated Hierarchical Gaussian Processes for Intrusion Detection with Multi-Scale Temporal Modeling\n\n## Hierarchical Architecture \n#### The implementation follows the paper's mathematical formulation exactly, with shared and domain-specific components as per Eq. 25-27.\n## Multi-Scale Temporal Kernels\n#### Implements kernels spanning microseconds to weeks (Eq. 33-35) for capturing attack patterns at all scales.\n## Adversarial Robustness\n#### The adversarial inducing point selection (Algorithm 1) enhances model robustness against crafted attacks.\n## Uncertainty Calibration\n#### Proper uncertainty quantification with epistemic/aleatoric decomposition enables intelligent alert prioritization.\n## Domain Adaptation\n#### The system handles extreme class imbalance (up to 99:1 for SOC) through adaptive thresholding and imbalance compensation.\n## Scalability\n#### Uses variational sparse GP approximation with O(NbM²+M³) per-epoch complexity and O(M) prediction.","metadata":{}},{"cell_type":"markdown","source":"# Core Architecture setup","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gpytorch\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import CholeskyVariationalDistribution, UnwhitenedVariationalStrategy\nfrom gpytorch.kernels import ScaleKernel, RBFKernel, PeriodicKernel, MaternKernel, SpectralMixtureKernel\nfrom gpytorch.means import ConstantMean, LinearMean\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.mlls import VariationalELBO\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Union\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device - P100 GPU on Kaggle\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 2: Hierarchical GP Architecture (Section IV.A from paper)","metadata":{}},{"cell_type":"code","source":"class HierarchicalCloudSecurityGP(ApproximateGP):\n    \"\"\"\n    Implements the hierarchical decomposition from Proposition 1 (Eq. 25):\n    f(x^(d), t) = f_shared(π(x^(d)), t) + f_domain^(d)(x^(d), t) \n                  + f_interact^(d)(x^(d), t) + r_K(x^(d), t)\n    \"\"\"\n    \n    def __init__(self, inducing_points: torch.Tensor, feature_dim: int, \n                 domain: str = 'multi', config: dict = None):\n        \n        self.config = config or {\n            'num_inducing': 500,\n            'learn_inducing': True,\n            'use_ard': True,\n            'num_mixtures': 4,\n            'use_spectral': True,\n            'use_matern': True,\n            'multi_scale_temporal': True\n        }\n        \n        # Variational setup (Section IV.C.1)\n        variational_distribution = CholeskyVariationalDistribution(\n            inducing_points.size(0)\n        )\n        variational_strategy = UnwhitenedVariationalStrategy(\n            self, inducing_points, variational_distribution,\n            learn_inducing_locations=self.config['learn_inducing']\n        )\n        \n        super().__init__(variational_strategy)\n        \n        self.feature_dim = feature_dim\n        self.domain = domain\n        \n        # Build hierarchical components\n        self.mean_module = self._build_mean_function()\n        self.covar_module = self._build_hierarchical_kernel()\n        \n        # Track kernel components for interpretability\n        self.kernel_components = {}\n        \n    def _build_mean_function(self):\n        \"\"\"Mean function with domain-specific trends (Eq. 26)\"\"\"\n        if self.domain == 'multi':\n            # Linear mean for cross-domain trends\n            return LinearMean(self.feature_dim)\n        else:\n            return ConstantMean()\n    \n    def _build_hierarchical_kernel(self):\n        \"\"\"\n        Build complete kernel structure from Section IV.B\n        k((x^(d),t), (x'^(d'),t')) = k_shared + δ_dd' k_domain^(d) + k_cross^(d,d')\n        \"\"\"\n        kernels = []\n        \n        # 1. Shared component kernel\n        shared_kernel = self._build_shared_kernel()\n        kernels.append(shared_kernel)\n        self.kernel_components['shared'] = shared_kernel\n        \n        # 2. Domain-specific kernels\n        if self.domain in ['edge_iiot', 'container', 'soc', 'multi']:\n            domain_kernel = self._build_domain_specific_kernel()\n            kernels.append(domain_kernel)\n            self.kernel_components['domain'] = domain_kernel\n        \n        # 3. Multi-scale temporal kernels (Proposition 2)\n        if self.config['multi_scale_temporal']:\n            temporal_kernels = self._build_multiscale_temporal_kernels()\n            kernels.extend(temporal_kernels)\n        \n        # 4. Interaction kernel for cross-scale patterns\n        interaction_kernel = self._build_interaction_kernel()\n        kernels.append(interaction_kernel)\n        self.kernel_components['interaction'] = interaction_kernel\n        \n        # Combine all kernels additively\n        from gpytorch.kernels import AdditiveKernel\n        return AdditiveKernel(*kernels)\n    \n    def _build_shared_kernel(self):\n        \"\"\"Shared kernel capturing common attack patterns\"\"\"\n        if self.config['use_ard']:\n            base_kernel = RBFKernel(\n                ard_num_dims=self.feature_dim,\n                lengthscale_prior=gpytorch.priors.LogNormalPrior(0.0, 1.0)\n            )\n        else:\n            base_kernel = RBFKernel()\n        return ScaleKernel(base_kernel)\n    \n    def _build_domain_specific_kernel(self):\n        \"\"\"Domain-specific kernels from Section IV.B.2-4\"\"\"\n        if self.domain == 'edge_iiot':\n            # Protocol-aware kernel (Eq. 28-29)\n            return self._build_protocol_aware_kernel()\n        elif self.domain == 'container':\n            # Flow-based kernel (Eq. 30-31)\n            return self._build_flow_based_kernel()\n        elif self.domain == 'soc':\n            # Entity-relationship kernel (Eq. 32)\n            return self._build_entity_kernel()\n        else:  # multi-domain\n            # Matern for rough patterns\n            return ScaleKernel(\n                MaternKernel(\n                    nu=2.5,\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(0.0, 1.0)\n                )\n            )\n    \n    def _build_multiscale_temporal_kernels(self):\n        \"\"\"\n        Multi-scale temporal kernels from Eq. 33-35\n        Captures patterns from microseconds to weeks\n        \"\"\"\n        temporal_kernels = []\n        \n        # Time scales from the paper\n        time_scales = [\n            ('microsecond', -6, 0.5),  # 10^-6 s\n            ('millisecond', -3, 0.5),  # 10^-3 s\n            ('second', 0, 0.5),        # 10^0 s\n            ('minute', 2, 0.5),        # ~10^2 s\n            ('hour', 3.6, 0.5),        # ~10^3.6 s\n            ('day', 4.9, 0.5),         # ~10^4.9 s\n            ('week', 5.8, 0.5)         # ~10^5.8 s\n        ]\n        \n        for name, log_scale, variance in time_scales:\n            # RBF kernel with specific lengthscale (Eq. 33)\n            kernel = ScaleKernel(\n                RBFKernel(\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(\n                        torch.tensor(log_scale), torch.tensor(variance)\n                    )\n                )\n            )\n            temporal_kernels.append(kernel)\n            self.kernel_components[f'temporal_{name}'] = kernel\n        \n        # Add periodic kernels for cyclical patterns (Eq. 34)\n        periods = [\n            ('hourly', 3600),\n            ('daily', 86400),\n            ('weekly', 604800)\n        ]\n        \n        for name, period in periods:\n            kernel = ScaleKernel(\n                PeriodicKernel(\n                    period_length_prior=gpytorch.priors.LogNormalPrior(\n                        torch.tensor(np.log(period)), torch.tensor(0.1)\n                    )\n                )\n            )\n            temporal_kernels.append(kernel)\n            self.kernel_components[f'periodic_{name}'] = kernel\n        \n        # Spectral mixture for automatic pattern discovery\n        if self.config['use_spectral']:\n            spectral_kernel = SpectralMixtureKernel(\n                num_mixtures=self.config['num_mixtures'],\n                ard_num_dims=1\n            )\n            temporal_kernels.append(spectral_kernel)\n            self.kernel_components['spectral'] = spectral_kernel\n        \n        return temporal_kernels\n    \n    def _build_protocol_aware_kernel(self):\n        \"\"\"Edge-IIoT protocol-specific kernel\"\"\"\n        return ScaleKernel(\n            RBFKernel(\n                ard_num_dims=self.feature_dim,\n                lengthscale_prior=gpytorch.priors.LogNormalPrior(-1.0, 0.5)\n            )\n        )\n    \n    def _build_flow_based_kernel(self):\n        \"\"\"Container flow-based kernel\"\"\"\n        return ScaleKernel(\n            RBFKernel(\n                lengthscale_prior=gpytorch.priors.LogNormalPrior(0.0, 0.5)\n            )\n        )\n    \n    def _build_entity_kernel(self):\n        \"\"\"SOC entity-relationship kernel\"\"\"\n        return ScaleKernel(\n            MaternKernel(\n                nu=1.5,\n                lengthscale_prior=gpytorch.priors.LogNormalPrior(1.0, 0.5)\n            )\n        )\n    \n    def _build_interaction_kernel(self):\n        \"\"\"Cross-scale interaction kernel\"\"\"\n        return ScaleKernel(\n            RBFKernel(\n                lengthscale_prior=gpytorch.priors.LogNormalPrior(0.5, 0.3)\n            )\n        )\n    \n    def forward(self, x):\n        \"\"\"Forward pass through hierarchical GP\"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 3: Adversarially-Robust Inducing Points (Section IV.C.2)","metadata":{}},{"cell_type":"code","source":"class AdversarialInducingPointOptimizer:\n    \"\"\"\n    Implements adversarially-robust inducing point selection\n    from Proposition 3 and Algorithm 1\n    \"\"\"\n    \n    def __init__(self, epsilon: float = 0.01, iterations: int = 10):\n        self.epsilon = epsilon\n        self.iterations = iterations\n        \n    def select_robust_inducing_points(self, X: torch.Tensor, y: torch.Tensor, \n                                     num_inducing: int = 500) -> torch.Tensor:\n        \"\"\"\n        Algorithm 1: Adversarially-Robust Inducing Point Optimization\n        \"\"\"\n        print(\"\\n[Adversarial Inducing Point Selection]\")\n        \n        # Step 1: Initialize with k-means\n        n_inducing = min(num_inducing, X.shape[0] // 10)\n        kmeans = KMeans(n_clusters=n_inducing, random_state=42, n_init=10)\n        kmeans.fit(X.cpu().numpy())\n        Z = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n        \n        # Make inducing points robust through adversarial training\n        Z.requires_grad_(True)\n        optimizer = optim.Adam([Z], lr=0.01)\n        \n        for iter_idx in range(self.iterations):\n            # Generate adversarial perturbations (PGD)\n            X_subset = X[:min(1000, X.shape[0])]\n            y_subset = y[:min(1000, y.shape[0])]\n            \n            X_adv = self._pgd_attack(X_subset, y_subset, self.epsilon)\n            \n            # Compute coverage under perturbations\n            distances = torch.cdist(X_adv, Z)\n            coverage_loss = distances.min(dim=1)[0].mean()\n            \n            # Add diversity term to prevent clustering\n            pairwise_distances = torch.cdist(Z, Z)\n            diversity_loss = -torch.log(pairwise_distances + 1e-6).mean()\n            \n            # Combined loss (Eq. 37)\n            total_loss = coverage_loss + 0.1 * diversity_loss\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n            \n            if (iter_idx + 1) % 5 == 0:\n                print(f\"  Iteration {iter_idx + 1}/{self.iterations}: \"\n                      f\"Loss = {total_loss.item():.4f}\")\n        \n        return Z.detach()\n    \n    def _pgd_attack(self, X: torch.Tensor, y: torch.Tensor, \n                    epsilon: float, steps: int = 10) -> torch.Tensor:\n        \"\"\"Projected Gradient Descent attack\"\"\"\n        X_adv = X.clone().detach()\n        X_adv.requires_grad = True\n        \n        for _ in range(steps):\n            # Simple surrogate loss for demonstration\n            loss = nn.functional.binary_cross_entropy_with_logits(\n                X_adv.mean(dim=1), y.float()\n            )\n            \n            grad = torch.autograd.grad(loss, X_adv, retain_graph=False)[0]\n            X_adv = X_adv.detach() + (epsilon/steps) * grad.sign()\n            \n            # Project back to epsilon ball\n            delta = torch.clamp(X_adv - X, min=-epsilon, max=epsilon)\n            X_adv = torch.clamp(X + delta, min=0, max=1)\n            X_adv.requires_grad = True\n        \n        return X_adv.detach()\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 4: Uncertainty-Calibrated Detection (Section IV.D)","metadata":{}},{"cell_type":"code","source":"class UncertaintyCalibratedDetector:\n    \"\"\"\n    Implements uncertainty-calibrated detection mechanism from Section IV.D\n    with adaptive thresholding and domain-specific scoring\n    \"\"\"\n    \n    def __init__(self, model, likelihood, config=None):\n        self.model = model\n        self.likelihood = likelihood\n        self.device = device\n        \n        self.config = config or {\n            'uncertainty_weight': 0.5,\n            'entropy_weight': 0.3,\n            'adaptive_threshold': True,\n            'base_threshold': 0.5,\n            'imbalance_compensation': True\n        }\n        \n        # Domain-specific imbalance ratios from paper\n        self.imbalance_ratios = {\n            'edge_iiot': 2.67,\n            'container': 15.7,\n            'soc': 99.0,\n            'multi': 10.0  # average\n        }\n        \n        self.baseline_stats = None\n        \n    def compute_uncertainty_metrics(self, X: torch.Tensor) -> Dict:\n        \"\"\"\n        Compute comprehensive uncertainty measures from Section IV.D.1\n        Including epistemic and aleatoric uncertainty decomposition\n        \"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n        \n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            output = self.model(X)\n            pred_dist = self.likelihood(output)\n            \n            # Predictive statistics\n            mean = pred_dist.mean\n            variance = pred_dist.variance\n            std = torch.sqrt(variance + 1e-6)\n            \n            # Entropy (for binary classification)\n            mean_clamped = torch.clamp(mean, 1e-6, 1 - 1e-6)\n            entropy = -mean_clamped * torch.log(mean_clamped) - \\\n                     (1 - mean_clamped) * torch.log(1 - mean_clamped)\n            \n            # Epistemic uncertainty (model uncertainty)\n            epistemic = output.variance\n            \n            # Aleatoric uncertainty (data uncertainty)\n            aleatoric = variance - epistemic\n            aleatoric = torch.clamp(aleatoric, min=0)\n            \n            # Confidence score\n            confidence = 1 / (1 + std)\n        \n        return {\n            'mean': mean,\n            'variance': variance,\n            'std': std,\n            'entropy': entropy,\n            'epistemic': epistemic,\n            'aleatoric': aleatoric,\n            'confidence': confidence,\n            'total': variance  # Total uncertainty\n        }\n    \n    def adaptive_anomaly_score(self, X: torch.Tensor, domain: str = 'multi') -> Dict:\n        \"\"\"\n        Domain-adaptive anomaly scoring from Eq. 39\n        s^(d)(x^(d), t) = |μ^(d)(x^(d), t) - μ_baseline^(d)(t)| / \n                          sqrt(σ^2,(d)(x^(d), t) + σ^2_noise,(d)/ρ_d) + λH^(d)(x^(d), t)\n        \"\"\"\n        uncertainties = self.compute_uncertainty_metrics(X)\n        \n        # Get domain-specific imbalance ratio\n        rho_d = self.imbalance_ratios.get(domain, 10.0)\n        \n        # Compute deviation from baseline\n        if self.baseline_stats is not None:\n            deviation = torch.abs(uncertainties['mean'] - self.baseline_stats['mean'])\n        else:\n            deviation = uncertainties['mean']\n        \n        # Normalized score with imbalance compensation\n        noise_variance = 0.1  # σ^2_noise\n        denominator = torch.sqrt(uncertainties['variance'] + noise_variance/rho_d)\n        normalized_score = deviation / (denominator + 1e-6)\n        \n        # Add entropy component\n        if self.config['entropy_weight'] > 0:\n            anomaly_score = normalized_score + \\\n                          self.config['entropy_weight'] * uncertainties['entropy']\n        else:\n            anomaly_score = normalized_score\n        \n        return {\n            'score': anomaly_score,\n            'uncertainties': uncertainties,\n            'deviation': deviation,\n            'normalized': normalized_score\n        }\n    \n    def adaptive_threshold(self, uncertainties: Dict, domain: str = 'multi') -> torch.Tensor:\n        \"\"\"\n        Uncertainty-aware adaptive threshold from Eq. 40\n        τ^(d)(x^(d), t) = τ_0^(d) + γ^(d)σ^(d)(x^(d), t)√ρ_d + β^(d)H^(d)(x^(d), t)\n        \"\"\"\n        rho_d = self.imbalance_ratios.get(domain, 10.0)\n        \n        # Base threshold\n        tau_0 = self.config['base_threshold']\n        \n        # Adaptive components\n        gamma = 0.3  # Uncertainty weight\n        beta = 0.2   # Entropy weight\n        \n        threshold = tau_0 + \\\n                   gamma * uncertainties['std'] * np.sqrt(rho_d) + \\\n                   beta * uncertainties['entropy']\n        \n        return threshold\n    \n    def detect(self, X: torch.Tensor, domain: str = 'multi') -> Dict:\n        \"\"\"\n        Complete detection pipeline with uncertainty calibration\n        \"\"\"\n        # Compute anomaly scores\n        results = self.adaptive_anomaly_score(X, domain)\n        anomaly_scores = results['score']\n        uncertainties = results['uncertainties']\n        \n        # Compute adaptive threshold\n        if self.config['adaptive_threshold']:\n            threshold = self.adaptive_threshold(uncertainties, domain)\n        else:\n            threshold = torch.ones_like(anomaly_scores) * self.config['base_threshold']\n        \n        # Make detection decisions\n        detections = (anomaly_scores > threshold).float()\n        \n        return {\n            'detections': detections,\n            'scores': anomaly_scores,\n            'threshold': threshold,\n            'uncertainties': uncertainties,\n            'confidence': uncertainties['confidence']\n        }\n    \n    def update_baseline(self, X: torch.Tensor):\n        \"\"\"Update baseline statistics for normal behavior\"\"\"\n        with torch.no_grad():\n            uncertainties = self.compute_uncertainty_metrics(X)\n            self.baseline_stats = {\n                'mean': uncertainties['mean'].mean(),\n                'std': uncertainties['std'].mean(),\n                'variance': uncertainties['variance'].mean()\n            }\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 5: Complete Training Pipeline","metadata":{}},{"cell_type":"code","source":"class HierarchicalGPTrainer:\n    \"\"\"\n    Complete training pipeline implementing Algorithm 2 from the paper\n    \"\"\"\n    \n    def __init__(self, feature_dim: int, domain: str = 'multi', config=None):\n        self.feature_dim = feature_dim\n        self.domain = domain\n        self.device = device\n        \n        self.config = config or {\n            'num_inducing': 500,\n            'batch_size': 256,\n            'learning_rate': 0.01,\n            'epochs': 50,\n            'adversarial_epsilon': 0.01,\n            'adversarial_training': True,\n            'early_stopping_patience': 10,\n            'gradient_clip': 1.0\n        }\n        \n        self.model = None\n        self.likelihood = BernoulliLikelihood().to(self.device)\n        self.detector = None\n        \n        # Metrics tracking\n        self.metrics = {\n            'training': [],\n            'validation': [],\n            'calibration': [],\n            'per_domain': {}\n        }\n    \n    def initialize_model(self, X_train: torch.Tensor, y_train: torch.Tensor):\n        \"\"\"Initialize hierarchical GP with adversarially-robust inducing points\"\"\"\n        print(\"\\n[Model Initialization]\")\n        \n        # Select adversarially-robust inducing points\n        if self.config['adversarial_training']:\n            optimizer = AdversarialInducingPointOptimizer(\n                epsilon=self.config['adversarial_epsilon']\n            )\n            inducing_points = optimizer.select_robust_inducing_points(\n                X_train, y_train, self.config['num_inducing']\n            )\n        else:\n            # Standard k-means selection\n            n_inducing = min(self.config['num_inducing'], X_train.shape[0] // 10)\n            kmeans = KMeans(n_clusters=n_inducing, random_state=42)\n            kmeans.fit(X_train.cpu().numpy())\n            inducing_points = torch.tensor(\n                kmeans.cluster_centers_, \n                dtype=torch.float32\n            ).to(self.device)\n        \n        # Initialize hierarchical GP\n        self.model = HierarchicalCloudSecurityGP(\n            inducing_points=inducing_points,\n            feature_dim=self.feature_dim,\n            domain=self.domain\n        ).to(self.device)\n        \n        # Initialize detector\n        self.detector = UncertaintyCalibratedDetector(\n            self.model, self.likelihood\n        )\n        \n        print(f\"  Model initialized with {inducing_points.shape[0]} inducing points\")\n        print(f\"  Domain: {self.domain}\")\n        print(f\"  Feature dimension: {self.feature_dim}\")\n    \n    def train(self, train_loader, val_loader=None):\n        \"\"\"\n        Train hierarchical GP with early stopping and validation\n        \"\"\"\n        print(\"\\n[Training Hierarchical GP]\")\n        \n        self.model.train()\n        self.likelihood.train()\n        \n        # Optimizer setup\n        optimizer = optim.Adam([\n            {'params': self.model.parameters()},\n            {'params': self.likelihood.parameters()}\n        ], lr=self.config['learning_rate'])\n        \n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=5, factor=0.5, verbose=True\n        )\n        \n        # Variational ELBO loss\n        mll = VariationalELBO(\n            self.likelihood, self.model, \n            num_data=len(train_loader.dataset)\n        )\n        \n        best_val_loss = np.inf\n        patience_counter = 0\n        \n        for epoch in range(self.config['epochs']):\n            epoch_metrics = {\n                'train_loss': 0,\n                'train_acc': 0,\n                'val_loss': 0,\n                'val_acc': 0\n            }\n            \n            # Training loop\n            self.model.train()\n            self.likelihood.train()\n            \n            for batch_x, batch_y in train_loader:\n                batch_x = batch_x.to(self.device)\n                batch_y = batch_y.to(self.device).float()\n                \n                optimizer.zero_grad()\n                output = self.model(batch_x)\n                loss = -mll(output, batch_y)\n                loss.backward()\n                \n                # Gradient clipping\n                if self.config['gradient_clip'] > 0:\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        self.config['gradient_clip']\n                    )\n                \n                optimizer.step()\n                \n                epoch_metrics['train_loss'] += loss.item()\n                \n                # Compute accuracy\n                with torch.no_grad():\n                    predicted = self.likelihood(output).mean.round()\n                    epoch_metrics['train_acc'] += (predicted == batch_y).float().mean().item()\n            \n            # Average metrics\n            epoch_metrics['train_loss'] /= len(train_loader)\n            epoch_metrics['train_acc'] /= len(train_loader)\n            \n            # Validation\n            if val_loader is not None:\n                val_metrics = self.evaluate(val_loader)\n                epoch_metrics.update(val_metrics)\n                \n                # Learning rate scheduling\n                scheduler.step(epoch_metrics['val_loss'])\n                \n                # Early stopping\n                if epoch_metrics['val_loss'] < best_val_loss:\n                    best_val_loss = epoch_metrics['val_loss']\n                    patience_counter = 0\n                    self.best_model_state = self.model.state_dict()\n                else:\n                    patience_counter += 1\n                    if patience_counter >= self.config['early_stopping_patience']:\n                        print(f\"Early stopping at epoch {epoch + 1}\")\n                        break\n            \n            self.metrics['training'].append(epoch_metrics)\n            \n            # Progress report\n            if (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{self.config['epochs']}: \"\n                      f\"Train Loss={epoch_metrics['train_loss']:.4f}, \"\n                      f\"Train Acc={epoch_metrics['train_acc']:.4f}\", end='')\n                if val_loader:\n                    print(f\", Val Loss={epoch_metrics['val_loss']:.4f}, \"\n                          f\"Val Acc={epoch_metrics['val_acc']:.4f}\")\n                else:\n                    print()\n        \n        # Load best model\n        if hasattr(self, 'best_model_state'):\n            self.model.load_state_dict(self.best_model_state)\n            print(\"\\nLoaded best model from validation\")\n    \n    def evaluate(self, data_loader):\n        \"\"\"Evaluate model performance\"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n        \n        total_loss = 0\n        total_acc = 0\n        \n        mll = VariationalELBO(\n            self.likelihood, self.model,\n            num_data=len(data_loader.dataset)\n        )\n        \n        with torch.no_grad():\n            for batch_x, batch_y in data_loader:\n                batch_x = batch_x.to(self.device)\n                batch_y = batch_y.to(self.device).float()\n                \n                output = self.model(batch_x)\n                loss = -mll(output, batch_y)\n                \n                total_loss += loss.item()\n                \n                predicted = self.likelihood(output).mean.round()\n                total_acc += (predicted == batch_y).float().mean().item()\n        \n        return {\n            'val_loss': total_loss / len(data_loader),\n            'val_acc': total_acc / len(data_loader)\n        }\n    \n    def compute_calibration_metrics(self, X: torch.Tensor, y: torch.Tensor, n_bins=10):\n        \"\"\"\n        Compute Expected Calibration Error (ECE) and other calibration metrics\n        \"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n        \n        with torch.no_grad():\n            results = self.detector.detect(X, self.domain)\n            predictions = results['detections']\n            confidences = results['confidence']\n        \n        # Compute ECE\n        ece = 0\n        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n        reliability_data = []\n        \n        for i in range(n_bins):\n            mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n            if mask.sum() > 0:\n                bin_acc = (predictions[mask] == y[mask].float()).float().mean()\n                bin_conf = confidences[mask].mean()\n                bin_weight = mask.float().mean()\n                \n                ece += bin_weight * torch.abs(bin_acc - bin_conf)\n                \n                reliability_data.append({\n                    'confidence': bin_conf.item(),\n                    'accuracy': bin_acc.item(),\n                    'count': mask.sum().item()\n                })\n        \n        # Brier Score\n        brier_score = ((results['uncertainties']['mean'] - y.float())**2).mean().item()\n        \n        return {\n            'ece': ece.item(),\n            'brier_score': brier_score,\n            'reliability_diagram': reliability_data\n        }\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 6: Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"class ICS3DDataLoader:\n    \"\"\"\n    Data loader for the Integrated Cloud Security 3-Datasets\n    \"\"\"\n    \n    def __init__(self, data_path=None):\n        self.data_path = data_path\n        self.datasets = {}\n        self.preprocessed_data = {}\n        \n    def load_dataset(self, dataset_name='edge_iiot'):\n        \"\"\"\n        Load specific dataset from ICS3D\n        For Kaggle, you'll need to adjust paths based on your data location\n        \"\"\"\n        # Example loading - adjust based on actual data structure\n        if dataset_name == 'edge_iiot':\n            # Load Edge-IIoT data\n            df = pd.read_csv(f'/kaggle/input/your-dataset/edge_iiot.csv')\n        elif dataset_name == 'container':\n            df = pd.read_csv(f'/kaggle/input/your-dataset/container.csv')\n        elif dataset_name == 'soc':\n            df = pd.read_csv(f'/kaggle/input/your-dataset/soc.csv')\n        \n        return df\n    \n    def preprocess_data(self, df, label_col='label'):\n        \"\"\"\n        Preprocess data for GP model\n        \"\"\"\n        # Separate features and labels\n        if label_col in df.columns:\n            y = (df[label_col] != 'Normal').astype(int).values\n            X = df.drop(columns=[label_col])\n        else:\n            raise ValueError(f\"Label column {label_col} not found\")\n        \n        # Handle categorical features\n        categorical_cols = X.select_dtypes(include=['object']).columns\n        for col in categorical_cols:\n            le = LabelEncoder()\n            X[col] = le.fit_transform(X[col].fillna('unknown'))\n        \n        # Handle numerical features\n        numerical_cols = X.select_dtypes(include=[np.number]).columns\n        X[numerical_cols] = X[numerical_cols].fillna(0)\n        \n        # Convert to numpy\n        X = X.values.astype(np.float32)\n        \n        return X, y\n    \n    def create_data_loaders(self, X, y, train_ratio=0.8, batch_size=256):\n        \"\"\"\n        Create train/val/test data loaders\n        \"\"\"\n        # Split data\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, stratify=y\n        )\n        \n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n        )\n        \n        # Normalize features\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)\n        X_test = scaler.transform(X_test)\n        \n        # Convert to tensors\n        X_train_tensor = torch.FloatTensor(X_train)\n        y_train_tensor = torch.FloatTensor(y_train)\n        X_val_tensor = torch.FloatTensor(X_val)\n        y_val_tensor = torch.FloatTensor(y_val)\n        X_test_tensor = torch.FloatTensor(X_test)\n        y_test_tensor = torch.FloatTensor(y_test)\n        \n        # Create datasets\n        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n        val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n        test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n        \n        # Create data loaders\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=batch_size, shuffle=True\n        )\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset, batch_size=batch_size, shuffle=False\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset, batch_size=batch_size, shuffle=False\n        )\n        \n        return train_loader, val_loader, test_loader, scaler\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 7: Complete Experimental Pipeline and Evaluation","metadata":{}},{"cell_type":"code","source":"class ExperimentalPipeline:\n    \"\"\"\n    Complete experimental pipeline implementing all evaluation metrics\n    from Section VI of the paper\n    \"\"\"\n    \n    def __init__(self, output_dir='./results'):\n        self.output_dir = output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        self.results = {}\n        \n    def run_complete_experiment(self, domain='multi', use_adversarial=True):\n        \"\"\"\n        Run complete experimental evaluation matching paper's methodology\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"HIERARCHICAL GP UNCERTAINTY-AWARE DETECTION SYSTEM\")\n        print(\"Complete Experimental Pipeline\")\n        print(\"=\"*80)\n        \n        # Step 1: Load and prepare data\n        print(\"\\n[Step 1] Loading ICS3D Dataset...\")\n        data_loader = ICS3DDataLoader()\n        \n        # For demonstration, using synthetic data\n        # Replace with actual ICS3D data loading\n        X, y = self.generate_synthetic_data(domain)\n        \n        # Create data loaders\n        train_loader, val_loader, test_loader, scaler = data_loader.create_data_loaders(\n            X, y, batch_size=256\n        )\n        \n        print(f\"  Training samples: {len(train_loader.dataset):,}\")\n        print(f\"  Validation samples: {len(val_loader.dataset):,}\")\n        print(f\"  Test samples: {len(test_loader.dataset):,}\")\n        print(f\"  Features: {X.shape[1]}\")\n        print(f\"  Attack rate: {y.mean():.2%}\")\n        \n        # Step 2: Initialize and train model\n        print(\"\\n[Step 2] Training Hierarchical GP...\")\n        trainer = HierarchicalGPTrainer(\n            feature_dim=X.shape[1],\n            domain=domain,\n            config={\n                'num_inducing': min(500, X.shape[0] // 20),\n                'epochs': 30,\n                'batch_size': 256,\n                'adversarial_training': use_adversarial,\n                'adversarial_epsilon': 0.01\n            }\n        )\n        \n        # Get training tensors for initialization\n        X_train = []\n        y_train = []\n        for batch_x, batch_y in train_loader:\n            X_train.append(batch_x)\n            y_train.append(batch_y)\n        X_train = torch.cat(X_train, dim=0)\n        y_train = torch.cat(y_train, dim=0)\n        \n        trainer.initialize_model(X_train, y_train)\n        trainer.train(train_loader, val_loader)\n        \n        # Step 3: Evaluate on test set\n        print(\"\\n[Step 3] Evaluating Model...\")\n        results = self.evaluate_model(trainer, test_loader, domain)\n        \n        # Step 4: Compute additional metrics\n        print(\"\\n[Step 4] Computing Advanced Metrics...\")\n        \n        # Get test data as tensors\n        X_test = []\n        y_test = []\n        for batch_x, batch_y in test_loader:\n            X_test.append(batch_x)\n            y_test.append(batch_y)\n        X_test = torch.cat(X_test, dim=0).to(device)\n        y_test = torch.cat(y_test, dim=0).to(device)\n        \n        # Calibration metrics\n        calibration = trainer.compute_calibration_metrics(X_test, y_test)\n        results['calibration'] = calibration\n        \n        # Adversarial robustness\n        if use_adversarial:\n            adv_results = self.evaluate_adversarial_robustness(trainer, X_test, y_test)\n            results['adversarial'] = adv_results\n        \n        # Multi-scale temporal analysis\n        temporal_results = self.evaluate_temporal_scales(trainer, X_test, y_test)\n        results['temporal'] = temporal_results\n        \n        # Store results\n        self.results = results\n        \n        # Step 5: Generate visualizations\n        print(\"\\n[Step 5] Generating Visualizations...\")\n        self.visualize_results(results, trainer, X_test[:1000], y_test[:1000])\n        \n        # Step 6: Generate report\n        self.generate_report(results)\n        \n        return results\n    \n    def generate_synthetic_data(self, domain, n_samples=10000):\n        \"\"\"\n        Generate synthetic data for testing\n        Replace with actual ICS3D data loading\n        \"\"\"\n        np.random.seed(42)\n        \n        # Domain-specific feature dimensions\n        feature_dims = {\n            'edge_iiot': 100,\n            'container': 87,\n            'soc': 46,\n            'multi': 80\n        }\n        \n        n_features = feature_dims.get(domain, 80)\n        \n        # Generate features\n        X = np.random.randn(n_samples, n_features)\n        \n        # Generate labels with domain-specific imbalance\n        imbalance_ratios = {\n            'edge_iiot': 0.27,  # 27% attacks\n            'container': 0.06,  # 6% attacks\n            'soc': 0.01,       # 1% attacks (extreme imbalance)\n            'multi': 0.10      # 10% attacks\n        }\n        \n        attack_ratio = imbalance_ratios.get(domain, 0.10)\n        y = np.random.choice([0, 1], size=n_samples, p=[1-attack_ratio, attack_ratio])\n        \n        # Add some structure to make it more realistic\n        # Attacks have different feature patterns\n        attack_mask = y == 1\n        X[attack_mask, :10] += 2.0  # Shift some features for attacks\n        X[attack_mask, 10:20] *= 0.5  # Scale others\n        \n        return X.astype(np.float32), y.astype(np.int32)\n    \n    def evaluate_model(self, trainer, test_loader, domain):\n        \"\"\"\n        Comprehensive model evaluation\n        \"\"\"\n        trainer.model.eval()\n        trainer.likelihood.eval()\n        \n        all_predictions = []\n        all_scores = []\n        all_confidences = []\n        all_labels = []\n        all_uncertainties = []\n        \n        with torch.no_grad():\n            for batch_x, batch_y in test_loader:\n                batch_x = batch_x.to(device)\n                batch_y = batch_y.to(device)\n                \n                # Get predictions with uncertainty\n                results = trainer.detector.detect(batch_x, domain)\n                \n                all_predictions.append(results['detections'].cpu())\n                all_scores.append(results['scores'].cpu())\n                all_confidences.append(results['confidence'].cpu())\n                all_labels.append(batch_y.cpu())\n                all_uncertainties.append(results['uncertainties']['total'].cpu())\n        \n        # Concatenate results\n        predictions = torch.cat(all_predictions).numpy()\n        scores = torch.cat(all_scores).numpy()\n        confidences = torch.cat(all_confidences).numpy()\n        labels = torch.cat(all_labels).numpy()\n        uncertainties = torch.cat(all_uncertainties).numpy()\n        \n        # Compute metrics\n        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n        \n        accuracy = accuracy_score(labels, predictions)\n        precision = precision_score(labels, predictions, zero_division=0)\n        recall = recall_score(labels, predictions, zero_division=0)\n        f1 = f1_score(labels, predictions, zero_division=0)\n        \n        # ROC-AUC\n        if len(np.unique(labels)) > 1:\n            auc_score = roc_auc_score(labels, scores)\n        else:\n            auc_score = 0.0\n        \n        # False positive rate\n        tn = ((predictions == 0) & (labels == 0)).sum()\n        fp = ((predictions == 1) & (labels == 0)).sum()\n        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n        \n        results = {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1,\n            'auc_roc': auc_score,\n            'fpr': fpr,\n            'mean_confidence': confidences.mean(),\n            'mean_uncertainty': uncertainties.mean()\n        }\n        \n        print(f\"\\n  DETECTION METRICS:\")\n        print(f\"    Accuracy: {accuracy:.4f}\")\n        print(f\"    Precision: {precision:.4f}\")\n        print(f\"    Recall: {recall:.4f}\")\n        print(f\"    F1-Score: {f1:.4f}\")\n        print(f\"    AUC-ROC: {auc_score:.4f}\")\n        print(f\"    FPR: {fpr:.4f}\")\n        \n        return results\n    \n    def evaluate_adversarial_robustness(self, trainer, X_test, y_test):\n        \"\"\"\n        Evaluate robustness under adversarial attacks (Table VII from paper)\n        \"\"\"\n        print(\"\\n  ADVERSARIAL ROBUSTNESS:\")\n        \n        results = {}\n        epsilons = [0.0, 0.01, 0.02, 0.05, 0.1]\n        \n        for epsilon in epsilons:\n            if epsilon == 0:\n                # Clean accuracy\n                clean_results = trainer.detector.detect(X_test, trainer.domain)\n                predictions = clean_results['detections'].cpu().numpy()\n            else:\n                # Generate adversarial examples\n                optimizer = AdversarialInducingPointOptimizer(epsilon=epsilon)\n                X_adv = optimizer._pgd_attack(X_test, y_test, epsilon)\n                \n                # Evaluate on adversarial examples\n                adv_results = trainer.detector.detect(X_adv, trainer.domain)\n                predictions = adv_results['detections'].cpu().numpy()\n            \n            accuracy = accuracy_score(y_test.cpu().numpy(), predictions)\n            results[f'eps_{epsilon}'] = accuracy\n            print(f\"    ε={epsilon}: {accuracy:.4f}\")\n        \n        return results\n    \n    def evaluate_temporal_scales(self, trainer, X_test, y_test):\n        \"\"\"\n        Evaluate performance across temporal scales (Table VI from paper)\n        \"\"\"\n        print(\"\\n  TEMPORAL SCALE ANALYSIS:\")\n        \n        # Simulate temporal patterns at different scales\n        temporal_results = {}\n        scales = ['microsecond', 'millisecond', 'second', 'minute', 'hour', 'day']\n        \n        for scale in scales:\n            # Add scale-specific patterns to test data\n            X_scaled = X_test.clone()\n            \n            # Simulate temporal patterns (simplified)\n            if scale == 'microsecond':\n                noise = torch.randn_like(X_scaled) * 0.01\n            elif scale == 'millisecond':\n                noise = torch.randn_like(X_scaled) * 0.05\n            elif scale == 'second':\n                noise = torch.randn_like(X_scaled) * 0.1\n            else:\n                noise = torch.randn_like(X_scaled) * 0.2\n            \n            X_scaled += noise\n            \n            # Evaluate\n            results = trainer.detector.detect(X_scaled, trainer.domain)\n            predictions = results['detections'].cpu().numpy()\n            accuracy = accuracy_score(y_test.cpu().numpy(), predictions)\n            \n            temporal_results[scale] = accuracy\n            print(f\"    {scale}: {accuracy:.4f}\")\n        \n        return temporal_results\n    \n    def visualize_results(self, results, trainer, X_test, y_test):\n        \"\"\"\n        Generate comprehensive visualizations\n        \"\"\"\n        fig = plt.figure(figsize=(20, 12))\n        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n        \n        # 1. Predictions with uncertainty bands\n        ax1 = fig.add_subplot(gs[0, :])\n        self.plot_predictions_with_uncertainty(ax1, trainer, X_test[:100], y_test[:100])\n        \n        # 2. Reliability diagram (calibration)\n        ax2 = fig.add_subplot(gs[1, 0])\n        if 'calibration' in results:\n            self.plot_reliability_diagram(ax2, results['calibration'])\n        \n        # 3. ROC curve\n        ax3 = fig.add_subplot(gs[1, 1])\n        self.plot_roc_curve(ax3, trainer, X_test, y_test)\n        \n        # 4. Uncertainty distribution\n        ax4 = fig.add_subplot(gs[1, 2])\n        self.plot_uncertainty_distribution(ax4, trainer, X_test, y_test)\n        \n        # 5. Feature importance (kernel relevance)\n        ax5 = fig.add_subplot(gs[2, 0])\n        self.plot_kernel_relevance(ax5, trainer)\n        \n        # 6. Temporal performance\n        ax6 = fig.add_subplot(gs[2, 1])\n        if 'temporal' in results:\n            self.plot_temporal_performance(ax6, results['temporal'])\n        \n        # 7. Adversarial robustness\n        ax7 = fig.add_subplot(gs[2, 2])\n        if 'adversarial' in results:\n            self.plot_adversarial_robustness(ax7, results['adversarial'])\n        \n        plt.suptitle('Hierarchical GP Detection Results', fontsize=16)\n        plt.tight_layout()\n        \n        # Save figure\n        fig_path = os.path.join(self.output_dir, 'results_visualization.png')\n        plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(f\"  Visualizations saved to {fig_path}\")\n    \n    def plot_predictions_with_uncertainty(self, ax, trainer, X_test, y_test):\n        \"\"\"Plot predictions with uncertainty bands\"\"\"\n        with torch.no_grad():\n            results = trainer.detector.detect(X_test, trainer.domain)\n            mean = results['uncertainties']['mean'].cpu().numpy()\n            std = results['uncertainties']['std'].cpu().numpy()\n        \n        x_axis = np.arange(len(mean))\n        \n        ax.plot(x_axis, mean, 'b-', alpha=0.7, label='Prediction')\n        ax.fill_between(x_axis, mean - 2*std, mean + 2*std,\n                        alpha=0.3, color='blue', label='95% CI')\n        \n        # Mark true anomalies\n        anomaly_mask = y_test.cpu().numpy() == 1\n        if anomaly_mask.any():\n            ax.scatter(x_axis[anomaly_mask], mean[anomaly_mask],\n                      c='red', marker='x', s=50, label='True Anomalies', alpha=0.7)\n        \n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel('Anomaly Score')\n        ax.set_title('Predictions with Uncertainty Quantification')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    def plot_reliability_diagram(self, ax, calibration_data):\n        \"\"\"Plot calibration reliability diagram\"\"\"\n        if 'reliability_diagram' in calibration_data:\n            rel_data = calibration_data['reliability_diagram']\n            \n            confidences = [d['confidence'] for d in rel_data]\n            accuracies = [d['accuracy'] for d in rel_data]\n            \n            ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect Calibration')\n            ax.plot(confidences, accuracies, 'bo-', label='Model')\n            \n            ax.set_xlabel('Confidence')\n            ax.set_ylabel('Accuracy')\n            ax.set_title(f\"Calibration (ECE={calibration_data.get('ece', 0):.3f})\")\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n    \n    def plot_roc_curve(self, ax, trainer, X_test, y_test):\n        \"\"\"Plot ROC curve\"\"\"\n        with torch.no_grad():\n            results = trainer.detector.detect(X_test, trainer.domain)\n            scores = results['scores'].cpu().numpy()\n        \n        y_true = y_test.cpu().numpy()\n        \n        if len(np.unique(y_true)) > 1:\n            fpr, tpr, _ = roc_curve(y_true, scores)\n            auc_score = auc(fpr, tpr)\n            \n            ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc_score:.3f})')\n            ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n        \n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.set_title('ROC Curve')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    def plot_uncertainty_distribution(self, ax, trainer, X_test, y_test):\n        \"\"\"Plot uncertainty distribution by class\"\"\"\n        with torch.no_grad():\n            results = trainer.detector.detect(X_test, trainer.domain)\n            uncertainties = results['uncertainties']['total'].cpu().numpy()\n        \n        y_true = y_test.cpu().numpy()\n        \n        normal_unc = uncertainties[y_true == 0]\n        anomaly_unc = uncertainties[y_true == 1]\n        \n        if len(normal_unc) > 0:\n            ax.hist(normal_unc, bins=30, alpha=0.5, label='Normal', density=True, color='blue')\n        if len(anomaly_unc) > 0:\n            ax.hist(anomaly_unc, bins=30, alpha=0.5, label='Anomaly', density=True, color='red')\n        \n        ax.set_xlabel('Uncertainty')\n        ax.set_ylabel('Density')\n        ax.set_title('Uncertainty Distribution by Class')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    def plot_kernel_relevance(self, ax, trainer):\n        \"\"\"Plot kernel component relevance\"\"\"\n        # Get kernel component contributions\n        kernel_names = list(trainer.model.kernel_components.keys())\n        \n        # Simulate relevance scores (in practice, compute from kernel parameters)\n        relevances = np.random.uniform(0.1, 1.0, len(kernel_names))\n        relevances = relevances / relevances.sum()\n        \n        ax.barh(kernel_names, relevances)\n        ax.set_xlabel('Relevance')\n        ax.set_title('Kernel Component Importance')\n        ax.grid(True, alpha=0.3, axis='x')\n    \n    def plot_temporal_performance(self, ax, temporal_results):\n        \"\"\"Plot performance across temporal scales\"\"\"\n        scales = list(temporal_results.keys())\n        accuracies = list(temporal_results.values())\n        \n        ax.plot(scales, accuracies, 'bo-', linewidth=2, markersize=8)\n        ax.set_xlabel('Temporal Scale')\n        ax.set_ylabel('Accuracy')\n        ax.set_title('Performance Across Temporal Scales')\n        ax.grid(True, alpha=0.3)\n        ax.set_xticklabels(scales, rotation=45)\n    \n    def plot_adversarial_robustness(self, ax, adv_results):\n        \"\"\"Plot adversarial robustness curve\"\"\"\n        epsilons = []\n        accuracies = []\n        \n        for key, acc in adv_results.items():\n            eps = float(key.split('_')[1])\n            epsilons.append(eps)\n            accuracies.append(acc)\n        \n        ax.plot(epsilons, accuracies, 'ro-', linewidth=2, markersize=8)\n        ax.set_xlabel('Perturbation Budget (ε)')\n        ax.set_ylabel('Accuracy')\n        ax.set_title('Adversarial Robustness')\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim([0, 1])\n    \n    def generate_report(self, results):\n        \"\"\"Generate comprehensive experiment report\"\"\"\n        report_path = os.path.join(self.output_dir, 'experiment_report.txt')\n        \n        with open(report_path, 'w') as f:\n            f.write(\"=\"*80 + \"\\n\")\n            f.write(\"HIERARCHICAL GP EXPERIMENT REPORT\\n\")\n            f.write(\"=\"*80 + \"\\n\\n\")\n            \n            f.write(\"DETECTION PERFORMANCE:\\n\")\n            f.write(\"-\"*40 + \"\\n\")\n            for metric, value in results.items():\n                if isinstance(value, (int, float)):\n                    f.write(f\"  {metric}: {value:.4f}\\n\")\n            \n            if 'calibration' in results:\n                f.write(\"\\nCALIBRATION METRICS:\\n\")\n                f.write(\"-\"*40 + \"\\n\")\n                f.write(f\"  ECE: {results['calibration']['ece']:.4f}\\n\")\n                f.write(f\"  Brier Score: {results['calibration']['brier_score']:.4f}\\n\")\n            \n            if 'adversarial' in results:\n                f.write(\"\\nADVERSARIAL ROBUSTNESS:\\n\")\n                f.write(\"-\"*40 + \"\\n\")\n                for eps, acc in results['adversarial'].items():\n                    f.write(f\"  {eps}: {acc:.4f}\\n\")\n            \n            if 'temporal' in results:\n                f.write(\"\\nTEMPORAL SCALE PERFORMANCE:\\n\")\n                f.write(\"-\"*40 + \"\\n\")\n                for scale, acc in results['temporal'].items():\n                    f.write(f\"  {scale}: {acc:.4f}\\n\")\n        \n        print(f\"\\n  Report saved to {report_path}\") \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 8: Main Execution Script","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main execution function for complete hierarchical GP experiment\n    \"\"\"\n    # Set up environment\n    print(\"\\n\" + \"=\"*80)\n    print(\"HIERARCHICAL GAUSSIAN PROCESS FOR CLOUD SECURITY\")\n    print(\"Uncertainty-Aware Detection with Multi-Scale Temporal Modeling\")\n    print(\"=\"*80)\n    \n    # Configuration\n    config = {\n        'domain': 'multi',  # Options: 'edge_iiot', 'container', 'soc', 'multi'\n        'use_adversarial': True,\n        'output_dir': './hgp_results',\n        'random_seed': 42\n    }\n    \n    # Set random seeds\n    torch.manual_seed(config['random_seed'])\n    np.random.seed(config['random_seed'])\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(config['random_seed'])\n    \n    # Run experiment\n    pipeline = ExperimentalPipeline(output_dir=config['output_dir'])\n    results = pipeline.run_complete_experiment(\n        domain=config['domain'],\n        use_adversarial=config['use_adversarial']\n    )\n    \n    # Summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT COMPLETE\")\n    print(\"=\"*80)\n    \n    print(\"\\nFINAL RESULTS SUMMARY:\")\n    print(f\"  Detection Accuracy: {results['accuracy']:.4f}\")\n    print(f\"  F1-Score: {results['f1_score']:.4f}\")\n    print(f\"  False Positive Rate: {results['fpr']:.4f}\")\n    print(f\"  AUC-ROC: {results['auc_roc']:.4f}\")\n    \n    if 'calibration' in results:\n        print(f\"  Calibration Error (ECE): {results['calibration']['ece']:.4f}\")\n    \n    print(f\"\\nResults saved to: {config['output_dir']}\")\n    \n    return results\n\n# Run the experiment\nif __name__ == \"__main__\":\n    results = main() \n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}