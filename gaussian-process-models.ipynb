{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gaussian Process Uncertainty-Aware Detection with Multi-Scale Temporal Modeling for Cloud-Based Network Intrusion Detection Systems","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# First modeling\n## GAUSSIAN PROCESS UNCERTAINTY-AWARE DETECTION SYSTEM\n## Complete Implementation for Integrated Cloud Security 3-Datasets (ICS3D)\n## Building on Q1-Level Paper with Real Cloud Security Data\n\n","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gpytorch\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import CholeskyVariationalDistribution, UnwhitenedVariationalStrategy\nfrom gpytorch.kernels import ScaleKernel, RBFKernel, PeriodicKernel, MaternKernel, AdditiveKernel, ProductKernel\nfrom gpytorch.means import ConstantMean, LinearMean\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.mlls import VariationalELBO\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, calibration_curve\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import entropy, ks_2samp, norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\nimport kagglehub\nfrom pathlib import Path\nimport json\nimport pickle\nfrom typing import List, Tuple, Dict, Optional, Union\nwarnings.filterwarnings('ignore')\n\n# Set device and seeds\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# ============================================================================\n# SECTION 1: ICS3D DATASET LOADER AND PREPROCESSOR\n# ============================================================================\n\nclass ICS3DDatasetLoader:\n    \"\"\"\n    Comprehensive loader for Integrated Cloud Security 3-Datasets\n    Handles multiple cloud providers' data formats\n    \"\"\"\n    \n    def __init__(self, base_path: str = None):\n        \"\"\"Initialize dataset loader\"\"\"\n        if base_path is None:\n            # Download from Kaggle\n            self.base_path = kagglehub.dataset_download(\n                \"rogernickanaedevha/integrated-cloud-security-3datasets-ics3d\"\n            )\n            print(f\"Dataset downloaded to: {self.base_path}\")\n        else:\n            self.base_path = Path(base_path)\n        \n        # Dataset configurations\n        self.datasets = {\n            'containers': {\n                'file': 'Containers_Dataset.csv',\n                'label_col': 'label',\n                'time_col': 'timestamp',\n                'provider': 'docker/kubernetes'\n            },\n            'edge_iot_dnn': {\n                'file': 'DNN-EdgeIIoT-dataset.csv',\n                'label_col': 'Attack_type',\n                'time_col': 'frame.time',\n                'provider': 'edge_computing'\n            },\n            'edge_iot_ml': {\n                'file': 'ML-EdgeIIoT-dataset.csv',\n                'label_col': 'Attack_type',\n                'time_col': 'frame.time',\n                'provider': 'edge_computing'\n            },\n            'microsoft_train': {\n                'file': 'Microsoft_GUIDE_Train.csv',\n                'label_col': 'Class',\n                'time_col': 'Time',\n                'provider': 'azure'\n            },\n            'microsoft_test': {\n                'file': 'Microsoft_GUIDE_Test.csv',\n                'label_col': 'Class',\n                'time_col': 'Time',\n                'provider': 'azure'\n            }\n        }\n        \n        self.loaded_data = {}\n        self.preprocessed_data = {}\n        \n    def load_all_datasets(self) -> Dict[str, pd.DataFrame]:\n        \"\"\"Load all available datasets\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"LOADING ICS3D DATASETS\")\n        print(\"=\"*60)\n        \n        for name, config in self.datasets.items():\n            file_path = Path(self.base_path) / config['file']\n            \n            if file_path.exists():\n                print(f\"\\nLoading {name}...\")\n                df = pd.read_csv(file_path, low_memory=False)\n                \n                # Basic info\n                print(f\"  Shape: {df.shape}\")\n                print(f\"  Provider: {config['provider']}\")\n                \n                # Check for label column\n                if config['label_col'] in df.columns:\n                    attack_rate = (df[config['label_col']] != 'Normal').mean()\n                    print(f\"  Attack rate: {attack_rate:.2%}\")\n                \n                self.loaded_data[name] = df\n            else:\n                print(f\"  Warning: {config['file']} not found\")\n        \n        return self.loaded_data\n    \n    def preprocess_dataset(self, dataset_name: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n        \"\"\"\n        Preprocess a specific dataset for GP model\n        \n        Returns:\n            X: Feature matrix\n            y: Labels (0=normal, 1=attack)\n            metadata: Additional information (timestamps, attack types, etc.)\n        \"\"\"\n        if dataset_name not in self.loaded_data:\n            raise ValueError(f\"Dataset {dataset_name} not loaded\")\n        \n        df = self.loaded_data[dataset_name].copy()\n        config = self.datasets[dataset_name]\n        \n        print(f\"\\nPreprocessing {dataset_name}...\")\n        \n        # Handle label encoding\n        if config['label_col'] in df.columns:\n            # Convert labels to binary (normal=0, attack=1)\n            if df[config['label_col']].dtype == 'object':\n                y = (df[config['label_col']] != 'Normal').astype(int).values\n                \n                # Store attack types\n                attack_types = df[config['label_col']].unique()\n                print(f\"  Found {len(attack_types)} unique attack types\")\n            else:\n                y = df[config['label_col']].values\n        else:\n            # If no labels, assume all normal\n            y = np.zeros(len(df))\n        \n        # Extract metadata\n        metadata = pd.DataFrame()\n        \n        # Add timestamp if available\n        if config['time_col'] in df.columns:\n            metadata['timestamp'] = pd.to_datetime(df[config['time_col']], errors='coerce')\n        else:\n            # Create synthetic timestamps\n            metadata['timestamp'] = pd.date_range(\n                start='2024-01-01', \n                periods=len(df), \n                freq='S'\n            )\n        \n        # Add attack type information\n        if config['label_col'] in df.columns:\n            metadata['attack_type'] = df[config['label_col']]\n        \n        # Feature engineering based on dataset type\n        if 'containers' in dataset_name:\n            X = self._process_container_features(df)\n        elif 'edge_iot' in dataset_name:\n            X = self._process_edge_iot_features(df)\n        elif 'microsoft' in dataset_name:\n            X = self._process_microsoft_features(df)\n        else:\n            # Generic processing\n            X = self._process_generic_features(df, config)\n        \n        print(f\"  Final shape: X={X.shape}, y={y.shape}\")\n        print(f\"  Attack ratio: {y.mean():.2%}\")\n        \n        self.preprocessed_data[dataset_name] = {\n            'X': X,\n            'y': y,\n            'metadata': metadata\n        }\n        \n        return X, y, metadata\n    \n    def _process_container_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Process container/Kubernetes specific features\"\"\"\n        feature_cols = []\n        \n        # Network features\n        network_cols = [col for col in df.columns if any(x in col.lower() for x in \n                       ['packet', 'byte', 'flow', 'port', 'protocol', 'ip'])]\n        feature_cols.extend(network_cols)\n        \n        # Container metrics\n        container_cols = [col for col in df.columns if any(x in col.lower() for x in \n                         ['cpu', 'memory', 'disk', 'container', 'pod', 'node'])]\n        feature_cols.extend(container_cols)\n        \n        # Remove duplicates and non-numeric\n        feature_cols = list(set(feature_cols))\n        \n        # Extract features\n        X = []\n        for col in feature_cols:\n            if col in df.columns:\n                if df[col].dtype in ['float64', 'int64']:\n                    X.append(df[col].fillna(0).values)\n                elif df[col].dtype == 'object':\n                    # Encode categorical\n                    le = LabelEncoder()\n                    X.append(le.fit_transform(df[col].fillna('unknown')))\n        \n        if X:\n            X = np.column_stack(X)\n        else:\n            # Fallback to all numeric columns\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n            X = df[numeric_cols].fillna(0).values\n        \n        return X\n    \n    def _process_edge_iot_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Process Edge/IoT specific features\"\"\"\n        # IoT-specific features\n        iot_features = []\n        \n        # Protocol features\n        protocol_cols = [col for col in df.columns if any(x in col.lower() for x in \n                        ['tcp', 'udp', 'http', 'mqtt', 'coap', 'dns'])]\n        \n        # Flow statistics\n        flow_cols = [col for col in df.columns if any(x in col.lower() for x in \n                    ['duration', 'packet', 'byte', 'rate', 'iat', 'flag'])]\n        \n        # Device features\n        device_cols = [col for col in df.columns if any(x in col.lower() for x in \n                      ['device', 'sensor', 'actuator', 'gateway'])]\n        \n        all_cols = list(set(protocol_cols + flow_cols + device_cols))\n        \n        # Extract numeric features\n        X = []\n        for col in all_cols:\n            if col in df.columns:\n                if df[col].dtype in ['float64', 'int64']:\n                    X.append(df[col].fillna(0).values)\n        \n        if X:\n            X = np.column_stack(X)\n        else:\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n            X = df[numeric_cols].fillna(0).values\n        \n        return X\n    \n    def _process_microsoft_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Process Microsoft Azure specific features\"\"\"\n        # Azure-specific features\n        azure_features = []\n        \n        # Cloud service features\n        service_cols = [col for col in df.columns if any(x in col.lower() for x in \n                       ['vm', 'storage', 'network', 'compute', 'sql', 'cosmos'])]\n        \n        # Security features\n        security_cols = [col for col in df.columns if any(x in col.lower() for x in \n                        ['firewall', 'nsg', 'ddos', 'waf', 'threat'])]\n        \n        # Performance metrics\n        perf_cols = [col for col in df.columns if any(x in col.lower() for x in \n                    ['latency', 'throughput', 'iops', 'bandwidth', 'cpu', 'memory'])]\n        \n        all_cols = list(set(service_cols + security_cols + perf_cols))\n        \n        X = []\n        for col in all_cols:\n            if col in df.columns and df[col].dtype in ['float64', 'int64']:\n                X.append(df[col].fillna(0).values)\n        \n        if X:\n            X = np.column_stack(X)\n        else:\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n            X = df[numeric_cols].fillna(0).values\n        \n        return X\n    \n    def _process_generic_features(self, df: pd.DataFrame, config: dict) -> np.ndarray:\n        \"\"\"Generic feature processing\"\"\"\n        # Get all numeric columns\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        \n        # Remove label column if present\n        if config['label_col'] in numeric_cols:\n            numeric_cols.remove(config['label_col'])\n        \n        # Extract features\n        if numeric_cols:\n            X = df[numeric_cols].fillna(0).values\n        else:\n            # If no numeric columns, create dummy features\n            X = np.random.randn(len(df), 10)\n        \n        return X\n    \n    def create_unified_dataset(self) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n        \"\"\"\n        Create a unified dataset combining all sources\n        \n        Returns:\n            X: Combined feature matrix\n            y: Combined labels\n            metadata: Combined metadata with source information\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"CREATING UNIFIED ICS3D DATASET\")\n        print(\"=\"*60)\n        \n        X_list = []\n        y_list = []\n        metadata_list = []\n        \n        for name in self.preprocessed_data:\n            data = self.preprocessed_data[name]\n            \n            # Add source information to metadata\n            data['metadata']['source'] = name\n            \n            X_list.append(data['X'])\n            y_list.append(data['y'])\n            metadata_list.append(data['metadata'])\n        \n        # Combine all data\n        if X_list:\n            # Pad features to same dimensionality\n            max_features = max(X.shape[1] for X in X_list)\n            \n            X_padded = []\n            for X in X_list:\n                if X.shape[1] < max_features:\n                    padding = np.zeros((X.shape[0], max_features - X.shape[1]))\n                    X = np.hstack([X, padding])\n                X_padded.append(X)\n            \n            X_unified = np.vstack(X_padded)\n            y_unified = np.hstack(y_list)\n            metadata_unified = pd.concat(metadata_list, ignore_index=True)\n            \n            print(f\"\\nUnified dataset shape: {X_unified.shape}\")\n            print(f\"Total samples: {len(X_unified):,}\")\n            print(f\"Features: {X_unified.shape[1]}\")\n            print(f\"Attack rate: {y_unified.mean():.2%}\")\n            \n            # Dataset composition\n            print(\"\\nDataset composition:\")\n            for source in metadata_unified['source'].unique():\n                count = (metadata_unified['source'] == source).sum()\n                pct = count / len(metadata_unified) * 100\n                print(f\"  {source}: {count:,} samples ({pct:.1f}%)\")\n            \n            return X_unified, y_unified, metadata_unified\n        else:\n            raise ValueError(\"No data available for unification\")\n\n# ============================================================================\n# SECTION 2: ADVANCED MULTI-SCALE GP FOR CLOUD SECURITY\n# ============================================================================\n\nclass CloudSecurityGP(ApproximateGP):\n    \"\"\"\n    Specialized Gaussian Process for Cloud Security\n    Implements methodology from Q1 paper\n    \"\"\"\n    \n    def __init__(self, inducing_points: torch.Tensor, \n                 feature_dim: int,\n                 cloud_provider: str = 'multi',\n                 config: dict = None):\n        \"\"\"\n        Initialize Cloud Security GP\n        \n        Args:\n            inducing_points: Inducing points for sparse GP\n            feature_dim: Number of input features\n            cloud_provider: One of 'aws', 'azure', 'gcp', 'multi'\n            config: Model configuration\n        \"\"\"\n        self.config = config or {\n            'num_inducing': 500,\n            'learn_inducing': True,\n            'use_ard': True,\n            'num_mixtures': 4,\n            'use_spectral': True,\n            'use_matern': True,\n            'cloud_specific': True\n        }\n        \n        # Variational setup\n        variational_distribution = CholeskyVariationalDistribution(\n            inducing_points.size(0)\n        )\n        variational_strategy = UnwhitenedVariationalStrategy(\n            self, inducing_points, variational_distribution,\n            learn_inducing_locations=self.config['learn_inducing']\n        )\n        \n        super().__init__(variational_strategy)\n        \n        self.feature_dim = feature_dim\n        self.cloud_provider = cloud_provider\n        \n        # Build model components\n        self.mean_module = self._build_mean_function()\n        self.covar_module = self._build_cloud_kernel()\n        \n        # Track kernel components\n        self.kernel_components = {}\n        \n    def _build_mean_function(self):\n        \"\"\"Build mean function with cloud-specific trends\"\"\"\n        if self.cloud_provider == 'multi':\n            # Linear mean to capture provider-specific trends\n            return LinearMean(self.feature_dim)\n        else:\n            # Constant mean for single provider\n            return ConstantMean()\n    \n    def _build_cloud_kernel(self):\n        \"\"\"Build kernel structure for cloud security\"\"\"\n        kernels = []\n        \n        # 1. Spatial kernel for feature similarities\n        if self.config['use_ard']:\n            spatial_kernel = ScaleKernel(\n                RBFKernel(\n                    ard_num_dims=self.feature_dim,\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(0.0, 1.0)\n                )\n            )\n        else:\n            spatial_kernel = ScaleKernel(RBFKernel())\n        \n        kernels.append(spatial_kernel)\n        self.kernel_components['spatial'] = spatial_kernel\n        \n        # 2. Multi-scale temporal kernels (from paper)\n        time_scales = [\n            ('microsecond', -6, 0.5),  # Side-channel attacks\n            ('millisecond', -3, 0.5),  # Buffer overflows\n            ('second', 0, 0.5),        # SYN floods\n            ('minute', 2, 0.5),        # Port scanning\n            ('hour', 4, 0.5),          # Data exfiltration\n            ('day', 5, 0.5)            # APT campaigns\n        ]\n        \n        for name, loc, scale in time_scales:\n            kernel = ScaleKernel(\n                RBFKernel(\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(\n                        torch.tensor(loc), torch.tensor(scale)\n                    )\n                )\n            )\n            kernels.append(kernel)\n            self.kernel_components[f'temporal_{name}'] = kernel\n        \n        # 3. Periodic kernels for cyclical patterns\n        periods = [\n            ('hourly', 3600),\n            ('daily', 86400),\n            ('weekly', 604800)\n        ]\n        \n        for name, period in periods:\n            kernel = ScaleKernel(\n                PeriodicKernel(\n                    period_length_prior=gpytorch.priors.LogNormalPrior(\n                        torch.tensor(np.log(period)), torch.tensor(0.1)\n                    )\n                )\n            )\n            kernels.append(kernel)\n            self.kernel_components[f'periodic_{name}'] = kernel\n        \n        # 4. Cloud-specific kernels\n        if self.config['cloud_specific']:\n            # MatÃ©rn kernel for rough attack patterns\n            matern_kernel = ScaleKernel(\n                MaternKernel(\n                    nu=2.5,\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(0.0, 1.0)\n                )\n            )\n            kernels.append(matern_kernel)\n            self.kernel_components['matern'] = matern_kernel\n            \n            # Container/microservice kernel\n            container_kernel = ScaleKernel(\n                RBFKernel(\n                    lengthscale_prior=gpytorch.priors.LogNormalPrior(-2.0, 0.5)\n                )\n            )\n            kernels.append(container_kernel)\n            self.kernel_components['container'] = container_kernel\n        \n        # 5. Spectral mixture for complex patterns\n        if self.config['use_spectral']:\n            from gpytorch.kernels import SpectralMixtureKernel\n            spectral_kernel = SpectralMixtureKernel(\n                num_mixtures=self.config['num_mixtures'],\n                ard_num_dims=1\n            )\n            kernels.append(spectral_kernel)\n            self.kernel_components['spectral'] = spectral_kernel\n        \n        # Combine kernels\n        composite_kernel = AdditiveKernel(*kernels)\n        \n        return composite_kernel\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n    \n    def get_kernel_decomposition(self, x):\n        \"\"\"Decompose kernel contributions for interpretability\"\"\"\n        contributions = {}\n        \n        with torch.no_grad():\n            for name, kernel in self.kernel_components.items():\n                K = kernel(x).evaluate()\n                contributions[name] = {\n                    'matrix': K.cpu().numpy(),\n                    'trace': K.trace().item(),\n                    'mean': K.mean().item(),\n                    'std': K.std().item()\n                }\n        \n        return contributions\n\n# ============================================================================\n# SECTION 3: CLOUD-AWARE UNCERTAINTY DETECTION SYSTEM\n# ============================================================================\n\nclass CloudUncertaintyDetector:\n    \"\"\"\n    Production-ready uncertainty-aware detection for cloud environments\n    Implements full methodology from Q1 paper\n    \"\"\"\n    \n    def __init__(self, feature_dim: int, cloud_provider: str = 'multi', config: dict = None):\n        \"\"\"Initialize cloud security detector\"\"\"\n        self.feature_dim = feature_dim\n        self.cloud_provider = cloud_provider\n        self.device = device\n        \n        # Configuration from paper\n        self.config = config or {\n            'num_inducing': 500,\n            'batch_size': 256,\n            'learning_rate': 0.01,\n            'epochs': 50,\n            'uncertainty_weight': 0.5,\n            'entropy_weight': 0.3,\n            'adaptive_threshold': True,\n            'adversarial_training': True,\n            'epsilon': 0.1\n        }\n        \n        # Models\n        self.model = None\n        self.likelihood = BernoulliLikelihood().to(self.device)\n        \n        # Metrics tracking\n        self.metrics = {\n            'training': [],\n            'validation': [],\n            'calibration': [],\n            'temporal': [],\n            'adversarial': [],\n            'per_provider': {}\n        }\n        \n        # Baseline statistics\n        self.baseline_stats = None\n        \n        # Drift detection\n        self.drift_detector = CloudDriftDetector()\n        \n    def select_adversarial_inducing_points(self, X: torch.Tensor, y: torch.Tensor, \n                                          epsilon: float = 0.1, \n                                          iterations: int = 10) -> torch.Tensor:\n        \"\"\"\n        Select adversarially robust inducing points (from paper methodology)\n        \n        Args:\n            X: Training features\n            y: Training labels\n            epsilon: Perturbation budget\n            iterations: Number of adversarial iterations\n        \"\"\"\n        print(\"\\nSelecting adversarial inducing points...\")\n        \n        # Initialize with k-means\n        n_inducing = min(self.config['num_inducing'], X.shape[0] // 10)\n        kmeans = KMeans(n_clusters=n_inducing, random_state=42)\n        kmeans.fit(X.cpu().numpy())\n        inducing_points = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(self.device)\n        \n        if self.config['adversarial_training']:\n            # Make inducing points robust\n            inducing_points.requires_grad_(True)\n            optimizer = optim.Adam([inducing_points], lr=0.01)\n            \n            for iter_idx in range(iterations):\n                # Generate adversarial perturbations using PGD\n                X_adv = self._pgd_attack(X[:1000], y[:1000], epsilon, steps=5)\n                \n                # Compute coverage under perturbations\n                distances = torch.cdist(X_adv, inducing_points)\n                coverage_loss = distances.min(dim=1)[0].mean()\n                \n                # Add diversity term\n                pairwise_distances = torch.cdist(inducing_points, inducing_points)\n                diversity_loss = -torch.log(pairwise_distances + 1e-6).mean()\n                \n                total_loss = coverage_loss + 0.1 * diversity_loss\n                \n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n                \n                if (iter_idx + 1) % 5 == 0:\n                    print(f\"  Iteration {iter_idx + 1}: Loss = {total_loss.item():.4f}\")\n            \n            inducing_points = inducing_points.detach()\n        \n        return inducing_points\n    \n    def _pgd_attack(self, X: torch.Tensor, y: torch.Tensor, \n                    epsilon: float, steps: int = 10) -> torch.Tensor:\n        \"\"\"Projected Gradient Descent attack for adversarial training\"\"\"\n        X_adv = X.clone().detach()\n        X_adv.requires_grad = True\n        \n        for _ in range(steps):\n            # Simplified attack (would use actual model in practice)\n            loss = nn.functional.binary_cross_entropy_with_logits(\n                X_adv.mean(dim=1), y.float()\n            )\n            \n            grad = torch.autograd.grad(loss, X_adv)[0]\n            X_adv = X_adv.detach() + epsilon * grad.sign()\n            \n            # Project back to epsilon ball\n            delta = torch.clamp(X_adv - X, min=-epsilon, max=epsilon)\n            X_adv = X + delta\n            X_adv.requires_grad = True\n        \n        return X_adv.detach()\n    \n    def train_model(self, train_loader, val_loader=None, verbose=True):\n        \"\"\"Train GP model with validation and early stopping\"\"\"\n        self.model.train()\n        self.likelihood.train()\n        \n        optimizer = optim.Adam([\n            {'params': self.model.parameters()},\n            {'params': self.likelihood.parameters()}\n        ], lr=self.config['learning_rate'])\n        \n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=5, factor=0.5, verbose=verbose\n        )\n        \n        mll = VariationalELBO(self.likelihood, self.model, \n                             num_data=len(train_loader.dataset))\n        \n        best_val_loss = np.inf\n        patience_counter = 0\n        \n        for epoch in range(self.config['epochs']):\n            epoch_metrics = {\n                'train_loss': 0,\n                'train_acc': 0,\n                'val_loss': 0,\n                'val_acc': 0\n            }\n            \n            # Training loop\n            for batch_x, batch_y in train_loader:\n                batch_x = batch_x.to(self.device)\n                batch_y = batch_y.to(self.device).float()\n                \n                optimizer.zero_grad()\n                output = self.model(batch_x)\n                loss = -mll(output, batch_y)\n                loss.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                \n                optimizer.step()\n                \n                epoch_metrics['train_loss'] += loss.item()\n                \n                # Compute accuracy\n                with torch.no_grad():\n                    predicted = self.likelihood(output).mean.round()\n                    epoch_metrics['train_acc'] += (predicted == batch_y).float().mean().item()\n            \n            # Normalize metrics\n            epoch_metrics['train_loss'] /= len(train_loader)\n            epoch_metrics['train_acc'] /= len(train_loader)\n            \n            # Validation\n            if val_loader is not None:\n                self.model.eval()\n                self.likelihood.eval()\n                \n                with torch.no_grad():\n                    for batch_x, batch_y in val_loader:\n                        batch_x = batch_x.to(self.device)\n                        batch_y = batch_y.to(self.device).float()\n                        \n                        output = self.model(batch_x)\n                        loss = -mll(output, batch_y)\n                        \n                        epoch_metrics['val_loss'] += loss.item()\n                        \n                        predicted = self.likelihood(output).mean.round()\n                        epoch_metrics['val_acc'] += (predicted == batch_y).float().mean().item()\n                \n                epoch_metrics['val_loss'] /= len(val_loader)\n                epoch_metrics['val_acc'] /= len(val_loader)\n                \n                # Learning rate scheduling\n                scheduler.step(epoch_metrics['val_loss'])\n                \n                # Early stopping\n                if epoch_metrics['val_loss'] < best_val_loss:\n                    best_val_loss = epoch_metrics['val_loss']\n                    patience_counter = 0\n                    self.best_model_state = self.model.state_dict()\n                else:\n                    patience_counter += 1\n                    if patience_counter >= 10:\n                        print(f\"Early stopping at epoch {epoch + 1}\")\n                        break\n                \n                self.model.train()\n                self.likelihood.train()\n            \n            self.metrics['training'].append(epoch_metrics)\n            \n            if verbose and (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{self.config['epochs']}: \"\n                      f\"Train Loss={epoch_metrics['train_loss']:.4f}, \"\n                      f\"Train Acc={epoch_metrics['train_acc']:.4f}, \"\n                      f\"Val Loss={epoch_metrics['val_loss']:.4f}, \"\n                      f\"Val Acc={epoch_metrics['val_acc']:.4f}\")\n        \n        # Load best model\n        if hasattr(self, 'best_model_state'):\n            self.model.load_state_dict(self.best_model_state)\n    \n    def compute_uncertainty_metrics(self, X: torch.Tensor) -> Dict:\n        \"\"\"Compute comprehensive uncertainty measures (from paper)\"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n        \n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            output = self.model(X)\n            pred_dist = self.likelihood(output)\n            \n            # Predictive statistics\n            mean = pred_dist.mean\n            variance = pred_dist.variance\n            std = torch.sqrt(variance + 1e-6)\n            \n            # Entropy (Equation 24 from paper)\n            entropy = -mean * torch.log(mean + 1e-6) - (1 - mean) * torch.log(1 - mean + 1e-6)\n            \n            # Epistemic uncertainty (model uncertainty)\n            epistemic = output.variance\n            \n            # Aleatoric uncertainty (data uncertainty)\n            aleatoric = variance - epistemic\n            aleatoric = torch.clamp(aleatoric, min=0)  # Ensure non-negative\n            \n            # Confidence\n            confidence = 1 / (1 + std)\n        \n        return {\n            'mean': mean,\n            'variance': variance,\n            'std': std,\n            'entropy': entropy,\n            'epistemic': epistemic,\n            'aleatoric': aleatoric,\n            'confidence': confidence\n        }\n    \n    def adaptive_anomaly_detection(self, X: torch.Tensor, metadata: pd.DataFrame = None) -> Dict:\n        \"\"\"\n        Uncertainty-calibrated anomaly detection (from paper Section 4.4)\n        \n        Args:\n            X: Input features\n            metadata: Optional metadata with timestamps, sources, etc.\n        \"\"\"\n        uncertainties = self.compute_uncertainty_metrics(X)\n        \n        # Compute anomaly score (Equation 23 from paper)\n        if self.baseline_stats is not None:\n            deviation = torch.abs(uncertainties['mean'] - self.baseline_stats['mean'])\n        else:\n            deviation = uncertainties['mean']\n        \n        normalized_score = deviation / (uncertainties['std'] + 0.1)\n        \n        # Add entropy component\n        if self.config['entropy_weight'] > 0:\n            uncertainty_score = normalized_score + self.config['entropy_weight'] * uncertainties['entropy']\n        else:\n            uncertainty_score = normalized_score\n        \n        # Adaptive threshold (Equation 26 from paper)\n        if self.config['adaptive_threshold']:\n            base_threshold = 0.5\n            threshold = base_threshold + self.config['uncertainty_weight'] * uncertainties['std']\n            \n            # Cloud-specific adjustments\n            if metadata is not None and 'source' in metadata.columns:\n                # Adjust threshold based on cloud provider\n                provider_adjustments = {\n                    'containers': 0.1,\n                    'edge_iot': 0.15,\n                    'microsoft': 0.05\n                }\n                \n                for provider, adjustment in provider_adjustments.items():\n                    if provider in metadata['source'].values:\n                        threshold = threshold + adjustment\n        else:\n            threshold = torch.ones_like(uncertainties['mean']) * 0.5\n        \n        # Detection decisions\n        detections = uncertainty_score > threshold\n        \n        return {\n            'detections': detections,\n            'scores': uncertainty_score,\n            'threshold': threshold,\n            'uncertainties': uncertainties,\n            'components': {\n                'deviation': deviation,\n                'normalized': normalized_score,\n                'entropy_weighted': uncertainty_score\n            }\n        }\n    \n    def evaluate_calibration(self, X: torch.Tensor, y: torch.Tensor, n_bins: int = 10) -> Dict:\n        \"\"\"Evaluate prediction calibration (Expected Calibration Error)\"\"\"\n        uncertainties = self.compute_uncertainty_metrics(X)\n        predictions = uncertainties['mean'].round()\n        confidences = uncertainties['confidence']\n        \n        # Compute ECE\n        ece = 0\n        mce = 0\n        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n        reliability_data = []\n        \n        for i in range(n_bins):\n            mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n            if mask.sum() > 0:\n                bin_acc = (predictions[mask] == y[mask].float()).float().mean()\n                bin_conf = confidences[mask].mean()\n                bin_weight = mask.float().mean()\n                \n                ece += bin_weight * torch.abs(bin_acc - bin_conf)\n                mce = max(mce, torch.abs(bin_acc - bin_conf).item())\n                \n                reliability_data.append({\n                    'confidence': bin_conf.item(),\n                    'accuracy': bin_acc.item(),\n                    'count': mask.sum().item()\n                })\n        \n        # Brier Score\n        brier_score = ((uncertainties['mean'] - y.float())**2).mean().item()\n        \n        return {\n            'ece': ece.item(),\n            'mce': mce,\n            'brier_score': brier_score,\n            'reliability_diagram': reliability_data\n        }\n    \n    def incremental_update(self, X_new: torch.Tensor, y_new: torch.Tensor):\n        \"\"\"Online learning update (from paper Section 4.6)\"\"\"\n        self.model.train()\n        self.likelihood.train()\n        \n        # Create mini-batch\n        dataset = torch.utils.data.TensorDataset(X_new, y_new.float())\n        loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n        \n        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n        mll = VariationalELBO(self.likelihood, self.model, num_data=len(X_new))\n        \n        # Single epoch update\n        for batch_x, batch_y in loader:\n            optimizer.zero_grad()\n            output = self.model(batch_x)\n            loss = -mll(output, batch_y)\n            loss.backward()\n            optimizer.step()\n        \n        self.model.eval()\n        self.likelihood.eval()\n\nclass CloudDriftDetector:\n    \"\"\"Detect concept drift in cloud environments\"\"\"\n    \n    def __init__(self, window_size: int = 1000, threshold: float = 0.05):\n        self.window_size = window_size\n        self.threshold = threshold\n        self.reference_window = []\n        self.current_window = []\n        self.drift_history = []\n    \n    def detect_drift(self, features: torch.Tensor, predictions: torch.Tensor, \n                     uncertainties: Dict) -> Tuple[bool, float]:\n        \"\"\"Detect concept drift using uncertainty and distribution changes\"\"\"\n        \n        # Update windows\n        self.current_window.append({\n            'features': features.cpu().numpy(),\n            'predictions': predictions.cpu().numpy(),\n            'uncertainty': uncertainties['total'].mean().item(),\n            'timestamp': datetime.now()\n        })\n        \n        if len(self.current_window) > self.window_size:\n            self.reference_window.append(self.current_window.pop(0))\n            if len(self.reference_window) > self.window_size:\n                self.reference_window.pop(0)\n        \n        # Check for drift\n        if len(self.reference_window) >= self.window_size // 2 and \\\n           len(self.current_window) >= self.window_size // 2:\n            \n            # Compare uncertainty distributions\n            ref_uncertainty = np.array([w['uncertainty'] for w in self.reference_window])\n            curr_uncertainty = np.array([w['uncertainty'] for w in self.current_window])\n            \n            # KS test for distribution shift\n            ks_stat, p_value = ks_2samp(ref_uncertainty, curr_uncertainty)\n            \n            # Uncertainty ratio\n            uncertainty_ratio = curr_uncertainty.mean() / (ref_uncertainty.mean() + 1e-6)\n            \n            # Combined drift score\n            drift_score = ks_stat * 0.5 + max(0, uncertainty_ratio - 1) * 0.5\n            \n            drift_detected = drift_score > self.threshold\n            \n            if drift_detected:\n                self.drift_history.append({\n                    'timestamp': datetime.now(),\n                    'score': drift_score,\n                    'ks_stat': ks_stat,\n                    'uncertainty_ratio': uncertainty_ratio\n                })\n            \n            return drift_detected, drift_score\n        \n        return False, 0.0\n\n# ============================================================================\n# SECTION 4: COMPREHENSIVE EXPERIMENT PIPELINE\n# ============================================================================\n\nclass ICS3DExperimentPipeline:\n    \"\"\"Complete experimental pipeline for ICS3D dataset\"\"\"\n    \n    def __init__(self, output_dir: str = './ics3d_results'):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n        \n        self.experiment_id = f\"ics3d_gp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.results = {}\n        \n    def run_complete_experiment(self, use_unified: bool = True):\n        \"\"\"Run complete experimental evaluation on ICS3D\"\"\"\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"ICS3D GAUSSIAN PROCESS UNCERTAINTY-AWARE DETECTION\")\n        print(\"COMPLETE EXPERIMENTAL PIPELINE\")\n        print(\"=\"*70)\n        \n        # Step 1: Load and prepare data\n        print(\"\\n[Step 1] Loading ICS3D datasets...\")\n        loader = ICS3DDatasetLoader()\n        loader.load_all_datasets()\n        \n        # Process each dataset\n        for name in loader.loaded_data:\n            loader.preprocess_dataset(name)\n        \n        # Create unified dataset\n        if use_unified:\n            X, y, metadata = loader.create_unified_dataset()\n        else:\n            # Use largest dataset\n            name = 'microsoft_train'  # or choose specific dataset\n            data = loader.preprocessed_data[name]\n            X, y, metadata = data['X'], data['y'], data['metadata']\n        \n        # Step 2: Prepare data\n        print(\"\\n[Step 2] Preparing data...\")\n        \n        # Split data\n        X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n            X, y, metadata, test_size=0.2, random_state=42, stratify=y\n        )\n        \n        # Normalize\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n        \n        # Convert to tensors\n        X_train_tensor = torch.FloatTensor(X_train).to(device)\n        y_train_tensor = torch.FloatTensor(y_train).to(device)\n        X_test_tensor = torch.FloatTensor(X_test).to(device)\n        y_test_tensor = torch.FloatTensor(y_test).to(device)\n        \n        print(f\"  Training samples: {len(X_train):,}\")\n        print(f\"  Test samples: {len(X_test):,}\")\n        print(f\"  Features: {X_train.shape[1]}\")\n        print(f\"  Attack rate (train): {y_train.mean():.2%}\")\n        print(f\"  Attack rate (test): {y_test.mean():.2%}\")\n        \n        # Step 3: Initialize detector\n        print(\"\\n[Step 3] Initializing Cloud Security GP...\")\n        \n        detector = CloudUncertaintyDetector(\n            feature_dim=X_train.shape[1],\n            cloud_provider='multi',\n            config={\n                'num_inducing': min(500, X_train.shape[0] // 20),\n                'epochs': 30,\n                'batch_size': 256,\n                'adversarial_training': True\n            }\n        )\n        \n        # Select adversarial inducing points\n        inducing_points = detector.select_adversarial_inducing_points(\n            X_train_tensor[:5000],\n            y_train_tensor[:5000],\n            epsilon=0.1,\n            iterations=10\n        )\n        \n        # Initialize model\n        detector.model = CloudSecurityGP(\n            inducing_points=inducing_points,\n            feature_dim=X_train.shape[1],\n            cloud_provider='multi'\n        ).to(device)\n        \n        # Step 4: Train model\n        print(\"\\n[Step 4] Training model...\")\n        \n        # Create data loaders\n        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=256, shuffle=True\n        )\n        \n        val_size = int(0.1 * len(X_train_tensor))\n        val_dataset = torch.utils.data.TensorDataset(\n            X_train_tensor[-val_size:],\n            y_train_tensor[-val_size:]\n        )\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset, batch_size=256, shuffle=False\n        )\n        \n        # Train\n        detector.train_model(train_loader, val_loader, verbose=True)\n        \n        # Step 5: Evaluate\n        print(\"\\n[Step 5] Evaluating model...\")\n        \n        # Get predictions\n        results = detector.adaptive_anomaly_detection(X_test_tensor, meta_test)\n        \n        predictions = results['detections'].cpu().numpy()\n        scores = results['scores'].cpu().numpy()\n        uncertainties = results['uncertainties']\n        \n        # Compute metrics\n        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n        \n        accuracy = accuracy_score(y_test, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            y_test, predictions, average='binary', zero_division=0\n        )\n        \n        if len(np.unique(y_test)) > 1:\n            auc = roc_auc_score(y_test, scores)\n        else:\n            auc = 0.0\n        \n        print(f\"\\n  DETECTION METRICS:\")\n        print(f\"    Accuracy: {accuracy:.4f}\")\n        print(f\"    Precision: {precision:.4f}\")\n        print(f\"    Recall: {recall:.4f}\")\n        print(f\"    F1-Score: {f1:.4f}\")\n        print(f\"    AUC-ROC: {auc:.4f}\")\n        \n        # Calibration metrics\n        calibration = detector.evaluate_calibration(X_test_tensor, y_test_tensor)\n        \n        print(f\"\\n  CALIBRATION METRICS:\")\n        print(f\"    ECE: {calibration['ece']:.4f}\")\n        print(f\"    MCE: {calibration['mce']:.4f}\")\n        print(f\"    Brier Score: {calibration['brier_score']:.4f}\")\n        \n        # False positive analysis\n        fp_mask = (predictions == 1) & (y_test == 0)\n        fp_count = fp_mask.sum()\n        fp_rate = fp_count / (y_test == 0).sum() if (y_test == 0).sum() > 0 else 0\n        \n        # Uncertainty-based FP reduction\n        high_uncertainty = uncertainties['entropy'] > uncertainties['entropy'].median()\n        uncertain_fps = fp_mask & high_uncertainty.cpu().numpy()\n        fp_reduction = 1 - (uncertain_fps.sum() / fp_count) if fp_count > 0 else 0\n        \n        print(f\"\\n  FALSE POSITIVE ANALYSIS:\")\n        print(f\"    FP Rate: {fp_rate:.2%}\")\n        print(f\"    FP Count: {fp_count}\")\n        print(f\"    FP Reduction (uncertainty): {fp_reduction:.1%}\")\n        \n        # Per-provider analysis\n        if 'source' in meta_test.columns:\n            print(f\"\\n  PER-PROVIDER PERFORMANCE:\")\n            for source in meta_test['source'].unique():\n                source_mask = meta_test['source'] == source\n                if source_mask.sum() > 0:\n                    source_acc = accuracy_score(\n                        y_test[source_mask], \n                        predictions[source_mask]\n                    )\n                    print(f\"    {source}: {source_acc:.4f}\")\n        \n        # Step 6: Visualization\n        print(\"\\n[Step 6] Generating visualizations...\")\n        self.visualize_results(detector, X_test_tensor[:1000], y_test_tensor[:1000], \n                               meta_test.iloc[:1000])\n        \n        # Step 7: Save results\n        self.results = {\n            'metrics': {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'auc': auc,\n                'fp_rate': fp_rate,\n                'fp_reduction': fp_reduction\n            },\n            'calibration': calibration,\n            'experiment_id': self.experiment_id,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        # Save to file\n        results_file = self.output_dir / f\"{self.experiment_id}_results.json\"\n        with open(results_file, 'w') as f:\n            json.dump(self.results, f, indent=2, default=str)\n        \n        print(f\"\\n[Step 7] Results saved to {results_file}\")\n        \n        return self.results\n    \n    def visualize_results(self, detector, X_test, y_test, metadata):\n        \"\"\"Generate comprehensive visualizations\"\"\"\n        \n        fig = plt.figure(figsize=(20, 12))\n        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n        \n        # Get predictions\n        results = detector.adaptive_anomaly_detection(X_test, metadata)\n        uncertainties = results['uncertainties']\n        \n        # 1. Predictions with uncertainty\n        ax1 = fig.add_subplot(gs[0, :])\n        \n        time_index = np.arange(len(X_test))\n        mean = uncertainties['mean'].cpu().numpy()\n        std = uncertainties['std'].cpu().numpy()\n        \n        ax1.plot(time_index, mean, 'b-', alpha=0.7, label='Prediction')\n        ax1.fill_between(time_index, mean - 2*std, mean + 2*std,\n                         alpha=0.3, color='blue', label='95% CI')\n        \n        # Mark true anomalies\n        anomaly_mask = y_test.cpu().numpy() == 1\n        if anomaly_mask.any():\n            ax1.scatter(time_index[anomaly_mask], mean[anomaly_mask],\n                       c='red', marker='x', s=50, label='True Anomalies', alpha=0.5)\n        \n        ax1.set_xlabel('Sample Index')\n        ax1.set_ylabel('Anomaly Probability')\n        ax1.set_title('GP Predictions with Uncertainty Quantification')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # 2. Uncertainty decomposition\n        ax2 = fig.add_subplot(gs[1, 0])\n        \n        epistemic = uncertainties['epistemic'].cpu().numpy()\n        aleatoric = uncertainties['aleatoric'].cpu().numpy()\n        \n        ax2.plot(time_index, epistemic, 'g-', alpha=0.7, label='Epistemic')\n        ax2.plot(time_index, aleatoric, 'orange', alpha=0.7, label='Aleatoric')\n        \n        ax2.set_xlabel('Sample Index')\n        ax2.set_ylabel('Uncertainty')\n        ax2.set_title('Uncertainty Decomposition')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # 3. Entropy\n        ax3 = fig.add_subplot(gs[1, 1])\n        \n        entropy = uncertainties['entropy'].cpu().numpy()\n        ax3.plot(time_index, entropy, 'purple', alpha=0.7)\n        ax3.fill_between(time_index, 0, entropy, alpha=0.3, color='purple')\n        \n        ax3.set_xlabel('Sample Index')\n        ax3.set_ylabel('Entropy')\n        ax3.set_title('Predictive Entropy')\n        ax3.grid(True, alpha=0.3)\n        \n        # 4. Score distribution\n        ax4 = fig.add_subplot(gs[1, 2])\n        \n        scores = results['scores'].cpu().numpy()\n        normal_scores = scores[y_test.cpu().numpy() == 0]\n        anomaly_scores = scores[y_test.cpu().numpy() == 1]\n        \n        if len(normal_scores) > 0:\n            ax4.hist(normal_scores, bins=30, alpha=0.5, label='Normal', density=True)\n        if len(anomaly_scores) > 0:\n            ax4.hist(anomaly_scores, bins=30, alpha=0.5, label='Anomaly', density=True)\n        \n        ax4.set_xlabel('Anomaly Score')\n        ax4.set_ylabel('Density')\n        ax4.set_title('Score Distribution by Class')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        # 5. Calibration plot\n        ax5 = fig.add_subplot(gs[2, 0])\n        \n        # Simple calibration plot\n        ax5.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect Calibration')\n        \n        # Bin predictions\n        n_bins = 10\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        bin_centers = []\n        bin_accuracies = []\n        \n        confidences = uncertainties['confidence'].cpu().numpy()\n        predictions = results['detections'].cpu().numpy()\n        y_true = y_test.cpu().numpy()\n        \n        for i in range(n_bins):\n            mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n            if mask.sum() > 0:\n                bin_centers.append((bin_boundaries[i] + bin_boundaries[i+1]) / 2)\n                bin_accuracies.append((predictions[mask] == y_true[mask]).mean())\n        \n        if bin_centers:\n            ax5.plot(bin_centers, bin_accuracies, 'bo-', label='Model')\n        \n        ax5.set_xlabel('Confidence')\n        ax5.set_ylabel('Accuracy')\n        ax5.set_title('Calibration Plot')\n        ax5.legend()\n        ax5.grid(True, alpha=0.3)\n        \n        # 6. ROC curve\n        ax6 = fig.add_subplot(gs[2, 1])\n        \n        from sklearn.metrics import roc_curve, auc\n        if len(np.unique(y_true)) > 1:\n            fpr, tpr, _ = roc_curve(y_true, scores)\n            roc_auc = auc(fpr, tpr)\n            \n            ax6.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n            ax6.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n        \n        ax6.set_xlabel('False Positive Rate')\n        ax6.set_ylabel('True Positive Rate')\n        ax6.set_title('ROC Curve')\n        ax6.legend()\n        ax6.grid(True, alpha=0.3)\n        \n        # 7. Confusion matrix\n        ax7 = fig.add_subplot(gs[2, 2])\n        \n        cm = confusion_matrix(y_true, predictions)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax7)\n        ax7.set_xlabel('Predicted')\n        ax7.set_ylabel('Actual')\n        ax7.set_title('Confusion Matrix')\n        \n        plt.suptitle('ICS3D GP Uncertainty-Aware Detection Results', fontsize=16, y=1.02)\n        plt.tight_layout()\n        \n        # Save figure\n        fig_path = self.output_dir / f\"{self.experiment_id}_visualization.png\"\n        plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(f\"  Visualization saved to {fig_path}\")\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"GAUSSIAN PROCESS UNCERTAINTY-AWARE DETECTION\")\n    print(\"INTEGRATED CLOUD SECURITY 3-DATASETS (ICS3D)\")\n    print(\"=\"*80)\n    \n    # Run complete experiment\n    pipeline = ICS3DExperimentPipeline()\n    results = pipeline.run_complete_experiment(use_unified=True)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT COMPLETE\")\n    print(\"=\"*80)\n    \n    print(\"\\nFINAL RESULTS SUMMARY:\")\n    print(f\"  Detection F1-Score: {results['metrics']['f1']:.4f}\")\n    print(f\"  AUC-ROC: {results['metrics']['auc']:.4f}\")\n    print(f\"  False Positive Reduction: {results['metrics']['fp_reduction']:.1%}\")\n    print(f\"  Calibration Error (ECE): {results['calibration']['ece']:.4f}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Set up environment\n    import os\n    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For better error messages\n    \n    # Run main experiment\n    results = main() \n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}