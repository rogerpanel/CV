{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rogernickanaedevha/neural-ode-model?scriptVersionId=272380189\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n# Paper 4: Neural ODE-Point Process Integration for Real-Time Adaptive Network Defense\n## Target: IEEE Transactions on Neural Networks and Learning Systems\n## Author: Roger Nick Anaedevha\n","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchdiffeq import odeint, odeint_adjoint\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, List, Dict, Optional\nimport warnings\nimport os\nimport kagglehub\nfrom tqdm import tqdm\nimport time\nfrom collections import defaultdict\nfrom scipy.stats import norm\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom pyro.optim import Adam as PyroAdam\n\nwarnings.filterwarnings('ignore')\n\n# Set random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ========================= Neural ODE Components =========================\n\nclass ODEFunc(nn.Module):\n    \"\"\"Neural ODE dynamics function\"\"\"\n    \n    def __init__(self, hidden_dim, n_layers=3):\n        super().__init__()\n        \n        layers = []\n        for i in range(n_layers):\n            if i == 0:\n                layers.append(nn.Linear(hidden_dim + 1, hidden_dim))  # +1 for time\n            else:\n                layers.append(nn.Linear(hidden_dim, hidden_dim))\n            layers.append(nn.Tanh())\n            \n        self.net = nn.Sequential(*layers)\n        self.hidden_dim = hidden_dim\n        \n    def forward(self, t, h):\n        \"\"\"\n        Args:\n            t: Current time\n            h: Hidden state [batch_size, hidden_dim]\n        \"\"\"\n        # Concatenate time to hidden state\n        t_vec = torch.ones(h.shape[0], 1).to(h.device) * t\n        h_t = torch.cat([h, t_vec], dim=1)\n        \n        # Compute dynamics\n        dh_dt = self.net(h_t)\n        \n        return dh_dt\n\nclass BayesianNeuralODE(nn.Module):\n    \"\"\"Bayesian Neural ODE for network dynamics modeling\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=3):\n        super().__init__()\n        \n        # Feature encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n        # ODE dynamics\n        self.ode_func = ODEFunc(hidden_dim, n_layers)\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n        \n        # Uncertainty parameters\n        self.log_noise = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x, t_span):\n        \"\"\"\n        Args:\n            x: Input features [batch_size, input_dim]\n            t_span: Time points for ODE integration\n        \"\"\"\n        # Encode input\n        h0 = self.encoder(x)\n        \n        # Solve ODE\n        h_t = odeint_adjoint(\n            self.ode_func,\n            h0,\n            t_span,\n            method='dopri5',\n            rtol=1e-3,\n            atol=1e-4\n        )\n        \n        # Select final time point\n        h_final = h_t[-1]\n        \n        # Decode\n        output = self.decoder(h_final)\n        \n        return output, h_final\n    \n    def sample_trajectory(self, x, t_span, n_samples=10):\n        \"\"\"Sample multiple trajectories for uncertainty estimation\"\"\"\n        trajectories = []\n        \n        for _ in range(n_samples):\n            # Add noise to initial condition\n            h0 = self.encoder(x)\n            noise = torch.randn_like(h0) * torch.exp(self.log_noise)\n            h0_noisy = h0 + noise\n            \n            # Solve ODE\n            h_t = odeint(\n                self.ode_func,\n                h0_noisy,\n                t_span,\n                method='dopri5'\n            )\n            \n            trajectories.append(h_t)\n            \n        return torch.stack(trajectories)\n\n# ========================= Point Process Components =========================\n\nclass HawkesProcess(nn.Module):\n    \"\"\"Multivariate Hawkes Process for attack event modeling\"\"\"\n    \n    def __init__(self, n_types, hidden_dim=64):\n        super().__init__()\n        \n        self.n_types = n_types\n        \n        # Base intensity\n        self.mu = nn.Parameter(torch.ones(n_types) * 0.1)\n        \n        # Excitation matrix\n        self.alpha = nn.Parameter(torch.ones(n_types, n_types) * 0.1)\n        \n        # Decay parameters\n        self.beta = nn.Parameter(torch.ones(n_types) * 1.0)\n        \n        # Neural intensity modulation\n        self.intensity_net = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, n_types),\n            nn.Softplus()\n        )\n        \n    def compute_intensity(self, t, history, hidden_state=None):\n        \"\"\"\n        Compute conditional intensity at time t\n        \n        Args:\n            t: Current time\n            history: List of (time, type) tuples\n            hidden_state: Optional hidden state from Neural ODE\n        \"\"\"\n        intensity = self.mu.clone()\n        \n        # Add self-excitation from history\n        for t_i, m_i in history:\n            if t_i < t:\n                dt = t - t_i\n                intensity += self.alpha[m_i, :] * torch.exp(-self.beta * dt)\n                \n        # Modulate with hidden state if available\n        if hidden_state is not None:\n            modulation = self.intensity_net(hidden_state)\n            intensity = intensity * modulation.squeeze()\n            \n        return torch.clamp(intensity, min=1e-6)\n    \n    def log_likelihood(self, events, T):\n        \"\"\"\n        Compute log-likelihood of event sequence\n        \n        Args:\n            events: List of (time, type) tuples\n            T: Observation window\n        \"\"\"\n        ll = 0\n        \n        for i, (t_i, m_i) in enumerate(events):\n            # Intensity at event time\n            history = events[:i]\n            lambda_i = self.compute_intensity(t_i, history)\n            ll += torch.log(lambda_i[m_i])\n            \n        # Integral term (compensator)\n        compensator = self.compute_compensator(events, T)\n        ll -= compensator\n        \n        return ll\n    \n    def compute_compensator(self, events, T):\n        \"\"\"Compute integrated intensity (compensator)\"\"\"\n        compensator = self.mu.sum() * T\n        \n        for t_i, m_i in events:\n            # Contribution from each event\n            integral = self.alpha[m_i, :].sum() / self.beta * (1 - torch.exp(-self.beta * (T - t_i)))\n            compensator += integral.sum()\n            \n        return compensator\n    \n    def sample_next_event(self, t_current, history, max_time=10.0):\n        \"\"\"Sample next event using thinning algorithm\"\"\"\n        t = t_current\n        \n        while t < max_time:\n            # Upper bound on intensity\n            lambda_max = self.compute_intensity(t, history).sum() * 1.5\n            \n            # Sample waiting time\n            dt = torch.distributions.Exponential(lambda_max).sample()\n            t = t + dt\n            \n            if t > max_time:\n                break\n                \n            # Accept/reject\n            lambda_t = self.compute_intensity(t, history)\n            u = torch.rand(1)\n            \n            if u < lambda_t.sum() / lambda_max:\n                # Accept event\n                # Sample type\n                probs = lambda_t / lambda_t.sum()\n                m = torch.multinomial(probs, 1).item()\n                return t, m\n                \n        return None, None\n\n# ========================= Neural ODE-Point Process Integration =========================\n\nclass NeuralODEPointProcess(nn.Module):\n    \"\"\"Integrated Neural ODE-Point Process model\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, n_attack_types):\n        super().__init__()\n        \n        # Neural ODE for continuous dynamics\n        self.neural_ode = BayesianNeuralODE(\n            input_dim, hidden_dim, n_attack_types\n        )\n        \n        # Hawkes process for discrete events\n        self.point_process = HawkesProcess(n_attack_types, hidden_dim)\n        \n        # Coupling network\n        self.coupling = nn.Sequential(\n            nn.Linear(hidden_dim + n_attack_types, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n        self.hidden_dim = hidden_dim\n        self.n_attack_types = n_attack_types\n        \n    def forward(self, x, t_span, events=None):\n        \"\"\"\n        Forward pass combining ODE and point process\n        \n        Args:\n            x: Input features\n            t_span: Time points for integration\n            events: Optional event history\n        \"\"\"\n        # Get ODE trajectory\n        output, h_final = self.neural_ode(x, t_span)\n        \n        # Compute point process intensity\n        if events is not None:\n            intensities = []\n            for t in t_span:\n                intensity = self.point_process.compute_intensity(\n                    t, events, h_final\n                )\n                intensities.append(intensity)\n            intensities = torch.stack(intensities)\n        else:\n            intensities = None\n            \n        return output, h_final, intensities\n    \n    def compute_elbo(self, x, y, t_span, events, n_samples=10):\n        \"\"\"Compute evidence lower bound for variational inference\"\"\"\n        \n        # Sample multiple trajectories\n        log_probs = []\n        kl_divs = []\n        \n        for _ in range(n_samples):\n            # Forward pass\n            output, h_final, intensities = self.forward(x, t_span, events)\n            \n            # Classification likelihood\n            log_p_y = -F.cross_entropy(output, y, reduction='none')\n            log_probs.append(log_p_y)\n            \n            # Point process likelihood\n            if events is not None:\n                pp_ll = self.point_process.log_likelihood(events, t_span[-1])\n                log_probs[-1] = log_probs[-1] + 0.1 * pp_ll\n                \n            # KL divergence (simplified - assuming Gaussian prior)\n            kl = 0.5 * torch.mean(h_final ** 2)\n            kl_divs.append(kl)\n            \n        # Average over samples\n        log_prob = torch.stack(log_probs).mean(0)\n        kl_div = torch.stack(kl_divs).mean()\n        \n        elbo = log_prob.mean() - kl_div\n        \n        return -elbo  # Return negative ELBO as loss\n\n# ========================= Variational Inference =========================\n\nclass VariationalInference:\n    \"\"\"Variational inference for Bayesian Neural ODE-PP\"\"\"\n    \n    def __init__(self, model, device):\n        self.model = model.to(device)\n        self.device = device\n        \n    def model_fn(self, x, y=None):\n        \"\"\"Pyro model for VI\"\"\"\n        # Priors on ODE parameters\n        ode_weight_prior = dist.Normal(0, 1)\n        ode_bias_prior = dist.Normal(0, 1)\n        \n        # Priors on point process parameters\n        mu_prior = dist.Gamma(1, 1)\n        alpha_prior = dist.Gamma(1, 1)\n        beta_prior = dist.Gamma(1, 1)\n        \n        # Sample parameters\n        pyro.module(\"neural_ode_pp\", self.model)\n        \n        # Likelihood\n        if y is not None:\n            with pyro.plate(\"data\", len(x)):\n                output, _, _ = self.model(x, torch.linspace(0, 1, 10))\n                pyro.sample(\"obs\", dist.Categorical(logits=output), obs=y)\n                \n    def guide_fn(self, x, y=None):\n        \"\"\"Pyro guide (variational distribution) for VI\"\"\"\n        # Variational parameters\n        pyro.module(\"neural_ode_pp\", self.model)\n        \n    def train_svi(self, train_loader, epochs=10):\n        \"\"\"Train using stochastic variational inference\"\"\"\n        optimizer = PyroAdam({\"lr\": 1e-3})\n        svi = SVI(self.model_fn, self.guide_fn, optimizer, loss=Trace_ELBO())\n        \n        losses = []\n        for epoch in range(epochs):\n            epoch_loss = 0\n            for x, y in train_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                loss = svi.step(x, y)\n                epoch_loss += loss\n                \n            avg_loss = epoch_loss / len(train_loader)\n            losses.append(avg_loss)\n            \n            if (epoch + 1) % 5 == 0:\n                print(f\"Epoch {epoch+1}/{epochs}, ELBO Loss: {avg_loss:.4f}\")\n                \n        return losses\n\n# ========================= Real-Time Adaptive Learning =========================\n\nclass RealTimeAdapter:\n    \"\"\"Real-time adaptive learning for streaming data\"\"\"\n    \n    def __init__(self, model, device, buffer_size=1000):\n        self.model = model.to(device)\n        self.device = device\n        self.buffer_size = buffer_size\n        \n        # Experience replay buffer\n        self.buffer_x = []\n        self.buffer_y = []\n        self.buffer_events = []\n        \n        # Online statistics\n        self.n_seen = 0\n        self.accuracy_window = []\n        \n    def update(self, x, y, events=None):\n        \"\"\"Online update with new sample\"\"\"\n        # Add to buffer\n        self.buffer_x.append(x)\n        self.buffer_y.append(y)\n        if events is not None:\n            self.buffer_events.append(events)\n            \n        # Maintain buffer size\n        if len(self.buffer_x) > self.buffer_size:\n            self.buffer_x.pop(0)\n            self.buffer_y.pop(0)\n            if len(self.buffer_events) > 0:\n                self.buffer_events.pop(0)\n                \n        self.n_seen += 1\n        \n        # Periodic adaptation\n        if self.n_seen % 100 == 0:\n            self.adapt()\n            \n    def adapt(self):\n        \"\"\"Adapt model with buffered data\"\"\"\n        if len(self.buffer_x) < 10:\n            return\n            \n        # Convert buffer to tensors\n        X = torch.stack(self.buffer_x)\n        y = torch.stack(self.buffer_y)\n        \n        # Quick fine-tuning\n        optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n        \n        for _ in range(5):  # Few gradient steps\n            optimizer.zero_grad()\n            \n            t_span = torch.linspace(0, 1, 10).to(self.device)\n            output, _, _ = self.model(X, t_span)\n            \n            loss = F.cross_entropy(output, y)\n            loss.backward()\n            optimizer.step()\n            \n    def predict_with_uncertainty(self, x, n_samples=10):\n        \"\"\"Predict with uncertainty quantification\"\"\"\n        self.model.eval()\n        \n        predictions = []\n        with torch.no_grad():\n            for _ in range(n_samples):\n                t_span = torch.linspace(0, 1, 10).to(self.device)\n                output, _, _ = self.model(x.unsqueeze(0), t_span)\n                prob = F.softmax(output, dim=1)\n                predictions.append(prob)\n                \n        predictions = torch.stack(predictions)\n        \n        # Mean and uncertainty\n        mean_pred = predictions.mean(0)\n        uncertainty = predictions.std(0)\n        \n        return mean_pred, uncertainty\n\n# ========================= Evaluation Framework =========================\n\nclass RealTimeEvaluator:\n    \"\"\"Comprehensive evaluation for real-time adaptive system\"\"\"\n    \n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n        self.results = {}\n        \n    def evaluate_detection_performance(self, test_loader):\n        \"\"\"Evaluate intrusion detection performance\"\"\"\n        print(\"\\n=== Detection Performance ===\")\n        \n        self.model.eval()\n        all_preds = []\n        all_labels = []\n        all_probs = []\n        \n        with torch.no_grad():\n            for x, y in test_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                \n                t_span = torch.linspace(0, 1, 10).to(self.device)\n                output, _, _ = self.model(x, t_span)\n                \n                probs = F.softmax(output, dim=1)\n                preds = torch.argmax(output, dim=1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(y.cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n                \n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average='weighted')\n        \n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        \n        self.results['accuracy'] = accuracy\n        self.results['f1'] = f1\n        \n        return self.results\n    \n    def evaluate_uncertainty_calibration(self, test_loader, n_samples=20):\n        \"\"\"Evaluate uncertainty calibration\"\"\"\n        print(\"\\n=== Uncertainty Calibration ===\")\n        \n        self.model.eval()\n        \n        confidences = []\n        accuracies = []\n        \n        with torch.no_grad():\n            for x, y in test_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                \n                # Get predictions with uncertainty\n                preds_list = []\n                for _ in range(n_samples):\n                    t_span = torch.linspace(0, 1, 10).to(self.device)\n                    output, _, _ = self.model(x, t_span)\n                    probs = F.softmax(output, dim=1)\n                    preds_list.append(probs)\n                    \n                # Average predictions\n                mean_probs = torch.stack(preds_list).mean(0)\n                pred_class = torch.argmax(mean_probs, dim=1)\n                confidence = mean_probs.max(dim=1)[0]\n                \n                # Check accuracy\n                correct = (pred_class == y).float()\n                \n                confidences.extend(confidence.cpu().numpy())\n                accuracies.extend(correct.cpu().numpy())\n                \n        # Calibration error\n        confidences = np.array(confidences)\n        accuracies = np.array(accuracies)\n        \n        # ECE calculation\n        n_bins = 10\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        ece = 0\n        \n        for i in range(n_bins):\n            mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i+1])\n            if mask.sum() > 0:\n                bin_acc = accuracies[mask].mean()\n                bin_conf = confidences[mask].mean()\n                ece += mask.sum() * np.abs(bin_acc - bin_conf)\n                \n        ece /= len(confidences)\n        \n        print(f\"Expected Calibration Error: {ece:.4f}\")\n        \n        self.results['ece'] = ece\n        \n        return self.results\n    \n    def evaluate_temporal_performance(self, test_loader):\n        \"\"\"Evaluate temporal modeling performance\"\"\"\n        print(\"\\n=== Temporal Performance ===\")\n        \n        # Simulate temporal attack patterns\n        self.model.eval()\n        \n        # Generate synthetic event sequence\n        events = []\n        current_time = 0\n        \n        for _ in range(100):\n            # Sample next event\n            dt = np.random.exponential(0.1)\n            event_type = np.random.randint(0, self.model.n_attack_types)\n            current_time += dt\n            events.append((current_time, event_type))\n            \n        # Compute log-likelihood\n        with torch.no_grad():\n            ll = self.model.point_process.log_likelihood(events, current_time)\n            \n        print(f\"Point Process Log-Likelihood: {ll.item():.4f}\")\n        \n        self.results['pp_likelihood'] = ll.item()\n        \n        return self.results\n    \n    def evaluate_real_time_adaptation(self, stream_loader, window_size=100):\n        \"\"\"Evaluate real-time adaptation capability\"\"\"\n        print(\"\\n=== Real-Time Adaptation ===\")\n        \n        adapter = RealTimeAdapter(self.model, self.device)\n        \n        accuracies = []\n        adaptation_times = []\n        \n        for i, (x, y) in enumerate(stream_loader):\n            if i >= 1000:  # Limit evaluation\n                break\n                \n            x, y = x.to(self.device), y.to(self.device)\n            \n            # Predict before adaptation\n            start_time = time.time()\n            mean_pred, uncertainty = adapter.predict_with_uncertainty(x[0])\n            pred = torch.argmax(mean_pred)\n            \n            # Check accuracy\n            correct = (pred == y[0]).item()\n            accuracies.append(correct)\n            \n            # Update adapter\n            adapter.update(x[0], y[0])\n            \n            adaptation_time = time.time() - start_time\n            adaptation_times.append(adaptation_time)\n            \n            # Print progress\n            if (i + 1) % 100 == 0:\n                recent_acc = np.mean(accuracies[-window_size:])\n                avg_time = np.mean(adaptation_times[-window_size:])\n                print(f\"Step {i+1}: Accuracy={recent_acc:.4f}, Time={avg_time*1000:.2f}ms\")\n                \n        final_accuracy = np.mean(accuracies)\n        avg_adaptation_time = np.mean(adaptation_times)\n        \n        print(f\"\\nFinal Streaming Accuracy: {final_accuracy:.4f}\")\n        print(f\"Average Adaptation Time: {avg_adaptation_time*1000:.2f}ms\")\n        \n        self.results['streaming_accuracy'] = final_accuracy\n        self.results['adaptation_time_ms'] = avg_adaptation_time * 1000\n        \n        return self.results, accuracies\n    \n    def plot_results(self, history, streaming_acc):\n        \"\"\"Plot comprehensive results\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n        \n        # Training loss\n        if 'train_loss' in history:\n            axes[0, 0].plot(history['train_loss'])\n            axes[0, 0].set_xlabel('Epoch')\n            axes[0, 0].set_ylabel('Loss')\n            axes[0, 0].set_title('Training Loss')\n            axes[0, 0].grid(True)\n        \n        # Validation accuracy\n        if 'val_acc' in history:\n            axes[0, 1].plot(history['val_acc'])\n            axes[0, 1].set_xlabel('Epoch')\n            axes[0, 1].set_ylabel('Accuracy')\n            axes[0, 1].set_title('Validation Accuracy')\n            axes[0, 1].grid(True)\n        \n        # ELBO\n        if 'elbo' in history:\n            axes[0, 2].plot(history['elbo'])\n            axes[0, 2].set_xlabel('Iteration')\n            axes[0, 2].set_ylabel('ELBO')\n            axes[0, 2].set_title('Evidence Lower Bound')\n            axes[0, 2].grid(True)\n        \n        # Streaming accuracy\n        if len(streaming_acc) > 0:\n            window = 50\n            smoothed = pd.Series(streaming_acc).rolling(window).mean()\n            axes[1, 0].plot(smoothed)\n            axes[1, 0].set_xlabel('Sample')\n            axes[1, 0].set_ylabel('Accuracy')\n            axes[1, 0].set_title(f'Streaming Accuracy (window={window})')\n            axes[1, 0].grid(True)\n        \n        # Performance metrics bar chart\n        metrics = ['Accuracy', 'F1', 'ECE']\n        values = [\n            self.results.get('accuracy', 0),\n            self.results.get('f1', 0),\n            1 - self.results.get('ece', 1)  # Convert to calibration score\n        ]\n        axes[1, 1].bar(metrics, values)\n        axes[1, 1].set_ylabel('Score')\n        axes[1, 1].set_title('Performance Metrics')\n        axes[1, 1].set_ylim([0, 1])\n        \n        # Temporal modeling\n        t = np.linspace(0, 10, 100)\n        intensity = np.exp(-t) * 0.5  # Example decay\n        axes[1, 2].plot(t, intensity)\n        axes[1, 2].set_xlabel('Time')\n        axes[1, 2].set_ylabel('Intensity')\n        axes[1, 2].set_title('Attack Intensity Function')\n        axes[1, 2].grid(True)\n        \n        plt.tight_layout()\n        plt.savefig('paper4_results.png', dpi=150)\n        plt.show()\n\n# ========================= Training Framework =========================\n\ndef train_neural_ode_pp(model, train_loader, val_loader, device, epochs=30):\n    \"\"\"Train Neural ODE-Point Process model\"\"\"\n    \n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    history = defaultdict(list)\n    best_val_acc = 0\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        \n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Time span for ODE\n            t_span = torch.linspace(0, 1, 10).to(device)\n            \n            # Generate synthetic events (for demonstration)\n            events = [(np.random.random(), np.random.randint(0, model.n_attack_types)) \n                     for _ in range(5)]\n            \n            # Compute ELBO loss\n            loss = model.compute_elbo(x, y, t_span, events)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n        # Validation\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                \n                t_span = torch.linspace(0, 1, 10).to(device)\n                output, _, _ = model(x, t_span)\n                \n                preds = torch.argmax(output, dim=1)\n                val_correct += (preds == y).sum().item()\n                val_total += len(y)\n                \n        val_acc = val_correct / val_total\n        \n        # Update scheduler\n        scheduler.step()\n        \n        # Save history\n        history['train_loss'].append(train_loss / len(train_loader))\n        history['val_acc'].append(val_acc)\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model_paper4.pt')\n            \n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print(f\"  Train Loss: {train_loss/len(train_loader):.4f}\")\n            print(f\"  Val Acc: {val_acc:.4f}\")\n            \n    return history\n\n# ========================= Data Preparation =========================\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"Custom dataset for time series security data\"\"\"\n    \n    def __init__(self, X, y, sequence_length=10):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n        self.sequence_length = sequence_length\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# ========================= Main Execution =========================\n\ndef main():\n    \"\"\"Main execution\"\"\"\n    print(\"=\"*80)\n    print(\"Paper 4: Neural ODE-Point Process Integration\")\n    print(\"=\"*80)\n    \n    # Load data\n    print(\"\\n1. Loading ICS3D datasets...\")\n    from Paper3_implementation import ICS3DDataLoader  # Reuse from Paper 3\n    \n    data_loader = ICS3DDataLoader()\n    \n    # Load dataset\n    print(\"   Loading Edge-IIoT dataset...\")\n    X, y = data_loader.load_edge_iiot('DNN')\n    \n    # Preprocess\n    from sklearn.preprocessing import StandardScaler, LabelEncoder\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    le = LabelEncoder()\n    y = le.fit_transform(y)\n    \n    # Split data\n    from sklearn.model_selection import train_test_split\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42\n    )\n    \n    # Create datasets\n    train_dataset = TimeSeriesDataset(X_train[:10000], y_train[:10000])\n    val_dataset = TimeSeriesDataset(X_val[:2000], y_val[:2000])\n    test_dataset = TimeSeriesDataset(X_test[:2000], y_test[:2000])\n    \n    # Create loaders\n    batch_size = 32\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    stream_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n    \n    print(f\"\\n2. Data Statistics:\")\n    print(f\"   Train samples: {len(train_dataset)}\")\n    print(f\"   Val samples: {len(val_dataset)}\")\n    print(f\"   Test samples: {len(test_dataset)}\")\n    print(f\"   Feature dimension: {X.shape[1]}\")\n    print(f\"   Number of classes: {len(np.unique(y))}\")\n    \n    # Initialize model\n    print(\"\\n3. Initializing Neural ODE-Point Process Model...\")\n    model = NeuralODEPointProcess(\n        input_dim=X.shape[1],\n        hidden_dim=128,\n        n_attack_types=len(np.unique(y))\n    )\n    \n    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Train model\n    print(\"\\n4. Training with Bayesian Neural ODE...\")\n    history = train_neural_ode_pp(\n        model, train_loader, val_loader, device, epochs=30\n    )\n    \n    # Variational Inference\n    print(\"\\n5. Variational Inference...\")\n    vi = VariationalInference(model, device)\n    # Note: Skipping full VI training for computational efficiency\n    # elbo_history = vi.train_svi(train_loader, epochs=10)\n    \n    # Comprehensive Evaluation\n    print(\"\\n6. Comprehensive Evaluation...\")\n    evaluator = RealTimeEvaluator(model, device)\n    \n    # Detection performance\n    results = evaluator.evaluate_detection_performance(test_loader)\n    \n    # Uncertainty calibration\n    results = evaluator.evaluate_uncertainty_calibration(test_loader)\n    \n    # Temporal performance\n    results = evaluator.evaluate_temporal_performance(test_loader)\n    \n    # Real-time adaptation\n    results, streaming_acc = evaluator.evaluate_real_time_adaptation(stream_loader)\n    \n    # Plot results\n    print(\"\\n7. Generating visualizations...\")\n    evaluator.plot_results(history, streaming_acc)\n    \n    # Final summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL RESULTS SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Detection Accuracy: {results['accuracy']:.4f}\")\n    print(f\"F1 Score: {results['f1']:.4f}\")\n    print(f\"Calibration Error (ECE): {results['ece']:.4f}\")\n    print(f\"Streaming Accuracy: {results['streaming_accuracy']:.4f}\")\n    print(f\"Adaptation Time: {results['adaptation_time_ms']:.2f}ms\")\n    print(f\"Point Process Log-Likelihood: {results['pp_likelihood']:.4f}\")\n    print(\"=\"*80)\n    \n    return model, history, results\n\nif __name__ == \"__main__\":\n    model, history, results = main()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}