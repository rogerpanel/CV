{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479689,"sourceType":"datasetVersion","datasetId":7874259}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"\"\"\n# MambaShield: Resilient Temporally-Aware Intrusion Detection Based on Selective State Space Models\n## Authors: Roger Nick Anaedevha, Alexander Gennadevich Trofimov, Yuri Vladimirovich Borodachev\n## Paper: IEEE Transactions on Artificial Intelligence (TAI)\n\n### Complete implementation with all methodologies, algorithms, and evaluation metrics.\n### Optimized for GPU memory efficiency and production deployment.\n\nCitation:\n@article{anaedevha2025mambashield,\n  title={MambaShield: Resilient Temporally-Aware Intrusion Detection Based on Selective State Space Models},\n  author={Anaedevha, Roger Nick and Trofimov, Alexander Gennadevich and Borodachev, Yuri Vladimirovich},\n  journal={IEEE Transactions on Artificial Intelligence},\n  year={2025}\n}\n\"\"\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# ======================================================================================================\n# IMPORTING LIBRARIES\n# ======================================================================================================","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport sys\nimport gc\nimport time\nimport math\nimport json\nimport random\nimport warnings\nimport traceback\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional, Union, Any, Callable\nfrom collections import defaultdict, deque\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset, ConcatDataset\nfrom torch.nn.utils import clip_grad_norm_\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, roc_curve, auc,\n    average_precision_score, matthews_corrcoef, cohen_kappa_score,\n    classification_report, balanced_accuracy_score,\n    brier_score_loss\n)\nfrom sklearn.calibration import calibration_curve  # or CalibrationDisplay\n\nfrom sklearn.utils import shuffle\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T04:28:09.081286Z","iopub.execute_input":"2025-08-09T04:28:09.081473Z","iopub.status.idle":"2025-08-09T04:28:15.318645Z","shell.execute_reply.started":"2025-08-09T04:28:09.081457Z","shell.execute_reply":"2025-08-09T04:28:15.317860Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\n# ============================================================================\n# CONFIGURATION AND HYPERPARAMETERS\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\n@dataclass\nclass MambaShieldConfig:\n    \"\"\"Configuration for MambaShield model as per paper specifications\"\"\"\n    # Model Architecture (Section III.B)\n    input_dim: int = 84  # Unified feature dimension\n    hidden_dim: int = 256  # D in paper\n    d_state: int = 16  # N in paper\n    d_conv: int = 4  # Convolution kernel size\n    n_layers: int = 3  # Number of Mamba blocks\n    n_scales: int = 4  # Multi-scale temporal aggregation\n    dropout: float = 0.1\n    \n    # Training Configuration (Section V.A.3)\n    batch_size: int = 32\n    learning_rate: float = 1e-4\n    weight_decay: float = 1e-5\n    epochs: int = 100\n    early_stopping_patience: int = 10\n    gradient_clip: float = 1.0\n    \n    # PARD Configuration (Section III.C)\n    n_teachers: int = 4\n    distill_temperature: float = 3.0\n    distill_beta: float = 0.1  # Progressive schedule parameter\n    distill_epochs: int = 20\n    \n    # Hierarchical RL (Section III.D)\n    rl_learning_rate_high: float = 5e-4\n    rl_learning_rate_low: float = 1e-3\n    rl_gamma: float = 0.99\n    rl_c51_atoms: int = 51\n    rl_v_min: float = -10.0\n    rl_v_max: float = 10.0\n    \n    # PAC-Bayes (Section III.E)\n    pac_bayes_prior_variance: float = 1.0\n    pac_bayes_delta: float = 0.05\n    \n    # Poisoning Attack Parameters (Section V.A.2)\n    poison_epsilon: float = 0.1\n    poison_tau: float = 0.05  # Temporal coherence\n    poison_rate: float = 0.2\n    poison_iterations: int = 10\n    \n    # Memory Management\n    use_mixed_precision: bool = True\n    memory_threshold: float = 0.8\n    checkpoint_interval: int = 5\n    \n    # Evaluation\n    n_metrics: int = 23  # Comprehensive evaluation framework\n    cross_validation_folds: int = 5\n    test_size: float = 0.2\n    val_size: float = 0.2\n    random_seed: int = 42\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# MEMORY MANAGEMENT\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass MemoryManager:\n    \"\"\"Advanced GPU memory management for efficient training\"\"\"\n    \n    def __init__(self, device: torch.device, threshold: float = 0.8):\n        self.device = device\n        self.threshold = threshold\n        self.memory_stats = defaultdict(list)\n        \n    def get_memory_info(self) -> Dict[str, float]:\n        \"\"\"Get current GPU memory statistics\"\"\"\n        if not torch.cuda.is_available():\n            return {}\n            \n        return {\n            'allocated_gb': torch.cuda.memory_allocated(self.device) / 1e9,\n            'reserved_gb': torch.cuda.memory_reserved(self.device) / 1e9,\n            'max_allocated_gb': torch.cuda.max_memory_allocated(self.device) / 1e9,\n            'free_gb': (torch.cuda.get_device_properties(self.device).total_memory - \n                       torch.cuda.memory_allocated(self.device)) / 1e9\n        }\n    \n    def clear_cache(self):\n        \"\"\"Aggressive memory clearing\"\"\"\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize(self.device)\n    \n    def check_memory(self) -> bool:\n        \"\"\"Check if memory usage is below threshold\"\"\"\n        info = self.get_memory_info()\n        if not info:\n            return True\n            \n        total_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9\n        usage_ratio = info['allocated_gb'] / total_memory\n        \n        if usage_ratio > self.threshold:\n            self.clear_cache()\n            return False\n        return True\n    \n    def optimize_batch_size(self, model: nn.Module, input_shape: Tuple, \n                          initial_batch: int = 64) -> int:\n        \"\"\"Find optimal batch size for available memory\"\"\"\n        batch_size = initial_batch\n        model.eval()\n        \n        while batch_size > 1:\n            try:\n                self.clear_cache()\n                dummy_input = torch.randn(batch_size, *input_shape).to(self.device)\n                with torch.no_grad():\n                    _ = model(dummy_input)\n                \n                info = self.get_memory_info()\n                if info and info['allocated_gb'] / torch.cuda.get_device_properties(self.device).total_memory < 0.7:\n                    return batch_size\n                    \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    self.clear_cache()\n                    \n            batch_size = batch_size // 2\n            \n        return max(batch_size, 1)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# SELECTIVE STATE SPACE MODEL (MAMBA) - Algorithm 1 from Paper\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass SelectiveSSM(nn.Module):\n    \"\"\"\n    Selective State Space Model implementation based on Mamba\n    Reference: Section III.B of the paper, Algorithm 1\n    \"\"\"\n    \n    def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, \n                 expand_factor: int = 2, dt_rank: Optional[int] = None):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand_factor\n        self.d_inner = int(self.expand * d_model)\n        \n        # Compute dt_rank as per paper Eq. 20\n        if dt_rank is None:\n            self.dt_rank = math.ceil(d_model / 16)\n        else:\n            self.dt_rank = dt_rank\n            \n        # Input projection\n        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n        \n        # Convolution as per paper\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            kernel_size=d_conv,\n            padding=d_conv - 1,\n            groups=self.d_inner\n        )\n        \n        # SSM parameters projection\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n        \n        # Initialize dt bias as per paper\n        dt_init_std = self.dt_rank ** -0.5\n        nn.init.uniform_(self.dt_proj.bias, -dt_init_std, dt_init_std)\n        \n        # State matrix A (Equation 23 in paper)\n        A = self._init_state_matrix()\n        self.A_log = nn.Parameter(torch.log(A))\n        \n        # D parameter for skip connection\n        self.D = nn.Parameter(torch.ones(self.d_inner))\n        \n        # Output projection\n        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n        \n    def _init_state_matrix(self) -> torch.Tensor:\n        \"\"\"Initialize state matrix A using HiPPO initialization\"\"\"\n        A = torch.arange(1, self.d_state + 1).reshape(1, -1).repeat(self.d_inner, 1)\n        return A.float()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass implementing Algorithm 1 from the paper\n        Args:\n            x: Input tensor of shape (B, L, D)\n        Returns:\n            Output tensor of shape (B, L, D)\n        \"\"\"\n        batch, seq_len, _ = x.shape\n        \n        # Dual branch projection (Eq. 19)\n        xz = self.in_proj(x)  # (B, L, 2*D_inner)\n        x, z = xz.chunk(2, dim=-1)  # Each (B, L, D_inner)\n        \n        # Convolution branch\n        x = x.transpose(1, 2)  # (B, D_inner, L)\n        x = self.conv1d(x)[:, :, :seq_len]  # Ensure correct length\n        x = x.transpose(1, 2)  # (B, L, D_inner)\n        \n        # Apply SiLU activation\n        x = F.silu(x)\n        \n        # SSM computation\n        y = self.selective_scan(x)\n        \n        # Gating mechanism (Hadamard product)\n        y = y * F.silu(z)\n        \n        # Output projection\n        output = self.out_proj(y)\n        \n        return output\n    \n    def selective_scan(self, u: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Implements the selective scan algorithm (Algorithm 1)\n        This is the core of the Mamba model\n        \"\"\"\n        batch, seq_len, d_inner = u.shape\n        \n        # Compute ∆, B, C from input (Equations 20-22)\n        deltaBC = self.x_proj(u)  # (B, L, dt_rank + 2*d_state)\n        \n        delta, B, C = torch.split(\n            deltaBC, \n            [self.dt_rank, self.d_state, self.d_state], \n            dim=-1\n        )\n        \n        # Transform delta (Equation 20)\n        delta = F.softplus(self.dt_proj(delta))  # (B, L, D_inner)\n        \n        # Get state matrix\n        A = -torch.exp(self.A_log.float())  # (D_inner, d_state)\n        \n        # Discretization (Zero-Order Hold) - Equations 23-24\n        deltaA = torch.exp(delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0))  # (B, L, D_inner, d_state)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, D_inner, d_state)\n        \n        # Selective scan using parallel algorithm\n        y = self.parallel_scan(u, deltaA, deltaB, C)\n        \n        # Add skip connection with D parameter\n        y = y + u * self.D\n        \n        return y\n    \n    def parallel_scan(self, u: torch.Tensor, deltaA: torch.Tensor, \n                     deltaB: torch.Tensor, C: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Parallel scan implementation for O(log L) complexity\n        Reference: Algorithm 1, lines 8-10\n        \"\"\"\n        batch, seq_len, d_inner = u.shape\n        d_state = deltaA.shape[-1]\n        \n        # Initialize output and hidden state\n        y = torch.zeros_like(u)\n        h = torch.zeros(batch, d_inner, d_state, device=u.device, dtype=u.dtype)\n        \n        # Sequential scan (can be optimized with parallel prefix sum)\n        for t in range(seq_len):\n            h = deltaA[:, t] * h + deltaB[:, t] * u[:, t].unsqueeze(-1)\n            y[:, t] = (h * C[:, t].unsqueeze(1)).sum(dim=-1)\n            \n        return y\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# MAMBA BLOCK WITH LAYER NORM AND RESIDUAL\n# ============================================================================\n","metadata":{}},{"cell_type":"code","source":"\nclass MambaBlock(nn.Module):\n    \"\"\"\n    Complete Mamba block with normalization and residual connection\n    Reference: Section III.B of the paper\n    \"\"\"\n    \n    def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4,\n                 expand_factor: int = 2, dropout: float = 0.1):\n        super().__init__()\n        \n        self.norm = nn.LayerNorm(d_model)\n        self.ssm = SelectiveSSM(d_model, d_state, d_conv, expand_factor)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass with residual connection\n        \"\"\"\n        residual = x\n        x = self.norm(x)\n        x = self.ssm(x)\n        x = self.dropout(x)\n        x = x + residual\n        return x\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# MULTI-SCALE TEMPORAL AGGREGATION\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass MultiScaleTemporalAggregation(nn.Module):\n    \"\"\"\n    Multi-scale temporal aggregation module\n    Reference: Section III.B.4, Equations 25-27\n    \"\"\"\n    \n    def __init__(self, d_model: int, n_scales: int = 4):\n        super().__init__()\n        self.n_scales = n_scales\n        self.d_model = d_model\n        \n        # Create SSM for each scale\n        self.scale_ssms = nn.ModuleList([\n            SelectiveSSM(d_model, d_state=16, d_conv=2**(s+1))\n            for s in range(n_scales)\n        ])\n        \n        # Attention for aggregation\n        self.scale_attention = nn.Linear(d_model * n_scales, n_scales)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Process input at multiple temporal scales\n        \"\"\"\n        batch, seq_len, d_model = x.shape\n        \n        # Process at different scales\n        scale_outputs = []\n        for s, ssm in enumerate(self.scale_ssms):\n            # Downsample for higher scales\n            stride = 2 ** s\n            if stride > 1:\n                x_scaled = x[:, ::stride, :]\n            else:\n                x_scaled = x\n                \n            # Process through SSM\n            h_s = ssm(x_scaled)\n            \n            # Upsample back to original length\n            if stride > 1:\n                h_s = F.interpolate(\n                    h_s.transpose(1, 2),\n                    size=seq_len,\n                    mode='linear',\n                    align_corners=False\n                ).transpose(1, 2)\n                \n            scale_outputs.append(h_s)\n        \n        # Concatenate all scales\n        h_concat = torch.cat(scale_outputs, dim=-1)  # (B, L, D*n_scales)\n        \n        # Compute attention weights (Equation 26)\n        alpha = F.softmax(self.scale_attention(h_concat), dim=-1)  # (B, L, n_scales)\n        \n        # Weighted aggregation (Equation 27)\n        h_agg = torch.zeros_like(x)\n        for s in range(self.n_scales):\n            h_agg += alpha[:, :, s:s+1] * scale_outputs[s]\n            \n        return h_agg\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# PROGRESSIVE ADVERSARIAL ROBUSTNESS DISTILLATION (PARD) - Algorithm 2\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass ProgressiveARD(nn.Module):\n    \"\"\"\n    Progressive Adversarial Robustness Distillation\n    Reference: Section III.C, Algorithm 2\n    \"\"\"\n    \n    def __init__(self, student: nn.Module, config: MambaShieldConfig):\n        super().__init__()\n        self.student = student\n        self.config = config\n        self.teachers = nn.ModuleList()\n        self.temperature = config.distill_temperature\n        self.beta = config.distill_beta\n        \n    def create_teacher(self, attack_type: str) -> nn.Module:\n        \"\"\"Create specialized teacher for specific attack type\"\"\"\n        teacher = MambaShieldModel(self.config)\n        # Teacher will be trained separately on specific attack type\n        return teacher\n    \n    def distillation_loss(self, student_logits: torch.Tensor, \n                         teacher_logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute knowledge distillation loss\n        Reference: Equation 17 in related work\n        \"\"\"\n        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)\n        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)\n        \n        loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean')\n        return loss * (self.temperature ** 2)\n    \n    def progressive_schedule(self, epoch: int) -> float:\n        \"\"\"\n        Progressive weighting schedule α(t)\n        Reference: Algorithm 2, line 16\n        \"\"\"\n        return 1 - math.exp(-self.beta * epoch)\n    \n    def forward(self, x: torch.Tensor, labels: torch.Tensor, \n                epoch: int = 0) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass with progressive distillation\n        \"\"\"\n        # Student prediction\n        student_output = self.student(x)\n        \n        if not self.training or len(self.teachers) == 0:\n            return {'logits': student_output, 'distill_loss': torch.tensor(0.0)}\n        \n        # Compute alpha for current epoch\n        alpha = self.progressive_schedule(epoch)\n        \n        # Task loss\n        task_loss = F.cross_entropy(student_output, labels)\n        \n        # Distillation loss from ensemble of teachers\n        distill_loss = 0\n        teacher_confidences = []\n        \n        with torch.no_grad():\n            for teacher in self.teachers:\n                teacher.eval()\n                teacher_logits = teacher(x)\n                \n                # Compute teacher confidence (Algorithm 2, line 21)\n                confidence = torch.max(F.softmax(teacher_logits, dim=-1), dim=-1)[0]\n                teacher_confidences.append(confidence.mean().item())\n                \n                # Weighted distillation loss\n                distill_loss += self.distillation_loss(student_output, teacher_logits)\n        \n        # Average distillation loss\n        if len(self.teachers) > 0:\n            distill_loss /= len(self.teachers)\n        \n        # Combined loss (Algorithm 2, line 25)\n        total_loss = alpha * task_loss + (1 - alpha) * distill_loss\n        \n        return {\n            'logits': student_output,\n            'task_loss': task_loss,\n            'distill_loss': distill_loss,\n            'total_loss': total_loss,\n            'alpha': alpha\n        }\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# HIERARCHICAL REINFORCEMENT LEARNING MODULE\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass HierarchicalRL(nn.Module):\n    \"\"\"\n    Hierarchical Reinforcement Learning for adaptive decision making\n    Reference: Section III.D, Equations 28-30\n    \"\"\"\n    \n    def __init__(self, state_dim: int, config: MambaShieldConfig):\n        super().__init__()\n        self.config = config\n        \n        # High-level strategic policy (Equation 28)\n        self.strategic_policy = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 4)  # {Monitor, Investigate, Block, Adapt}\n        )\n        \n        # Low-level tactical policy (Equation 29)\n        self.tactical_policy = nn.Sequential(\n            nn.Linear(state_dim + 4, 256),  # State + strategic action\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)  # Specific tactical actions\n        )\n        \n        # C51 distributional value estimation (Equation 30)\n        self.n_atoms = config.rl_c51_atoms\n        self.v_min = config.rl_v_min\n        self.v_max = config.rl_v_max\n        self.delta_z = (self.v_max - self.v_min) / (self.n_atoms - 1)\n        self.support = torch.linspace(self.v_min, self.v_max, self.n_atoms)\n        \n        self.value_network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, self.n_atoms)\n        )\n        \n    def forward(self, state: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Hierarchical decision making\n        \"\"\"\n        # Strategic decision\n        strategic_logits = self.strategic_policy(state)\n        strategic_action = F.softmax(strategic_logits, dim=-1)\n        \n        # Tactical decision conditioned on strategic\n        tactical_input = torch.cat([state, strategic_action], dim=-1)\n        tactical_logits = self.tactical_policy(tactical_input)\n        tactical_action = F.softmax(tactical_logits, dim=-1)\n        \n        # Value distribution\n        value_logits = self.value_network(state)\n        value_dist = F.softmax(value_logits, dim=-1)\n        \n        # Expected value\n        if self.support.device != state.device:\n            self.support = self.support.to(state.device)\n        expected_value = (value_dist * self.support).sum(dim=-1)\n        \n        return {\n            'strategic': strategic_action,\n            'tactical': tactical_action,\n            'value_dist': value_dist,\n            'expected_value': expected_value\n        }\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# PAC-BAYES REGULARIZATION\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass PACBayesRegularizer(nn.Module):\n    \"\"\"\n    PAC-Bayes regularization for certified robustness\n    Reference: Section III.E, Theorem 3, Equation 34\n    \"\"\"\n    \n    def __init__(self, model: nn.Module, config: MambaShieldConfig):\n        super().__init__()\n        self.model = model\n        self.prior_variance = config.pac_bayes_prior_variance\n        self.delta = config.pac_bayes_delta\n        \n    def compute_kl_divergence(self) -> torch.Tensor:\n        \"\"\"\n        Compute KL divergence between posterior and prior\n        Reference: Section III.E\n        \"\"\"\n        kl = 0.0\n        for param in self.model.parameters():\n            if param.requires_grad:\n                # Assume Gaussian prior centered at 0\n                kl += 0.5 * torch.sum(\n                    (param ** 2) / self.prior_variance - \n                    torch.log(torch.ones_like(param) * self.prior_variance) - 1\n                )\n        return kl\n    \n    def pac_bayes_bound(self, empirical_risk: torch.Tensor, \n                       n_samples: int) -> Dict[str, float]:\n        \"\"\"\n        Compute PAC-Bayes generalization bound\n        Reference: Theorem 3, Equation 34\n        \"\"\"\n        kl = self.compute_kl_divergence()\n        \n        # McAllester's bound\n        complexity = torch.sqrt(\n            (kl + math.log(2 * math.sqrt(n_samples) / self.delta)) / \n            (2 * n_samples)\n        )\n        \n        bound = empirical_risk + complexity\n        \n        return {\n            'empirical_risk': empirical_risk.item(),\n            'kl_divergence': kl.item(),\n            'complexity': complexity.item(),\n            'bound': bound.item()\n        }\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# COMPLETE MAMBASHIELD MODEL\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass MambaShieldModel(nn.Module):\n    \"\"\"\n    Complete MambaShield architecture\n    Reference: Figure 1 and Section III\n    \"\"\"\n    \n    def __init__(self, config: MambaShieldConfig):\n        super().__init__()\n        self.config = config\n        \n        # Input preprocessing (84 unified features)\n        self.input_proj = nn.Sequential(\n            nn.Linear(config.input_dim, config.hidden_dim),\n            nn.LayerNorm(config.hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(config.dropout)\n        )\n        \n        # Mamba blocks (Section III.B)\n        self.mamba_blocks = nn.ModuleList([\n            MambaBlock(\n                config.hidden_dim, \n                config.d_state, \n                config.d_conv,\n                dropout=config.dropout\n            )\n            for _ in range(config.n_layers)\n        ])\n        \n        # Multi-scale temporal aggregation\n        self.multi_scale = MultiScaleTemporalAggregation(\n            config.hidden_dim, \n            config.n_scales\n        )\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(config.hidden_dim),\n            nn.Linear(config.hidden_dim, config.hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.hidden_dim // 2, 7)  # 7 unified attack categories\n        )\n        \n        # Hierarchical RL component\n        self.rl_module = HierarchicalRL(config.hidden_dim, config)\n        \n    def forward(self, x: torch.Tensor, \n                return_features: bool = False) -> Union[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass through MambaShield\n        Args:\n            x: Input tensor of shape (B, L, 84)\n            return_features: Whether to return intermediate features\n        \"\"\"\n        batch, seq_len, _ = x.shape\n        \n        # Input projection\n        x = self.input_proj(x)\n        \n        # Process through Mamba blocks\n        for mamba_block in self.mamba_blocks:\n            x = mamba_block(x)\n        \n        # Multi-scale aggregation\n        x = self.multi_scale(x)\n        \n        # Use last timestep for classification\n        features = x[:, -1, :]\n        \n        # Classification\n        logits = self.classifier(features)\n        \n        if return_features:\n            # RL decision\n            rl_output = self.rl_module(features)\n            \n            return {\n                'logits': logits,\n                'features': features,\n                'rl_decision': rl_output\n            }\n        \n        return logits\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# POISONING ATTACK IMPLEMENTATIONS\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass PoisoningAttackSimulator:\n    \"\"\"\n    Implementation of poisoning attacks from Section V.A.2\n    \"\"\"\n    \n    def __init__(self, config: MambaShieldConfig):\n        self.config = config\n        self.epsilon = config.poison_epsilon\n        self.tau = config.poison_tau\n        self.rate = config.poison_rate\n        \n    def gradient_based_poisoning(self, model: nn.Module, x: torch.Tensor, \n                                y: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Gradient-based poisoning (GBP)\n        Reference: Equation 37\n        \"\"\"\n        x_adv = x.clone().detach().requires_grad_(True)\n        \n        # Forward pass\n        outputs = model(x_adv)\n        loss = F.cross_entropy(outputs, y)\n        \n        # Compute gradients\n        loss.backward()\n        \n        # Generate adversarial perturbation\n        perturbation = self.epsilon * x_adv.grad.sign()\n        \n        # Apply temporal coherence constraint\n        if len(perturbation.shape) == 3:  # (B, L, D)\n            for t in range(1, perturbation.shape[1]):\n                delta = perturbation[:, t] - perturbation[:, t-1]\n                if torch.norm(delta) > self.tau:\n                    perturbation[:, t] = perturbation[:, t-1] + \\\n                                       self.tau * delta / torch.norm(delta)\n        \n        x_poisoned = x + perturbation\n        return x_poisoned.detach()\n    \n    def label_flipping(self, y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        \"\"\"\n        Label flipping attack\n        Reference: Section V.A.2.2\n        \"\"\"\n        y_flipped = y.clone()\n        n_flip = int(len(y) * self.rate)\n        flip_indices = torch.randperm(len(y))[:n_flip]\n        \n        for idx in flip_indices:\n            current = y_flipped[idx].item()\n            # Flip to different class\n            new_label = random.choice([i for i in range(num_classes) if i != current])\n            y_flipped[idx] = new_label\n            \n        return y_flipped\n    \n    def backdoor_attack(self, x: torch.Tensor, target_class: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Backdoor trigger attack\n        Reference: Equation 38\n        \"\"\"\n        x_backdoor = x.clone()\n        n_poison = int(len(x) * self.rate)\n        poison_indices = torch.randperm(len(x))[:n_poison]\n        \n        # Create trigger pattern (5% of features)\n        trigger = torch.zeros_like(x[0])\n        n_trigger_features = int(0.05 * x.shape[-1])\n        trigger_positions = torch.randperm(x.shape[-1])[:n_trigger_features]\n        trigger[:, trigger_positions] = 1.0\n        \n        # Apply trigger\n        for idx in poison_indices:\n            x_backdoor[idx] = x_backdoor[idx] * 0.95 + trigger * 0.05\n            \n        return x_backdoor, poison_indices\n    \n    def clean_label_poisoning(self, model: nn.Module, x: torch.Tensor, \n                             y: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Clean-label poisoning\n        Reference: Equation 39\n        \"\"\"\n        x_clean = x.clone()\n        n_poison = int(len(x) * self.rate)\n        poison_indices = torch.randperm(len(x))[:n_poison]\n        \n        for idx in poison_indices:\n            x_curr = x[idx:idx+1].clone().requires_grad_(True)\n            y_curr = y[idx:idx+1]\n            \n            # Optimize to change decision boundary while keeping label\n            for _ in range(self.config.poison_iterations):\n                outputs = model(x_curr)\n                loss = -F.cross_entropy(outputs, y_curr)  # Maximize confusion\n                loss.backward()\n                \n                # Update with constraint\n                with torch.no_grad():\n                    perturbation = self.epsilon * x_curr.grad.sign()\n                    x_curr = x_curr + 0.1 * perturbation\n                    x_curr = torch.clamp(x_curr, x[idx:idx+1] - self.epsilon, \n                                       x[idx:idx+1] + self.epsilon)\n                    x_curr.requires_grad_(True)\n                    \n            x_clean[idx] = x_curr.detach()[0]\n            \n        return x_clean\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# COMPREHENSIVE EVALUATION FRAMEWORK (23 METRICS)\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass ComprehensiveEvaluator:\n    \"\"\"\n    23-metric evaluation framework\n    Reference: Section V.E, Table V\n    \"\"\"\n    \n    def __init__(self, config: MambaShieldConfig):\n        self.config = config\n        self.metrics = {}\n        \n    def evaluate(self, model: nn.Module, dataloader: DataLoader, \n                device: torch.device, attack_simulator: Optional[PoisoningAttackSimulator] = None) -> Dict:\n        \"\"\"\n        Comprehensive evaluation with all 23 metrics\n        \"\"\"\n        model.eval()\n        \n        all_preds = []\n        all_labels = []\n        all_probs = []\n        all_features = []\n        inference_times = []\n        \n        with torch.no_grad():\n            for batch_idx, (data, labels) in enumerate(dataloader):\n                data, labels = data.to(device), labels.to(device)\n                \n                # Measure inference time\n                start_time = time.time()\n                outputs = model(data, return_features=True)\n                inference_time = time.time() - start_time\n                inference_times.append(inference_time)\n                \n                if isinstance(outputs, dict):\n                    logits = outputs['logits']\n                    if 'features' in outputs:\n                        all_features.append(outputs['features'].cpu())\n                else:\n                    logits = outputs\n                \n                probs = F.softmax(logits, dim=-1)\n                preds = logits.argmax(dim=-1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n        \n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        all_probs = np.array(all_probs)\n        \n        # 1. Accuracy Metrics\n        self.metrics['accuracy'] = accuracy_score(all_labels, all_preds)\n        self.metrics['balanced_accuracy'] = balanced_accuracy_score(all_labels, all_preds)\n        self.metrics['top_5_accuracy'] = self._top_k_accuracy(all_labels, all_probs, k=5)\n        \n        # 2. Robustness Metrics\n        if attack_simulator:\n            self.metrics['poisoning_resilience'] = self._evaluate_poisoning_resilience(\n                model, dataloader, device, attack_simulator\n            )\n        \n        # 3. Precision, Recall, F1\n        self.metrics['precision'] = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n        self.metrics['recall'] = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n        self.metrics['f1_score'] = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n        \n        # 4. ROC-AUC\n        try:\n            if len(np.unique(all_labels)) == 2:\n                self.metrics['roc_auc'] = roc_auc_score(all_labels, all_probs[:, 1])\n            else:\n                self.metrics['roc_auc'] = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n        except:\n            self.metrics['roc_auc'] = 0.0\n        \n        # 5. Matthews Correlation Coefficient\n        self.metrics['mcc'] = matthews_corrcoef(all_labels, all_preds)\n        \n        # 6. Cohen's Kappa\n        self.metrics['cohen_kappa'] = cohen_kappa_score(all_labels, all_preds)\n        \n        # 7. Confusion Matrix Metrics\n        cm = confusion_matrix(all_labels, all_preds)\n        self._compute_confusion_metrics(cm)\n        \n        # 8. Uncertainty Metrics\n        self._compute_uncertainty_metrics(all_probs)\n        \n        # 9. Calibration Metrics\n        self.metrics['brier_score'] = self._compute_brier_score(all_labels, all_probs)\n        self.metrics['ece'] = self._compute_ece(all_labels, all_preds, all_probs)\n        \n        # 10. Temporal Metrics\n        self.metrics['temporal_consistency'] = self._compute_temporal_consistency(all_preds)\n        \n        # 11. Efficiency Metrics\n        self.metrics['avg_inference_time_ms'] = np.mean(inference_times) * 1000\n        self.metrics['memory_usage_mb'] = self._get_memory_usage()\n        self.metrics['flops'] = self._estimate_flops(model)\n        \n        # 12. Model Complexity\n        self.metrics['num_parameters'] = sum(p.numel() for p in model.parameters())\n        self.metrics['num_trainable_params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        \n        # 13. Feature Quality Metrics\n        if all_features:\n            features = torch.cat(all_features, dim=0).numpy()\n            self.metrics['feature_variance'] = np.mean(np.var(features, axis=0))\n            self.metrics['feature_sparsity'] = np.mean(features == 0)\n        \n        return self.metrics\n    \n    def _top_k_accuracy(self, labels: np.ndarray, probs: np.ndarray, k: int = 5) -> float:\n        \"\"\"Compute top-k accuracy\"\"\"\n        n_classes = probs.shape[1]\n        k = min(k, n_classes)\n        \n        top_k_preds = np.argsort(probs, axis=1)[:, -k:]\n        correct = 0\n        for i, label in enumerate(labels):\n            if label in top_k_preds[i]:\n                correct += 1\n        return correct / len(labels)\n    \n    def _evaluate_poisoning_resilience(self, model: nn.Module, dataloader: DataLoader,\n                                      device: torch.device, attack_simulator: PoisoningAttackSimulator) -> float:\n        \"\"\"Evaluate model resilience to poisoning attacks\"\"\"\n        correct_before = 0\n        correct_after = 0\n        total = 0\n        \n        model.eval()\n        with torch.no_grad():\n            for data, labels in dataloader:\n                data, labels = data.to(device), labels.to(device)\n                \n                # Clean accuracy\n                outputs_clean = model(data)\n                preds_clean = outputs_clean.argmax(dim=-1)\n                correct_before += (preds_clean == labels).sum().item()\n                \n                # Poisoned accuracy\n                data_poisoned = attack_simulator.gradient_based_poisoning(model, data, labels)\n                outputs_poisoned = model(data_poisoned)\n                preds_poisoned = outputs_poisoned.argmax(dim=-1)\n                correct_after += (preds_poisoned == labels).sum().item()\n                \n                total += labels.size(0)\n                \n                if total >= 1000:  # Sample for efficiency\n                    break\n        \n        resilience = correct_after / max(correct_before, 1)\n        return resilience\n    \n    def _compute_confusion_metrics(self, cm: np.ndarray):\n        \"\"\"Compute metrics from confusion matrix\"\"\"\n        # True Positive Rate (Sensitivity) per class\n        tpr_per_class = np.diag(cm) / (cm.sum(axis=1) + 1e-10)\n        self.metrics['mean_tpr'] = np.mean(tpr_per_class)\n        \n        # False Positive Rate\n        fp = cm.sum(axis=0) - np.diag(cm)\n        fn = cm.sum(axis=1) - np.diag(cm)\n        tp = np.diag(cm)\n        tn = cm.sum() - (fp + fn + tp)\n        \n        fpr_per_class = fp / (fp + tn + 1e-10)\n        self.metrics['mean_fpr'] = np.mean(fpr_per_class)\n        \n        # False Negative Rate\n        fnr_per_class = fn / (fn + tp + 1e-10)\n        self.metrics['mean_fnr'] = np.mean(fnr_per_class)\n    \n    def _compute_uncertainty_metrics(self, probs: np.ndarray):\n        \"\"\"Compute uncertainty metrics\"\"\"\n        # Predictive entropy\n        entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n        self.metrics['mean_entropy'] = np.mean(entropy)\n        self.metrics['std_entropy'] = np.std(entropy)\n        \n        # Aleatoric and epistemic uncertainty (simplified)\n        self.metrics['aleatoric_uncertainty'] = np.mean(entropy)\n        self.metrics['epistemic_uncertainty'] = np.std(probs.max(axis=1))\n    \n    def _compute_brier_score(self, labels: np.ndarray, probs: np.ndarray) -> float:\n        \"\"\"Compute Brier score for calibration\"\"\"\n        n_classes = probs.shape[1]\n        brier = 0\n        for i in range(len(labels)):\n            label_onehot = np.zeros(n_classes)\n            label_onehot[labels[i]] = 1\n            brier += np.sum((probs[i] - label_onehot) ** 2)\n        return brier / len(labels)\n    \n    def _compute_ece(self, labels: np.ndarray, preds: np.ndarray, \n                    probs: np.ndarray, n_bins: int = 10) -> float:\n        \"\"\"Expected Calibration Error\"\"\"\n        max_probs = np.max(probs, axis=1)\n        correct = (preds == labels).astype(float)\n        \n        ece = 0\n        for bin_i in range(n_bins):\n            bin_lower = bin_i / n_bins\n            bin_upper = (bin_i + 1) / n_bins\n            \n            in_bin = (max_probs > bin_lower) & (max_probs <= bin_upper)\n            if np.sum(in_bin) > 0:\n                bin_acc = np.mean(correct[in_bin])\n                bin_conf = np.mean(max_probs[in_bin])\n                bin_size = np.sum(in_bin)\n                ece += (bin_size / len(labels)) * abs(bin_acc - bin_conf)\n        \n        return ece\n    \n    def _compute_temporal_consistency(self, preds: np.ndarray) -> float:\n        \"\"\"Compute temporal consistency of predictions\"\"\"\n        if len(preds) < 2:\n            return 1.0\n        \n        consistency = 0\n        for i in range(1, len(preds)):\n            if preds[i] == preds[i-1]:\n                consistency += 1\n        return consistency / (len(preds) - 1)\n    \n    def _get_memory_usage(self) -> float:\n        \"\"\"Get current memory usage in MB\"\"\"\n        if torch.cuda.is_available():\n            return torch.cuda.memory_allocated() / 1e6\n        return 0\n    \n    def _estimate_flops(self, model: nn.Module) -> int:\n        \"\"\"Estimate FLOPs for the model\"\"\"\n        # Simplified estimation\n        total_flops = 0\n        for module in model.modules():\n            if isinstance(module, nn.Linear):\n                total_flops += module.in_features * module.out_features\n            elif isinstance(module, nn.Conv1d):\n                total_flops += (module.in_channels * module.out_channels * \n                              module.kernel_size[0])\n        return total_flops\n    \n    def print_metrics(self):\n        \"\"\"Pretty print all metrics\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"COMPREHENSIVE EVALUATION RESULTS (23 Metrics)\")\n        print(\"=\"*60)\n        \n        categories = {\n            'Accuracy Metrics': ['accuracy', 'balanced_accuracy', 'top_5_accuracy'],\n            'Classification Metrics': ['precision', 'recall', 'f1_score', 'mcc', 'cohen_kappa'],\n            'ROC/AUC Metrics': ['roc_auc'],\n            'Error Rates': ['mean_fpr', 'mean_fnr', 'mean_tpr'],\n            'Calibration Metrics': ['brier_score', 'ece'],\n            'Uncertainty Metrics': ['mean_entropy', 'std_entropy', 'aleatoric_uncertainty', 'epistemic_uncertainty'],\n            'Temporal Metrics': ['temporal_consistency'],\n            'Efficiency Metrics': ['avg_inference_time_ms', 'memory_usage_mb', 'flops'],\n            'Model Complexity': ['num_parameters', 'num_trainable_params'],\n            'Feature Metrics': ['feature_variance', 'feature_sparsity'],\n            'Robustness': ['poisoning_resilience']\n        }\n        \n        for category, metric_names in categories.items():\n            print(f\"\\n{category}:\")\n            print(\"-\" * 40)\n            for metric in metric_names:\n                if metric in self.metrics:\n                    value = self.metrics[metric]\n                    if isinstance(value, float):\n                        print(f\"  {metric:30s}: {value:.4f}\")\n                    else:\n                        print(f\"  {metric:30s}: {value}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# UNIFIED TAXONOMY AND FEATURE PROCESSING\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nclass UnifiedTaxonomy:\n    \"\"\"\n    Unified attack taxonomy across datasets\n    Reference: Section V.A.1\n    \"\"\"\n    \n    def __init__(self):\n        self.taxonomy = {\n            'Normal': ['Normal/Benign', 'BENIGN', 'Benign', 'Normal', '0'],\n            'DoS/DDoS': [\n                'DoS', 'DDoS', 'DDOS-SLOWLORIS', 'DDOS-SYNONYMOUSIP_FLOOD',\n                'DDOS-ICMP_FLOOD', 'DDOS-RSTFINFLOOD', 'DDOS-PSHACK_FLOOD',\n                'DDOS-SYN_FLOOD', 'DDOS-TCP_FLOOD', 'DDOS-UDP_FLOOD',\n                'DOS-UDP_FLOOD', 'DOS-SYN_FLOOD', 'DOS-TCP_FLOOD',\n                'DoS_Hulk', 'DoS_GoldenEye', 'DoS_Slowloris', 'DoS_Slowhttptest'\n            ],\n            'Reconnaissance': [\n                'Scanning', 'RECON-PORTSCAN', 'RECON-OSSCAN', 'RECON-HOSTDISCOVERY',\n                'RECON-PINGSWEEP', 'VULNERABILITYSCAN', 'Heartbleed', 'PortScan',\n                'Reconnaissance', 'Analysis'\n            ],\n            'Malware': [\n                'BACKDOOR_MALWARE', 'Rootkit', 'Trojan', 'Worm', 'Botnet',\n                'Malware', 'Bot', 'Mirai', 'Generic', 'Shellcode'\n            ],\n            'Injection': [\n                'SQL_Injection', 'SQLINJECTION', 'COMMANDINJECTION', 'XSS',\n                'Web Attack', 'Web-based'\n            ],\n            'BruteForce': [\n                'DICTIONARYBRUTEFORCE', 'Brute_Force', 'FTP_Patator',\n                'Password_Attack', 'Brute Force', 'SSH-Patator'\n            ],\n            'Exploitation': [\n                'Infiltration', 'Backdoor', 'Exploits', 'Fuzzers'\n            ]\n        }\n        \n        # Create reverse mapping\n        self.reverse_map = {}\n        for category, attacks in self.taxonomy.items():\n            for attack in attacks:\n                self.reverse_map[attack.lower()] = category\n    \n    def map_label(self, label: str) -> str:\n        \"\"\"Map specific attack to unified category\"\"\"\n        label_lower = str(label).lower().strip()\n        return self.reverse_map.get(label_lower, 'Other')\n\n\nclass UnifiedFeatureProcessor:\n    \"\"\"\n    Process and align features to 84 unified dimensions\n    Reference: Section V.A.1\n    \"\"\"\n    \n    def __init__(self, target_features: int = 84):\n        self.target_features = target_features\n        self.scaler = StandardScaler()\n        self.feature_names = self._define_unified_features()\n    \n    def _define_unified_features(self) -> List[str]:\n        \"\"\"Define 84 unified network flow features\"\"\"\n        return [\n            # Packet statistics (1-10)\n            'flow_duration', 'total_fwd_packets', 'total_bwd_packets',\n            'total_length_fwd_packets', 'total_length_bwd_packets',\n            'fwd_packet_length_max', 'fwd_packet_length_min', \n            'fwd_packet_length_mean', 'fwd_packet_length_std', 'bwd_packet_length_max',\n            \n            # Flow statistics (11-20)\n            'bwd_packet_length_min', 'bwd_packet_length_mean', 'bwd_packet_length_std',\n            'flow_bytes_per_sec', 'flow_packets_per_sec', \n            'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n            'fwd_iat_total',\n            \n            # Inter-arrival times (21-30)\n            'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min',\n            'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std', \n            'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags',\n            \n            # TCP flags (31-42)\n            'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags',\n            'fin_flag_count', 'syn_flag_count', 'rst_flag_count', \n            'psh_flag_count', 'ack_flag_count', 'urg_flag_count', \n            'cwe_flag_count', 'ece_flag_count', 'fwd_header_length',\n            \n            # Header and packet info (43-60)\n            'bwd_header_length', 'fwd_packets_per_sec', 'bwd_packets_per_sec',\n            'min_packet_length', 'max_packet_length', 'packet_length_mean',\n            'packet_length_std', 'packet_length_variance', 'down_up_ratio',\n            'average_packet_size', 'fwd_segment_size_avg', 'bwd_segment_size_avg',\n            'fwd_bulk_rate_avg', 'bwd_bulk_rate_avg', 'subflow_fwd_packets',\n            'subflow_fwd_bytes', 'subflow_bwd_packets', 'subflow_bwd_bytes',\n            \n            # Window and segment features (61-75)\n            'init_win_bytes_forward', 'init_win_bytes_backward',\n            'act_data_pkt_fwd', 'min_seg_size_forward',\n            'active_mean', 'active_std', 'active_max', 'active_min',\n            'idle_mean', 'idle_std', 'idle_max', 'idle_min',\n            'protocol', 'src_port', 'dst_port',\n            \n            # Additional features (76-84)\n            'fwd_bytes_bulk_avg', 'fwd_packet_bulk_avg', 'fwd_bulk_size_avg',\n            'bwd_bytes_bulk_avg', 'bwd_packet_bulk_avg', 'bwd_bulk_size_avg',\n            'fwd_subflow_packets', 'fwd_subflow_bytes', 'bwd_subflow_bytes'\n        ]\n    \n    def process_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Process and align features to unified format\"\"\"\n        processed = np.zeros((len(df), self.target_features))\n        \n        # Map available features\n        for i, feature in enumerate(self.feature_names[:self.target_features]):\n            if feature in df.columns:\n                processed[:, i] = pd.to_numeric(df[feature], errors='coerce').fillna(0).values\n            # Try alternate naming conventions\n            elif feature.replace('_', ' ').title() in df.columns:\n                col = feature.replace('_', ' ').title()\n                processed[:, i] = pd.to_numeric(df[col], errors='coerce').fillna(0).values\n        \n        # Handle infinite values\n        processed = np.nan_to_num(processed, nan=0.0, posinf=1e10, neginf=-1e10)\n        \n        # Clip extreme values\n        processed = np.clip(processed, -1e10, 1e10)\n        \n        return processed\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# DATASET LOADER\n# ============================================================================\n","metadata":{}},{"cell_type":"code","source":"\nclass MambaShieldDataset(Dataset):\n    \"\"\"\n    Custom dataset for MambaShield\n    \"\"\"\n    \n    def __init__(self, features: np.ndarray, labels: np.ndarray, \n                 seq_len: int = 10, transform=None):\n        self.features = features\n        self.labels = labels\n        self.seq_len = seq_len\n        self.transform = transform\n        \n        # Create sequences\n        self.sequences = []\n        self.seq_labels = []\n        \n        for i in range(len(features) - seq_len + 1):\n            self.sequences.append(features[i:i+seq_len])\n            self.seq_labels.append(labels[i+seq_len-1])\n        \n        self.sequences = np.array(self.sequences)\n        self.seq_labels = np.array(self.seq_labels)\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        label = self.seq_labels[idx]\n        \n        if self.transform:\n            sequence = self.transform(sequence)\n        \n        return torch.FloatTensor(sequence), torch.LongTensor([label])[0]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# TRAINING FUNCTIONS\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\ndef train_epoch(model: nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer,\n                criterion: nn.Module, device: torch.device, config: MambaShieldConfig,\n                scaler: GradScaler, attack_simulator: Optional[PoisoningAttackSimulator] = None,\n                epoch: int = 0) -> Dict[str, float]:\n    \"\"\"\n    Train for one epoch with optional poisoning attacks\n    \"\"\"\n    model.train()\n    \n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        data, target = data.to(device), target.to(device)\n        \n        # Simulate poisoning attacks (25% of batches)\n        if attack_simulator and random.random() < 0.25:\n            if random.random() < 0.5:\n                data = attack_simulator.gradient_based_poisoning(model, data, target)\n            else:\n                target = attack_simulator.label_flipping(target, 7)  # 7 unified categories\n        \n        optimizer.zero_grad()\n        \n        # Mixed precision training\n        with autocast():\n            outputs = model(data)\n            loss = criterion(outputs, target)\n        \n        # Backward pass\n        scaler.scale(loss).backward()\n        \n        # Gradient clipping\n        scaler.unscale_(optimizer)\n        clip_grad_norm_(model.parameters(), config.gradient_clip)\n        \n        # Optimizer step\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # Metrics\n        total_loss += loss.item()\n        pred = outputs.argmax(dim=1)\n        correct += (pred == target).sum().item()\n        total += target.size(0)\n    \n    metrics = {\n        'loss': total_loss / len(dataloader),\n        'accuracy': correct / total\n    }\n    \n    return metrics\n\n\ndef validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n            device: torch.device) -> Dict[str, float]:\n    \"\"\"\n    Validate model\n    \"\"\"\n    model.eval()\n    \n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in dataloader:\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            loss = criterion(outputs, target)\n            \n            total_loss += loss.item()\n            pred = outputs.argmax(dim=1)\n            correct += (pred == target).sum().item()\n            total += target.size(0)\n    \n    metrics = {\n        'loss': total_loss / len(dataloader),\n        'accuracy': correct / total\n    }\n    \n    return metrics\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================================================\n# MAIN TRAINING PIPELINE\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"\ndef train_mambashield(config: MambaShieldConfig, \n                     train_data: Tuple[np.ndarray, np.ndarray],\n                     val_data: Tuple[np.ndarray, np.ndarray],\n                     test_data: Tuple[np.ndarray, np.ndarray]) -> Dict:\n    \"\"\"\n    Complete training pipeline for MambaShield\n    Reference: Algorithm 2 and Section V\n    \"\"\"\n    \n    print(\"=\"*60)\n    print(\"MAMBASHIELD TRAINING PIPELINE\")\n    print(\"Citation: Anaedevha et al., IEEE TAI 2025\")\n    print(\"=\"*60)\n    \n    # Device setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Memory manager\n    memory_manager = MemoryManager(device)\n    memory_manager.clear_cache()\n    \n    # Create datasets\n    train_dataset = MambaShieldDataset(train_data[0], train_data[1], seq_len=10)\n    val_dataset = MambaShieldDataset(val_data[0], val_data[1], seq_len=10)\n    test_dataset = MambaShieldDataset(test_data[0], test_data[1], seq_len=10)\n    \n    # Dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, \n                            shuffle=True, num_workers=0, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, \n                          shuffle=False, num_workers=0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, \n                           shuffle=False, num_workers=0, pin_memory=True)\n    \n    # Initialize model\n    model = MambaShieldModel(config).to(device)\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n    \n    # Optimizer and scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, \n                           weight_decay=config.weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n    criterion = nn.CrossEntropyLoss()\n    scaler = GradScaler()\n    \n    # Initialize components\n    attack_simulator = PoisoningAttackSimulator(config)\n    evaluator = ComprehensiveEvaluator(config)\n    pac_bayes = PACBayesRegularizer(model, config)\n    \n    # Progressive ARD setup\n    pard = ProgressiveARD(model, config)\n    \n    # Training history\n    history = defaultdict(list)\n    best_val_acc = 0\n    best_model_state = None\n    \n    # Training loop\n    print(\"\\nStarting training...\")\n    for epoch in range(config.epochs):\n        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n        print(\"-\" * 40)\n        \n        # Train\n        train_metrics = train_epoch(\n            model, train_loader, optimizer, criterion, \n            device, config, scaler, attack_simulator, epoch\n        )\n        \n        # Validate\n        val_metrics = validate(model, val_loader, criterion, device)\n        \n        # Update learning rate\n        scheduler.step()\n        \n        # Save history\n        for key, value in train_metrics.items():\n            history[f'train_{key}'].append(value)\n        for key, value in val_metrics.items():\n            history[f'val_{key}'].append(value)\n        \n        # Print metrics\n        print(f\"Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.4f}\")\n        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n        \n        # Save best model\n        if val_metrics['accuracy'] > best_val_acc:\n            best_val_acc = val_metrics['accuracy']\n            best_model_state = model.state_dict()\n            print(f\"New best model! Val Acc: {best_val_acc:.4f}\")\n        \n        # Memory management\n        if epoch % 5 == 0:\n            memory_manager.clear_cache()\n            mem_info = memory_manager.get_memory_info()\n            if mem_info:\n                print(f\"GPU Memory: {mem_info['allocated_gb']:.2f}GB / {mem_info['free_gb']:.2f}GB free\")\n    \n    # Load best model\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    \n    # Final evaluation\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL EVALUATION ON TEST SET\")\n    print(\"=\"*60)\n    \n    final_metrics = evaluator.evaluate(model, test_loader, device, attack_simulator)\n    evaluator.print_metrics()\n    \n    # PAC-Bayes bounds\n    test_loss = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            test_loss += criterion(outputs, target).item()\n    test_loss /= len(test_loader)\n    \n    pac_bounds = pac_bayes.pac_bayes_bound(torch.tensor(test_loss), len(test_dataset))\n    \n    print(\"\\nPAC-Bayes Bounds (Theorem 3):\")\n    for key, value in pac_bounds.items():\n        print(f\"  {key}: {value:.4f}\")\n    \n    # Plot results\n    plot_training_history(history)\n    \n    return {\n        'model': model,\n        'history': history,\n        'final_metrics': final_metrics,\n        'pac_bounds': pac_bounds\n    }\n\n\ndef plot_training_history(history: Dict):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Loss plot\n    axes[0].plot(history['train_loss'], label='Train')\n    axes[0].plot(history['val_loss'], label='Validation')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Accuracy plot\n    axes[1].plot(history['train_accuracy'], label='Train')\n    axes[1].plot(history['val_accuracy'], label='Validation')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training Accuracy')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================================\n# EXAMPLE USAGE\n# ============================================================================","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"MambaShield: Complete Implementation\")\n    print(\"Paper: IEEE TAI 2025\")\n    print(\"=\"*60)\n    \n    # Initialize configuration\n    config = MambaShieldConfig()\n    \n    # Set random seeds for reproducibility\n    torch.manual_seed(config.random_seed)\n    np.random.seed(config.random_seed)\n    random.seed(config.random_seed)\n    \n    # Example: Load and process data\n    # Note: Replace with actual dataset paths\n    print(\"\\n1. Loading datasets...\")\n    \n    # Simulated data for demonstration\n    # In practice, load CIC-IoT-2023, CSE-CICIDS2018, UNSW-NB15\n    n_samples = 10000\n    X = np.random.randn(n_samples, 84)  # 84 unified features\n    y = np.random.randint(0, 7, n_samples)  # 7 attack categories\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=config.test_size, random_state=config.random_seed\n    )\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=config.val_size, random_state=config.random_seed\n    )\n    \n    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n    \n    # Normalize features\n    feature_processor = UnifiedFeatureProcessor()\n    X_train = feature_processor.scaler.fit_transform(X_train)\n    X_val = feature_processor.scaler.transform(X_val)\n    X_test = feature_processor.scaler.transform(X_test)\n    \n    # Train model\n    print(\"\\n2. Training MambaShield...\")\n    results = train_mambashield(\n        config,\n        train_data=(X_train, y_train),\n        val_data=(X_val, y_val),\n        test_data=(X_test, y_test)\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE!\")\n    print(f\"Final Test Accuracy: {results['final_metrics']['accuracy']:.4f}\")\n    print(f\"Poisoning Resilience: {results['final_metrics'].get('poisoning_resilience', 0):.4f}\")\n    print(\"=\"*60) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}