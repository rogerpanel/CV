\chapter{Implementation Details and Hyperparameters}
\label{app:implementation}

This appendix provides comprehensive implementation details, hyperparameter configurations, and computational infrastructure specifications to facilitate reproducibility.

\section{Software Framework and Dependencies}

All implementations utilize Python 3.10 with the following core dependencies:

\textbf{Deep Learning Frameworks:}
\begin{itemize}
\item PyTorch 2.0.1 with CUDA 11.8 support
\item TensorFlow 2.13.0 for specific baseline comparisons
\item PyTorch Lightning 2.0.5 for training orchestration
\end{itemize}

\textbf{Scientific Computing:}
\begin{itemize}
\item NumPy 1.24.3 for numerical operations
\item SciPy 1.11.1 for optimization and special functions
\item Pandas 2.0.3 for data manipulation
\item Scikit-learn 1.3.0 for preprocessing and metrics
\end{itemize}

\textbf{ODE Solvers:}
\begin{itemize}
\item torchdiffeq 0.2.3 for Neural ODE integration
\item DifferentialEquations.jl interfaced via PyJulia for specialized solvers
\end{itemize}

\textbf{Optimal Transport:}
\begin{itemize}
\item POT (Python Optimal Transport) 0.9.1
\item GeomLoss 0.2.6 for Sinkhorn divergences
\end{itemize}

\textbf{Privacy and Security:}
\begin{itemize}
\item Opacus 1.4.0 for differential privacy
\item PySyft 0.8.2 for federated learning
\end{itemize}

\section{Hyperparameter Configurations}

\subsection{Temporal Adaptive Neural ODE}

\textbf{Architecture:}
\begin{itemize}
\item Input dimension: 64 (after feature extraction)
\item Hidden dimensions: [128, 256, 256, 128]
\item ODE function: 3-layer MLP with hidden dim 256
\item Integration interval: $[0, 1]$
\item ODE solver: Dopri5 with adaptive step size
\item Relative tolerance: $10^{-3}$
\item Absolute tolerance: $10^{-4}$
\end{itemize}

\textbf{Temporal Adaptive Batch Normalization:}
\begin{itemize}
\item Time embedding dimension: 32
\item Time encoder: 2-layer MLP [32, 64, 32]
\item Normalization momentum: 0.1
\item Epsilon for numerical stability: $10^{-5}$
\end{itemize}

\textbf{Point Process Model:}
\begin{itemize}
\item Transformer layers: 4
\item Attention heads: 8
\item Hidden dimension: 256
\item Feed-forward dimension: 1024
\item Dropout rate: 0.1
\item Log-barrier coefficient: 0.01
\end{itemize}

\textbf{Training:}
\begin{itemize}
\item Optimizer: AdamW
\item Learning rate: $3 \times 10^{-4}$ with cosine annealing
\item Weight decay: $10^{-5}$
\item Batch size: 128
\item Maximum epochs: 100
\item Early stopping patience: 15 epochs
\item Gradient clipping: max norm 1.0
\end{itemize}

\subsection{Privacy-Preserving Optimal Transport}

\textbf{Optimal Transport:}
\begin{itemize}
\item Sinkhorn regularization $\lambda$: 0.1 (initial), adaptive scheduling
\item Sinkhorn iterations: maximum 100, convergence threshold $10^{-6}$
\item Importance sparsification: keep top 20\% of couplings
\item Ground cost metric: Euclidean distance
\end{itemize}

\textbf{Differential Privacy:}
\begin{itemize}
\item Privacy budget $\epsilon$: 0.85
\item Failure probability $\delta$: $10^{-5}$
\item Noise mechanism: Gaussian
\item Clipping norm: 1.0 for gradients
\end{itemize}

\textbf{Federated Learning:}
\begin{itemize}
\item Number of clients: 10-50 (varies by experiment)
\item Client sampling fraction: 0.3 per round
\item Local epochs: 5
\item Local batch size: 64
\item Global rounds: 200
\item Byzantine trimming fraction $\beta$: 0.25
\end{itemize}

\subsection{Encrypted Traffic Analysis}

\textbf{CNN Architecture:}
\begin{itemize}
\item Conv layers: [64, 128, 256] filters
\item Kernel sizes: [5, 3, 3]
\item MaxPool: size 2 after each conv layer
\item Activation: ReLU
\item Dropout: 0.3
\end{itemize}

\textbf{LSTM Architecture:}
\begin{itemize}
\item Bidirectional: Yes
\item Hidden size: 128
\item Layers: 2
\item Dropout: 0.2
\end{itemize}

\textbf{Transformer Architecture:}
\begin{itemize}
\item Embedding dimension: 256
\item Encoder layers: 6
\item Attention heads: 8
\item Feed-forward dimension: 2048
\item Positional encoding: sinusoidal
\item Dropout: 0.1
\end{itemize}

\subsection{Graph Neural Networks}

\textbf{Heterogeneous Graph Pooling:}
\begin{itemize}
\item Message passing layers: 3
\item Hidden dimension: 256
\item Relation-specific transformations: Yes
\item Pooling ratio: 0.2
\item Attention heads (pooling): 4
\end{itemize}

\textbf{Temporal Graph Model:}
\begin{itemize}
\item GRU hidden size: 256
\item Temporal snapshots: 10
\item Snapshot interval: 5 minutes
\item Temporal attention heads: 4
\end{itemize}

\textbf{Continuous-Time Temporal Graph Neural Network (CT-TGNN):}
\begin{itemize}
\item Graph ODE hidden dimension: 256
\item Time constants $\tau_s$: $\{10^{-6}, 10^{-3}, 1, 3600\}$ seconds
\item ODE solver: Dopri5 with adaptive step size
\item Encrypted edge feature dimension: 128
\item Attention heads for edge weighting: 8
\item Integration tolerance: $10^{-4}$
\end{itemize}

\textbf{Triple-Embedding Temporal Graph Neural Network (TripleE-TGNN):}
\begin{itemize}
\item Service-level embedding dimension: 128
\item Trace-level embedding dimension: 256
\item Node-level embedding dimension: 128
\item BiLSTM hidden size (trace paths): 256
\item Cross-granularity attention heads: 8
\item GRU hidden size (temporal dynamics): 256
\end{itemize}

\section{Novel Algorithm Pseudocode}

This section provides formal algorithm pseudocode for the major novel methods developed in this dissertation.

\begin{algorithm}
\caption{CT-TGNN Training for Encrypted Traffic}
\label{alg:cttgnn_training}
\begin{algorithmic}[1]
\REQUIRE Temporal graph sequence $\{\mathcal{G}^{(t)}\}_{t=1}^T$, encrypted edge features $\{\mathbf{E}^{(t)}\}$, labels $\mathbf{y}$
\REQUIRE Learning rate $\eta$, ODE tolerance $\epsilon_{ode}$, time constants $\{\tau_s\}_{s=1}^S$
\ENSURE Trained parameters $\theta^*$
\STATE Initialize node embeddings $\mathbf{H}(0)$, parameters $\theta$
\FOR{epoch $= 1$ to $N_{epochs}$}
    \FOR{each time interval $[t_i, t_{i+1}]$}
        \STATE // Multi-scale continuous graph dynamics
        \FOR{scale $s = 1$ to $S$}
            \STATE $\mathbf{H}_s(t_i) \leftarrow \text{ScaleEncoder}_s(\mathbf{H}(t_i))$
            \STATE Solve $\frac{d\mathbf{H}_s(\tau)}{d\tau} = \frac{1}{\tau_s}\text{GNODE}_s(\mathbf{H}_s(\tau), \mathcal{A}(\tau), \tau)$
            \STATE \quad for $\tau \in [t_i, t_{i+1}]$ using Dopri5 with tolerance $\epsilon_{ode}$
        \ENDFOR
        \STATE // Compute attention weights over scales
        \STATE $\beta_s(t_{i+1}) \leftarrow \text{Softmax}(\mathbf{w}_s^T \mathbf{H}_s(t_{i+1}))$ for all $s$
        \STATE $\mathbf{H}(t_{i+1}) \leftarrow \sum_{s=1}^S \beta_s(t_{i+1}) \mathbf{H}_s(t_{i+1})$
        \STATE // Discrete event update (if event occurs at $t_{i+1}$)
        \IF{event at $t_{i+1}$}
            \STATE $\Delta\mathbf{H} \leftarrow g_\psi(\mathbf{H}(t_{i+1}^-), \mathbf{x}_{event}, k_{event})$
            \STATE $\mathbf{H}(t_{i+1}^+) \leftarrow \mathbf{H}(t_{i+1}^-) + \Delta\mathbf{H}$
        \ENDIF
    \ENDFOR
    \STATE // Compute predictions and loss
    \STATE $\hat{\mathbf{y}} \leftarrow \text{Classifier}(\mathbf{H}(T))$
    \STATE $\mathcal{L} \leftarrow \text{CrossEntropy}(\hat{\mathbf{y}}, \mathbf{y}) + \lambda_{reg}\|\theta\|_2^2$
    \STATE // Backpropagation through adjoint method
    \STATE $\nabla_\theta \mathcal{L} \leftarrow \text{AdjointBackward}(\mathcal{L}, \theta)$
    \STATE $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}$
\ENDFOR
\RETURN $\theta^*$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{TripleE-TGNN Multi-Granularity Inference}
\label{alg:triplee_inference}
\begin{algorithmic}[1]
\REQUIRE Microservices graph $\mathcal{G}(t) = (\mathcal{V}_S, \mathcal{V}_T, \mathcal{V}_P, \mathcal{E})$
\REQUIRE Service features $\{\mathbf{x}_s\}$, trace features $\{\mathbf{x}_\tau\}$, pod features $\{\mathbf{x}_p\}$
\REQUIRE Trained parameters $\theta^*$
\ENSURE Threat prediction $\hat{y}$ and granularity-specific scores
\STATE // Service-level embedding
\FOR{each service $s \in \mathcal{V}_S$}
    \STATE $\mathbf{x}_s^{agg} \leftarrow \text{AggregateMetrics}(\mathbf{x}_s)$
    \STATE $\mathbf{h}_s^{service} \leftarrow \text{GNN}_{service}(\mathbf{x}_s^{agg}, \mathcal{G}_S)$
\ENDFOR
\STATE // Trace-level embedding
\FOR{each trace $\tau \in \mathcal{V}_T$}
    \STATE Extract service path $(s_1, s_2, \ldots, s_k)$ from trace $\tau$
    \STATE $\mathbf{h}_\tau^{path} \leftarrow \text{BiLSTM}(\mathbf{e}_{s_1}, \ldots, \mathbf{e}_{s_k})$
    \STATE $\{\alpha_i\} \leftarrow \text{Attention}(\{\mathbf{h}_i^{span}\}, \mathbf{h}_\tau^{context})$
    \STATE $\mathbf{h}_\tau^{trace} \leftarrow \mathbf{h}_\tau^{path} \oplus \sum_i \alpha_i \mathbf{h}_i^{span}$
\ENDFOR
\STATE // Node-level embedding
\FOR{each pod $p \in \mathcal{V}_P$}
    \STATE $\mathbf{x}_p^{combined} \leftarrow [\mathbf{x}_p^{resource}; \mathbf{x}_p^{syscall}]$
    \STATE $\mathbf{h}_p^{node} \leftarrow \text{GNN}_{node}(\mathbf{x}_p^{combined}, \mathcal{G}_P)$
\ENDFOR
\STATE // Cross-granularity attention fusion
\FOR{each service $s$}
    \STATE Find connected traces $\mathcal{T}_s$ and pods $\mathcal{P}_s$
    \STATE $\mathbf{h}_s^{trace\rightarrow service} \leftarrow \text{Attention}(\mathbf{h}_s^{service}, \{\mathbf{h}_\tau^{trace}\}_{\tau \in \mathcal{T}_s})$
    \STATE $\mathbf{h}_s^{node\rightarrow service} \leftarrow \text{Attention}(\mathbf{h}_s^{service}, \{\mathbf{h}_p^{node}\}_{p \in \mathcal{P}_s})$
    \STATE $\mathbf{h}_s^{fused} \leftarrow [\mathbf{h}_s^{service}; \mathbf{h}_s^{trace\rightarrow service}; \mathbf{h}_s^{node\rightarrow service}]$
\ENDFOR
\STATE // Hierarchical pooling
\STATE $\mathbf{h}_{graph}^{service} \leftarrow \text{MeanPool}(\{\mathbf{h}_s^{service}\})$
\STATE $\mathbf{h}_{graph}^{trace} \leftarrow \text{MaxPool}(\{\mathbf{h}_\tau^{trace}\})$
\STATE $\mathbf{h}_{graph}^{node} \leftarrow \text{AttentionPool}(\{\mathbf{h}_p^{node}\})$
\STATE // Final classification
\STATE $\mathbf{h}_{graph} \leftarrow [\mathbf{h}_{graph}^{service}; \mathbf{h}_{graph}^{trace}; \mathbf{h}_{graph}^{node}]$
\STATE $\hat{y} \leftarrow \text{MLP}_{classifier}(\mathbf{h}_{graph})$
\RETURN $\hat{y}$, $\{\mathbf{h}_s^{service}\}$, $\{\mathbf{h}_\tau^{trace}\}$, $\{\mathbf{h}_p^{node}\}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Graph Neural ODE Forward Pass}
\label{alg:gnode_forward}
\begin{algorithmic}[1]
\REQUIRE Initial node embeddings $\mathbf{H}(t_0)$, adjacency matrix $\mathcal{A}$
\REQUIRE Integration interval $[t_0, t_1]$, ODE solver parameters
\ENSURE Final node embeddings $\mathbf{H}(t_1)$
\STATE Define ODE dynamics function:
\STATE \quad $f_{ode}(\mathbf{H}, t) = \text{GraphConv}(\text{TA-BN}(\mathbf{H}, t), \mathcal{A})$
\STATE where:
\STATE \quad $\text{TA-BN}(\mathbf{H}, t) = \gamma(t) \odot \frac{\mathbf{H} - \mu(t)}{\sqrt{\sigma^2(t) + \epsilon}} + \beta(t)$
\STATE \quad $\text{GraphConv}(\mathbf{H}, \mathcal{A}) = \sigma(\tilde{\mathcal{D}}^{-1/2}\tilde{\mathcal{A}}\tilde{\mathcal{D}}^{-1/2}\mathbf{H}\mathbf{W})$
\STATE // Integrate ODE using adaptive solver
\STATE $\mathbf{H}(t_1) \leftarrow \text{ODESolve}(f_{ode}, \mathbf{H}(t_0), [t_0, t_1])$
\STATE // ODESolve uses Dormand-Prince (Dopri5) method with:
\STATE \quad Relative tolerance: $rtol = 10^{-3}$
\STATE \quad Absolute tolerance: $atol = 10^{-4}$
\STATE \quad Adaptive step size control
\RETURN $\mathbf{H}(t_1)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Privacy-Preserving Federated Optimal Transport}
\label{alg:ppfot}
\begin{algorithmic}[1]
\REQUIRE $K$ clients with local data $\{\mathcal{D}_k\}_{k=1}^K$, target data $\mathcal{D}_T$
\REQUIRE Privacy budget $\epsilon$, failure probability $\delta$, Byzantine fraction $\beta$
\ENSURE Global adapted model $f_T$
\STATE Initialize global model $f_0$
\FOR{round $r = 1$ to $R$}
    \STATE Server samples client subset $\mathcal{S}_r \subseteq [K]$ with $|\mathcal{S}_r| = C$
    \STATE Server broadcasts $f_{r-1}$ to clients in $\mathcal{S}_r$
    \STATE // Parallel client updates
    \FOR{each client $k \in \mathcal{S}_r$ in parallel}
        \STATE // Compute local-to-target optimal transport
        \STATE Sample local features $\{\mathbf{x}_i^{(k)}\}_{i=1}^{n_k}$, target features $\{\mathbf{y}_j^{(T)}\}_{j=1}^{n_T}$
        \STATE Compute cost matrix $\mathbf{C}_{ij}^{(k)} = \|\mathbf{x}_i^{(k)} - \mathbf{y}_j^{(T)}\|_2^2$
        \STATE // Differentially private Sinkhorn
        \STATE $\tilde{\mathbf{C}}^{(k)} \leftarrow \mathbf{C}^{(k)} + \text{GaussianNoise}(\sigma_{DP})$
        \STATE $\boldsymbol{\gamma}^{(k)} \leftarrow \text{Sinkhorn}(\tilde{\mathbf{C}}^{(k)}, \lambda, \epsilon_{conv})$
        \STATE // Transport-based domain adaptation
        \STATE $\tilde{\mathbf{x}}_i^{(k)} \leftarrow \sum_j \gamma_{ij}^{(k)} \mathbf{y}_j^{(T)}$ for all $i$
        \STATE // Local model training on transported features
        \STATE $f_k^{(r)} \leftarrow \text{Train}(f_{r-1}, \{(\tilde{\mathbf{x}}_i^{(k)}, y_i^{(k)})\}, E_{local})$
        \STATE // Differential privacy for model update
        \STATE $\Delta_k^{(r)} \leftarrow f_k^{(r)} - f_{r-1}$
        \STATE Clip: $\tilde{\Delta}_k^{(r)} \leftarrow \Delta_k^{(r)} / \max(1, \|\Delta_k^{(r)}\|_2 / C_{clip})$
        \STATE Add noise: $\hat{\Delta}_k^{(r)} \leftarrow \tilde{\Delta}_k^{(r)} + \mathcal{N}(0, \sigma_{DP}^2 C_{clip}^2 \mathbf{I})$
        \STATE Send $\hat{\Delta}_k^{(r)}$ to server
    \ENDFOR
    \STATE // Byzantine-robust aggregation at server
    \STATE Collect $\{\hat{\Delta}_k^{(r)}\}_{k \in \mathcal{S}_r}$
    \STATE Compute Wasserstein distances: $d_k \leftarrow W_2(\hat{\Delta}_k^{(r)}, \bar{\Delta}^{(r)})$
    \STATE Identify outliers: $\mathcal{O} \leftarrow \{k : d_k > \text{percentile}(\{d_j\}, 100(1-\beta))\}$
    \STATE Remove outliers: $\mathcal{S}_r' \leftarrow \mathcal{S}_r \setminus \mathcal{O}$
    \STATE Trimmed mean: $\bar{\Delta}^{(r)} \leftarrow \frac{1}{|\mathcal{S}_r'|}\sum_{k \in \mathcal{S}_r'} \hat{\Delta}_k^{(r)}$
    \STATE // Update global model
    \STATE $f_r \leftarrow f_{r-1} + \bar{\Delta}^{(r)}$
\ENDFOR
\RETURN $f_T \leftarrow f_R$
\end{algorithmic}
\end{algorithm}

\section{Data Preprocessing}

\subsection{Feature Engineering}

\textbf{Network Flow Features:}
\begin{itemize}
\item Packet count (forward/backward)
\item Byte count (forward/backward)
\item Duration
\item Inter-arrival time statistics (mean, std, min, max)
\item Packet size statistics
\item Flags distribution (SYN, ACK, FIN, RST, etc.)
\item Protocol type (one-hot encoded)
\item Port numbers (embedded)
\end{itemize}

\textbf{Normalization:}
\begin{itemize}
\item Numerical features: StandardScaler (zero mean, unit variance)
\item Categorical features: One-hot encoding or learned embeddings
\item Temporal features: Min-max scaling to [0, 1]
\item Outlier handling: Winsorization at 1st and 99th percentiles
\end{itemize}

\subsection{Data Augmentation}

\textbf{Temporal Jittering:}
\begin{itemize}
\item Random time shifts: $\pm$ 10\% of event timestamps
\item Probability: 0.3
\end{itemize}

\textbf{Feature Noise Injection:}
\begin{itemize}
\item Gaussian noise: $\mathcal{N}(0, 0.01 \sigma_{feature})$
\item Probability: 0.2
\end{itemize}

\textbf{Sequence Augmentation:}
\begin{itemize}
\item Random cropping: 80-100\% of original length
\item Probability: 0.4
\end{itemize}

\section{Computational Infrastructure}

\subsection{Training Infrastructure}

\textbf{Hardware Configuration:}
\begin{itemize}
\item GPUs: 4× NVIDIA A100 (40GB) for main experiments
\item CPUs: 2× AMD EPYC 7742 (64 cores each)
\item RAM: 512 GB DDR4
\item Storage: 10 TB NVMe SSD for datasets
\item Network: 10 Gbps Ethernet for distributed training
\end{itemize}

\textbf{Distributed Training:}
\begin{itemize}
\item Strategy: PyTorch Distributed Data Parallel (DDP)
\item Mixed precision: FP16 with automatic mixed precision (AMP)
\item Gradient accumulation steps: 4 (effective batch size 512)
\item Communication backend: NCCL for GPU-GPU communication
\end{itemize}

\subsection{Inference Optimization}

\textbf{Model Optimization:}
\begin{itemize}
\item Quantization: INT8 for deployment models
\item Pruning: 30\% structured pruning of fully connected layers
\item Knowledge distillation: 10× compression for edge deployment
\end{itemize}

\textbf{Deployment Configuration:}
\begin{itemize}
\item Inference framework: ONNX Runtime
\item Batch size: 256 for throughput, 1 for latency-critical
\item Threading: 8 CPU threads for CPU inference
\item GPU: NVIDIA T4 (16GB) for production deployment
\end{itemize}

\section{Evaluation Protocols}

\subsection{Cross-Validation Strategy}

\textbf{Temporal Split:}
\begin{itemize}
\item Training: First 70\% of time period
\item Validation: Next 15\% of time period
\item Testing: Final 15\% of time period
\item Rationale: Preserves temporal ordering for realistic evaluation
\end{itemize}

\textbf{K-Fold Cross-Validation:}
\begin{itemize}
\item Folds: 5
\item Strategy: Stratified by attack type
\item Used for: Hyperparameter tuning and ablation studies
\end{itemize}

\subsection{Statistical Significance Testing}

\textbf{Paired t-test:}
\begin{itemize}
\item Null hypothesis: No difference between methods
\item Significance level: $\alpha = 0.05$
\item Correction: Bonferroni for multiple comparisons
\item Runs: 5 independent runs with different random seeds
\end{itemize}

\textbf{Bootstrapping:}
\begin{itemize}
\item Samples: 1000 bootstrap samples
\item Confidence intervals: 95\%
\item Metrics: Accuracy, F1-score, AUC-ROC
\end{itemize}

\section{Code and Data Availability}

\textbf{Source Code:}
\begin{itemize}
\item Repository: \url{https://github.com/rogerpanel/CV}
\item Commits: See thesis references for specific versions
\item License: MIT License for research use
\item Documentation: README with installation and usage instructions
\end{itemize}

\textbf{Datasets:}
\begin{itemize}
\item ICS3D: Kaggle DOI 10.34740/kaggle/dsv/12483891
\item Preprocessing scripts: Included in repository
\item License: CC BY-NC-SA 4.0
\end{itemize}

\textbf{Trained Models:}
\begin{itemize}
\item Available: Checkpoints for main experiments
\item Format: PyTorch .pth files
\item Size: Approximately 2-15 GB per model
\end{itemize}
