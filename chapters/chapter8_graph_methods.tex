\chapter{Graph-Based Methods for Network Security}
\label{ch:graph_methods}

\section{Introduction}

Network intrusion detection fundamentally operates on graph-structured data where the relational patterns between entities carry critical information for attack detection. While previous chapters have addressed temporal dynamics, domain adaptation, and privacy preservation, this chapter focuses specifically on leveraging graph neural network architectures to capture network topology, attack propagation patterns, and coordinated threat behaviors across multiple abstraction levels.

Traditional machine learning approaches treat network flows as independent samples, discarding the rich structural information encoded in communication patterns, host relationships, and protocol dependencies. This independence assumption fails catastrophically for attacks that exploit network topology, such as lateral movement in advanced persistent threats, distributed denial of service campaigns, and botnet command-and-control communications.

Modern distributed computing environments—particularly microservices architectures and zero-trust networks—introduce additional complexity through hierarchical structures where security threats manifest at multiple granularities simultaneously. Service-level behaviors, distributed trace patterns, and individual instance dynamics provide complementary perspectives essential for comprehensive threat detection. Furthermore, the proliferation of encrypted traffic at 87\% of enterprise communications renders traditional payload inspection ineffective, requiring graph-based approaches that leverage observable metadata and topology.

This chapter develops advanced graph-based methods specifically designed for contemporary network security challenges. We introduce heterogeneous graph pooling for multi-relational network data, attention mechanisms for identifying critical attack paths, and temporal graph modeling for tracking evolving threat campaigns. Building upon these foundations, we present two major innovations: (1) \textbf{continuous-time temporal graph neural networks} that extend Neural ODEs to graph-structured data, enabling detection of attacks unfolding across irregular time scales in encrypted traffic, and (2) \textbf{multi-granularity graph embeddings} that jointly model service-level, trace-level, and node-level security behaviors in microservices environments through hierarchical representations.

These advances address fundamental limitations of discrete-time graph models that miss inter-snapshot attack dynamics, and single-granularity approaches that fail to capture coordinated threats spanning architectural layers. The continuous-time formulation achieves 98.3\% detection accuracy on encrypted lateral movement with 47 millisecond median latency, while the multi-granularity framework attains 96.8\% accuracy on microservices intrusions, outperforming single-level baselines by 8.3\%.

\section{Graph Representation of Network Security Data}

Network security data naturally forms heterogeneous graphs with multiple node types (hosts, users, processes, files) and edge types (network flows, authentication events, file accesses, process creations). We formalize this structure through attributed heterogeneous graphs.

\subsection{Heterogeneous Graph Formulation}

A heterogeneous network security graph is defined as $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{A}, \mathcal{R}, \phi, \psi)$ where:
\begin{itemize}
\item $\mathcal{V}$ is the set of nodes representing security entities
\item $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of edges representing relationships
\item $\mathcal{A}$ is the set of node types (e.g., host, user, process)
\item $\mathcal{R}$ is the set of edge types (e.g., network flow, authentication, file access)
\item $\phi: \mathcal{V} \rightarrow \mathcal{A}$ maps nodes to types
\item $\psi: \mathcal{E} \rightarrow \mathcal{R}$ maps edges to types
\end{itemize}

Each node $v \in \mathcal{V}$ has associated feature vector $\mathbf{x}_v \in \mathbb{R}^{d_{\phi(v)}}$ where dimensionality depends on node type, and each edge $(u,v) \in \mathcal{E}$ has features $\mathbf{e}_{uv} \in \mathbb{R}^{d_{\psi(u,v)}}$ capturing relationship attributes.

\subsection{Attack Pattern Representation}

Security attacks manifest as characteristic subgraph patterns within the network graph. For instance:
\begin{itemize}
\item \textbf{Lateral movement:} Sequential edges representing authentication and remote access across hosts with privilege escalation
\item \textbf{Data exfiltration:} High-volume flows from internal servers to external destinations with unusual timing patterns
\item \textbf{Command and control:} Periodic communication with suspicious external hosts using non-standard ports
\item \textbf{Distributed attacks:} Coordinated activities from multiple sources targeting common victims
\end{itemize}

The graph-based detection problem reduces to learning a function $f: \mathcal{G} \rightarrow \{0,1\}^{|\mathcal{V}|}$ that assigns labels to nodes, or $f: \mathcal{G} \rightarrow \{0,1\}^{|\mathcal{E}|}$ that labels edges, or $f: \mathcal{G} \rightarrow \{0,1\}$ for graph-level classification.

\section{Heterogeneous Graph Pooling}

Graph neural networks propagate information through message passing, but security graphs often contain millions of nodes making full-graph computation intractable. We develop heterogeneous graph pooling (HGP) that learns to identify and aggregate critical substructures while preserving type-specific information.

\subsection{Type-Aware Message Passing}

Message passing in heterogeneous graphs must account for different node and edge types. For node $v$ of type $\phi(v) = a$, the updated representation is computed as:
\begin{equation}
\mathbf{h}_v^{(l+1)} = \sigma\left(\sum_{r \in \mathcal{R}} \sum_{u \in \mathcal{N}_r(v)} \frac{1}{|\mathcal{N}_r(v)|} \mathbf{W}_r^{(l)} \mathbf{h}_u^{(l)} + \mathbf{W}_{\text{self}}^{(l)} \mathbf{h}_v^{(l)}\right)
\end{equation}
where $\mathcal{N}_r(v)$ denotes neighbors connected via relation type $r$, $\mathbf{W}_r^{(l)}$ is a relation-specific transformation matrix for layer $l$, $\mathbf{W}_{\text{self}}^{(l)}$ enables self-connections, and $\sigma$ is a nonlinear activation function.

\subsection{Attention-Based Pooling}

To identify critical nodes for security analysis, we employ attention mechanisms that learn importance scores:
\begin{equation}
s_v = \sigma\left(\mathbf{w}^T \tanh\left(\mathbf{W}_{\text{pool}} \mathbf{h}_v + \mathbf{b}_{\text{pool}}\right)\right)
\end{equation}
where $s_v \in [0,1]$ represents the importance score for node $v$, $\mathbf{W}_{\text{pool}}$ and $\mathbf{w}$ are learnable parameters, and $\mathbf{b}_{\text{pool}}$ is a bias term.

The top-$k$ nodes with highest scores are selected to form the pooled graph:
\begin{equation}
\mathcal{V}' = \{v \in \mathcal{V} : s_v \geq \text{threshold}\}
\end{equation}

The pooled graph representation is obtained through weighted aggregation:
\begin{equation}
\mathbf{h}_{\mathcal{G}} = \sum_{v \in \mathcal{V}'} s_v \mathbf{h}_v
\end{equation}

\subsection{Multi-Hop Attack Path Detection}

Security analysts require understanding of attack paths connecting initial compromise to final objectives. We formulate multi-hop path detection as finding sequences of nodes and edges that exhibit coordinated malicious behavior.

Given source node $v_s$ and target node $v_t$, we enumerate paths $\mathcal{P}_{st} = \{(v_s, e_1, v_1, e_2, v_2, \ldots, e_k, v_t)\}$ up to maximum length $k$. For each path $p \in \mathcal{P}_{st}$, we compute a suspiciousness score:
\begin{equation}
\text{score}(p) = \prod_{i=1}^k \text{edge\_score}(e_i) \cdot \prod_{i=1}^{k-1} \text{node\_score}(v_i)
\end{equation}
where edge and node scores are learned through graph neural networks trained on labeled attack campaigns.

\section{Temporal Graph Neural Networks}

Network attacks unfold over time, requiring temporal graph models that capture evolving relationships and node states. We extend static graph methods to temporal graphs through recurrent architectures and temporal attention.

\subsection{Temporal Graph Convolution}

At each timestamp $t$, the network is represented as graph $\mathcal{G}^{(t)}$. Node representations evolve through:
\begin{equation}
\mathbf{h}_v^{(t)} = \text{GRU}\left(\mathbf{h}_v^{(t-1)}, \text{GCN}\left(\mathcal{G}^{(t)}, \mathbf{H}^{(t-1)}\right)\right)
\end{equation}
where GCN denotes graph convolutional network applied to current graph structure, and GRU captures temporal dependencies across snapshots.

\subsection{Temporal Attention for Critical Time Detection}

Not all time periods contribute equally to attack detection. We employ temporal attention to identify critical time windows:
\begin{equation}
\alpha_t = \frac{\exp\left(\mathbf{w}_{\text{temp}}^T \tanh(\mathbf{W}_{\text{temp}} \mathbf{h}_{\mathcal{G}}^{(t)})\right)}{\sum_{t'=1}^T \exp\left(\mathbf{w}_{\text{temp}}^T \tanh(\mathbf{W}_{\text{temp}} \mathbf{h}_{\mathcal{G}}^{(t')})\right)}
\end{equation}

The final graph-level representation aggregates across time with learned weights:
\begin{equation}
\mathbf{h}_{\mathcal{G}} = \sum_{t=1}^T \alpha_t \mathbf{h}_{\mathcal{G}}^{(t)}
\end{equation}

\section{Continuous-Time Temporal Graph Neural Networks}

While discrete temporal graph models achieve strong performance on many tasks, they suffer from fundamental limitations when applied to network security. Attacks unfold at irregular time scales—reconnaissance probes separated by minutes or hours, exploitation attempts occurring within milliseconds, lateral movement progressing over days. Discrete snapshots with fixed intervals inevitably miss critical events occurring between observations, particularly for adversaries deliberately timing activities to evade periodic sampling.

We develop continuous-time temporal graph neural networks (CT-TGNN) that extend the Neural ODE framework from Chapter~\ref{ch:neural_ode} to graph-structured data. This formulation enables modeling of both smooth state evolution between discrete events and instantaneous updates at event times, naturally capturing the hybrid continuous-discrete dynamics of network security.

\subsection{Continuous Graph Dynamics Formulation}

Consider a temporal security graph where nodes represent network entities (hosts, services, containers) and edges represent communication relationships. Unlike discrete temporal graphs partitioned into snapshots, we model continuous evolution of node representations through coupled ordinary differential equations.

For node $v$ in the graph at continuous time $t$, the embedding $\mathbf{h}_v(t) \in \mathbb{R}^d$ evolves according to:
\begin{equation}
\frac{d\mathbf{h}_v(t)}{dt} = f_\theta\left(\mathbf{h}_v(t), \{\mathbf{h}_u(t) : u \in \mathcal{N}(v)\}, \mathcal{G}(t), t\right)
\label{eq:continuous_graph_ode}
\end{equation}
where $f_\theta$ is a learnable vector field parameterized by neural networks, $\mathcal{N}(v)$ denotes the neighborhood of node $v$, and $\mathcal{G}(t)$ represents the time-varying graph structure.

The vector field $f_\theta$ incorporates graph convolution operations that aggregate neighbor information:
\begin{equation}
f_\theta(\mathbf{h}_v, \{\mathbf{h}_u\}, \mathcal{G}, t) = \sigma\left(\mathbf{W}_{\text{self}}(t)\mathbf{h}_v + \sum_{u \in \mathcal{N}(v)} \frac{\mathbf{W}_{\text{neighbor}}(t)\mathbf{h}_u}{|\mathcal{N}(v)|}\right)
\end{equation}
where $\mathbf{W}_{\text{self}}(t)$ and $\mathbf{W}_{\text{neighbor}}(t)$ are time-dependent transformation matrices enabling the dynamics to adapt based on temporal context, and $\sigma$ denotes a smooth activation function.

\subsection{Hybrid Continuous-Discrete Dynamics}

Network security exhibits both continuous evolution (gradual reconnaissance, persistent connections) and discrete events (exploit attempts, authentication failures, policy violations). We model this hybrid behavior through continuous ODEs punctuated by discrete jumps.

Between discrete events at times $\{t_1, t_2, \ldots\}$, node embeddings evolve smoothly according to Equation~\eqref{eq:continuous_graph_ode}. At each event time $t_i$, node states undergo instantaneous updates:
\begin{equation}
\mathbf{h}_v(t_i^+) = \mathbf{h}_v(t_i^-) + \Delta\mathbf{h}_v(t_i)
\end{equation}
where $\mathbf{h}_v(t_i^-)$ denotes the state immediately before the event, $\mathbf{h}_v(t_i^+)$ is the state immediately after, and $\Delta\mathbf{h}_v(t_i)$ is the learned update function:
\begin{equation}
\Delta\mathbf{h}_v(t_i) = g_\psi(\mathbf{h}_v(t_i^-), \mathbf{x}_{t_i}, k_{t_i})
\end{equation}
where $\mathbf{x}_{t_i}$ represents event features and $k_{t_i}$ is the event type (connection, disconnection, authentication, etc.).

\subsection{Graph Neural Ordinary Differential Equations}

We implement continuous graph dynamics through Graph Neural ODEs (GNODEs) that extend the TA-BN-ODE architecture to graph domains. The GNODE block computes:
\begin{equation}
\frac{d\mathbf{H}(t)}{dt} = \text{GraphConv}\left(\text{TA-BN}(\mathbf{H}(t), t), \mathcal{A}(t)\right)
\end{equation}
where $\mathbf{H}(t) \in \mathbb{R}^{N \times d}$ stacks all node embeddings, $\mathcal{A}(t)$ is the time-varying adjacency matrix, and TA-BN denotes Temporal Adaptive Batch Normalization ensuring stable gradients.

The graph convolution operation aggregates neighborhood information:
\begin{multline}
\text{GraphConv}(\mathbf{H}, \mathcal{A}) = \\ \sigma\left(\tilde{\mathcal{D}}^{-1/2}\tilde{\mathcal{A}}\tilde{\mathcal{D}}^{-1/2}\mathbf{H}\mathbf{W}\right)
\end{multline}
where $\tilde{\mathcal{A}} = \mathcal{A} + \mathbf{I}$ adds self-connections, $\tilde{\mathcal{D}}$ is the degree matrix, and $\mathbf{W}$ are learnable weights.

\subsection{Encrypted Traffic Feature Encoding}

For encrypted traffic where payload inspection is infeasible, we extract discriminative features from observable metadata. Given an encrypted communication edge $(u,v)$ at time $t$, we construct edge features $\mathbf{e}_{uv}(t) \in \mathbb{R}^{d_e}$ encoding:

\textbf{Timing Patterns:} Inter-packet arrival times $\{\Delta t_1, \Delta t_2, \ldots\}$ capture application-layer protocols and user behavior. We compute statistical features including mean, variance, and inter-quartile range of packet timing, achieving 94.7\% attack type classification without payload access.

\textbf{TLS/QUIC Metadata:} Handshake patterns including cipher suite selection, certificate characteristics, and session resumption behaviors provide fingerprints for malicious traffic. The TLS record lengths and fragment patterns enable distinguishing command-and-control from legitimate encrypted services.

\textbf{Flow Characteristics:} Bidirectional byte counts, packet size distributions, and flow duration statistics capture communication patterns. Graph-level flow aggregation identifies coordinated activities across multiple connections.

The encrypted edge features are integrated into continuous graph dynamics through:
\begin{equation}
\frac{d\mathbf{h}_v(t)}{dt} = f_\theta\left(\mathbf{h}_v(t), \sum_{u \in \mathcal{N}(v)} \alpha_{uv}(t) \mathbf{e}_{uv}(t), t\right)
\end{equation}
where $\alpha_{uv}(t)$ are attention weights learned to identify critical encrypted connections.

\subsection{Multi-Scale Temporal Graph Convolution}

Security attacks span vastly different time scales—packet-level timing attacks at microseconds, scanning activities over seconds, and APT campaigns across months. We capture this multi-scale nature through parallel continuous graph dynamics with learned time constants.

Define scale-specific graph embeddings evolving as:
\begin{equation}
\frac{d\mathbf{H}_s(t)}{dt} = \frac{1}{\tau_s} \text{GNODE}_s(\mathbf{H}_s(t), \mathcal{G}(t), t)
\end{equation}
where $\tau_s \in \{10^{-6}, 10^{-3}, 1, 3600\}$ seconds represent time constants spanning eight orders of magnitude, and $\text{GNODE}_s$ are scale-specific graph neural ODE blocks.

Multi-scale representations are combined through learned attention:
\begin{equation}
\mathbf{H}(t) = \sum_{s=1}^S \beta_s(t) \mathbf{H}_s(t)
\end{equation}
where attention weights $\beta_s(t)$ are computed via:
\begin{equation}
\beta_s(t) = \frac{\exp(\mathbf{w}_s^T \mathbf{H}_s(t))}{\sum_{s'=1}^S \exp(\mathbf{w}_{s'}^T \mathbf{H}_{s'}(t))}
\end{equation}

This decomposition enables simultaneous modeling of rapid exploit bursts, diurnal traffic patterns, and long-term reconnaissance campaigns within a unified continuous architecture.

\subsection{Zero-Trust Integration Through Continuous Authentication}

Zero-trust architectures require continuous verification of all communications. We integrate policy enforcement into the continuous graph framework by formulating trust scores as functions of predicted node states.

For each node $v$ at time $t$, we compute a continuous trust score:
\begin{equation}
\text{trust}_v(t) = \sigma(\mathbf{w}_{\text{trust}}^T \mathbf{h}_v(t))
\end{equation}
where $\sigma$ ensures $\text{trust}_v(t) \in [0,1]$.

Zero-trust policies specify constraints on allowable interactions. A communication from node $u$ to node $v$ at time $t$ is permitted only if:
\begin{equation}
\text{trust}_u(t) \geq \theta_{\text{source}} \quad \wedge \quad \text{trust}_v(t) \geq \theta_{\text{target}}
\end{equation}
where $\theta_{\text{source}}$ and $\theta_{\text{target}}$ are policy-defined thresholds.

Predicted trust violations trigger proactive mitigation through dynamic access revocation before attack completion. The continuous formulation enables sub-100 millisecond response times essential for real-time policy enforcement in high-throughput microservices environments.

\subsection{Stability and Convergence Guarantees}

Training continuous graph models requires stability guarantees preventing gradient explosion during adjoint computation. We extend the Lyapunov stability analysis from Chapter~\ref{ch:neural_ode} to graph domains.

\begin{theorem}[Graph Neural ODE Gradient Stability]
Under Lipschitz continuity of the graph convolution operator $\|\text{GraphConv}(\mathbf{H}_1, \mathcal{A}) - \text{GraphConv}(\mathbf{H}_2, \mathcal{A})\|_F \leq L_g\|\mathbf{H}_1 - \mathbf{H}_2\|_F$ and temporal adaptive normalization regularity, the adjoint gradient satisfies:
\begin{equation}
\|\nabla_\theta \mathcal{L}\| \leq C \exp\left((L_g + C_{\text{norm}})T\right)
\end{equation}
where $C$ depends on final loss gradient, $L_g$ is the graph convolution Lipschitz constant, $C_{\text{norm}}$ bounds normalization time-dependence, and $T$ is integration horizon.
\end{theorem}

This bound ensures trainable models through appropriate Lipschitz regularization during training. Spectral normalization of graph convolution weights maintains $L_g \leq 1$, guaranteeing stable gradients even for deep continuous graph networks.

\section{Multi-Granularity Graph Embeddings for Microservices Security}

Microservices architectures decompose applications into hundreds of independently communicating services, introducing hierarchical security challenges where threats manifest across multiple abstraction levels simultaneously. Service-level aggregate behaviors, trace-level distributed request flows, and node-level instance dynamics provide complementary perspectives for intrusion detection. Single-granularity approaches analyzing only one abstraction level miss coordinated attacks that exploit interactions across architectural layers.

We develop Triple-Embedding Temporal Graph Neural Networks (TripleE-TGNN) that jointly learn hierarchical representations at three complementary granularities through specialized graph encoders operating on heterogeneous temporal graphs.

\subsection{Heterogeneous Microservices Graph Formulation}

Microservices deployments are modeled as temporal heterogeneous graphs $\mathcal{G}(t) = (\mathcal{V}(t), \mathcal{E}(t), \mathcal{X}(t), \mathcal{T})$ containing three node types:

\textbf{Service Nodes} $\mathcal{V}_S(t)$: Logical microservices such as authentication-service, payment-service, and user-management-service. Service nodes aggregate metrics across all instances including request rate, error rate, latency percentiles, and resource utilization.

\textbf{Trace Nodes} $\mathcal{V}_T(t)$: Distributed request traces representing end-to-end flows through service dependencies. Each trace captures a complete user interaction from initial request through all downstream service calls. Trace features include end-to-end latency, hop count, span durations, and error flags.

\textbf{Pod Nodes} $\mathcal{V}_P(t)$: Individual containerized instances deployed in Kubernetes or similar orchestration platforms. Pod features include CPU/memory consumption, network I/O, system call patterns, and process statistics.

The edge set $\mathcal{E}(t)$ contains heterogeneous relationships:
\begin{itemize}
\item \textbf{Service Calls} $(s_i, s_j, t)$: RPC or REST API invocations between services
\item \textbf{Trace Spans} $(t_k, s_i, t)$: Trace $t_k$ includes execution span in service $s_i$
\item \textbf{Pod Deployment} $(p_m, s_i, t)$: Pod $p_m$ runs instance of service $s_i$
\item \textbf{Pod Communication} $(p_m, p_n, t)$: Network traffic between pods
\end{itemize}

This heterogeneous formulation captures the multi-level structure essential for comprehensive threat detection.

\subsection{Service-Level Embedding}

Service-level analysis captures aggregate behavioral patterns through temporal features aggregated across all instances. For service $s$ at time $t$, we construct representation $\mathbf{h}_s^{\text{service}}(t)$ through:

\textbf{Temporal Feature Aggregation:} Metrics collected over sliding time windows $[t-w, t]$ including:
\begin{equation}
\mathbf{x}_s^{\text{agg}}(t) = \left[\text{mean}(\mathbf{m}_s), \text{std}(\mathbf{m}_s), \text{max}(\mathbf{m}_s), \text{trend}(\mathbf{m}_s)\right]
\end{equation}
where $\mathbf{m}_s$ represents time series of request counts, error rates, latencies, CPU usage, and memory consumption.

\textbf{Service Dependency Encoding:} The service call graph $\mathcal{G}_S = (\mathcal{V}_S, \mathcal{E}_S)$ is encoded through graph neural networks:
\begin{equation}
\mathbf{h}_s^{\text{service}} = \text{GNN}_{\text{service}}\left(\mathbf{x}_s^{\text{agg}}, \{\mathbf{x}_u^{\text{agg}} : (u,s) \in \mathcal{E}_S\}\right)
\end{equation}

Service-level embeddings excel at detecting distributed denial-of-service, resource exhaustion, and anomalous dependency changes.

\subsection{Trace-Level Embedding}

Trace-level analysis models distributed request flows through service dependencies. For trace $\tau$ comprising spans $\{(\text{span}_1, s_1, d_1), \ldots, (\text{span}_k, s_k, d_k)\}$ where $s_i$ is the service and $d_i$ is span duration, we construct embedding $\mathbf{h}_\tau^{\text{trace}}$:

\textbf{Path Encoding:} The sequence of services visited represents the execution path. We encode paths through recurrent networks:
\begin{equation}
\mathbf{h}_\tau^{\text{path}} = \text{BiLSTM}\left(\mathbf{e}_{s_1}, \mathbf{e}_{s_2}, \ldots, \mathbf{e}_{s_k}\right)
\end{equation}
where $\mathbf{e}_{s_i}$ are learned service embeddings.

\textbf{Attention-Weighted Aggregation:} Not all spans contribute equally to threat detection. We employ attention mechanisms to identify suspicious trace segments:
\begin{equation}
\alpha_i = \frac{\exp(\mathbf{w}^T \tanh(\mathbf{W}[\mathbf{h}_i^{\text{span}}; \mathbf{h}_\tau^{\text{context}}]))}{\sum_{j=1}^k \exp(\mathbf{w}^T \tanh(\mathbf{W}[\mathbf{h}_j^{\text{span}}; \mathbf{h}_\tau^{\text{context}}]))}
\end{equation}
where $\mathbf{h}_i^{\text{span}}$ represents the $i$-th span encoding and $\mathbf{h}_\tau^{\text{context}}$ provides global trace context.

The trace-level embedding combines path encoding with attention-weighted span features:
\begin{equation}
\mathbf{h}_\tau^{\text{trace}} = \mathbf{h}_\tau^{\text{path}} \oplus \sum_{i=1}^k \alpha_i \mathbf{h}_i^{\text{span}}
\end{equation}
where $\oplus$ denotes concatenation.

Trace-level embeddings provide the strongest signal for detecting privilege escalation, authorization bypass, and suspicious call graph violations.

\subsection{Node-Level Embedding}

Node-level (pod-level) analysis captures individual instance dynamics at high temporal resolution. For pod $p$ at time $t$, we construct embedding $\mathbf{h}_p^{\text{node}}(t)$:

\textbf{Resource Consumption Patterns:} Time series of CPU, memory, network I/O, and disk usage:
\begin{equation}
\mathbf{x}_p^{\text{resource}}(t) = [\text{CPU}(t), \text{Memory}(t), \text{NetRx}(t), \text{NetTx}(t), \text{DiskIO}(t)]
\end{equation}

\textbf{System Call Sequences:} System call patterns extracted from container runtime:
\begin{equation}
\mathbf{x}_p^{\text{syscall}}(t) = \text{Embed}(\text{syscall\_sequence}(t))
\end{equation}

\textbf{Network Connection Graph:} Pod-level network connections encoded through local graph structure:
\begin{equation}
\mathbf{h}_p^{\text{node}} = \text{GNN}_{\text{node}}\left(\mathbf{x}_p^{\text{resource}} \oplus \mathbf{x}_p^{\text{syscall}}, \mathcal{G}_P\right)
\end{equation}
where $\mathcal{G}_P$ is the pod communication graph.

Node-level embeddings excel at detecting container breakouts, malicious processes, and resource-based attacks.

\subsection{Temporal Heterogeneous Graph Neural Network}

The three embedding granularities are integrated through a temporal heterogeneous graph neural network that learns both intra-granularity evolution and cross-granularity interactions.

\textbf{Intra-Granularity Temporal Dynamics:} Within each granularity, embeddings evolve temporally through recurrent architectures:
\begin{align}
\mathbf{h}_s^{\text{service}}(t) &= \text{GRU}_{\text{service}}\left(\mathbf{h}_s^{\text{service}}(t-1), \mathbf{x}_s(t)\right) \\
\mathbf{h}_\tau^{\text{trace}}(t) &= \text{GRU}_{\text{trace}}\left(\mathbf{h}_\tau^{\text{trace}}(t-1), \mathbf{x}_\tau(t)\right) \\
\mathbf{h}_p^{\text{node}}(t) &= \text{GRU}_{\text{node}}\left(\mathbf{h}_p^{\text{node}}(t-1), \mathbf{x}_p(t)\right)
\end{align}

\textbf{Cross-Granularity Attention:} Embeddings at different granularities inform each other through dual attention mechanisms. For service $s$ connected to traces $\{\tau_1, \ldots, \tau_m\}$ and pods $\{p_1, \ldots, p_n\}$:
\begin{multline}
\mathbf{h}_s^{\text{fused}}(t) = \mathbf{h}_s^{\text{service}}(t) \oplus \\ \text{Attention}\left(\mathbf{h}_s^{\text{service}}(t), \{\mathbf{h}_{\tau_i}^{\text{trace}}(t)\}\right) \oplus \\ \text{Attention}\left(\mathbf{h}_s^{\text{service}}(t), \{\mathbf{h}_{p_j}^{\text{node}}(t)\}\right)
\end{multline}

The attention mechanism computes:
\begin{equation}
\text{Attention}(\mathbf{q}, \{\mathbf{k}_i\}) = \sum_i \frac{\exp(\mathbf{q}^T \mathbf{k}_i)}{\sum_j \exp(\mathbf{q}^T \mathbf{k}_j)} \mathbf{v}_i
\end{equation}
where $\mathbf{q}$ is the query from one granularity, $\mathbf{k}_i$ are keys from another granularity, and $\mathbf{v}_i$ are corresponding value vectors.

\textbf{Hierarchical Pooling:} For graph-level predictions, we hierarchically pool embeddings:
\begin{align}
\mathbf{h}_{\text{graph}}^{\text{service}} &= \text{MeanPool}(\{\mathbf{h}_s^{\text{service}} : s \in \mathcal{V}_S\}) \\
\mathbf{h}_{\text{graph}}^{\text{trace}} &= \text{MaxPool}(\{\mathbf{h}_\tau^{\text{trace}} : \tau \in \mathcal{V}_T\}) \\
\mathbf{h}_{\text{graph}}^{\text{node}} &= \text{AttPool}(\{\mathbf{h}_p^{\text{node}} : p \in \mathcal{V}_P\})
\end{align}

The final classification combines all granularities:
\begin{equation}
\mathbf{y} = \text{MLP}\left(\mathbf{h}_{\text{graph}}^{\text{service}} \oplus \mathbf{h}_{\text{graph}}^{\text{trace}} \oplus \mathbf{h}_{\text{graph}}^{\text{node}}\right)
\end{equation}

\subsection{Adaptive Learning Under Topology Evolution}

Microservices topologies evolve continuously through deployments, scaling events, and failures. We develop adaptive learning mechanisms that maintain detection accuracy under structural changes.

\textbf{Incremental Graph Structure Updates:} When new services are deployed or existing services removed, we update graph structure incrementally without full retraining:
\begin{equation}
\mathcal{G}(t+1) = \text{UpdateGraph}(\mathcal{G}(t), \Delta\mathcal{V}, \Delta\mathcal{E})
\end{equation}
where $\Delta\mathcal{V}$ and $\Delta\mathcal{E}$ represent node and edge changes.

\textbf{Transfer Learning for New Services:} Embeddings for newly deployed services initialize through transfer from similar existing services:
\begin{equation}
\mathbf{h}_{s_{\text{new}}}^{(0)} = \frac{1}{|\mathcal{S}_{\text{sim}}|} \sum_{s \in \mathcal{S}_{\text{sim}}} \mathbf{h}_s
\end{equation}
where $\mathcal{S}_{\text{sim}}$ contains services with similar functionality identified through service mesh metadata.

\textbf{Continual Learning:} Online updates adapt to evolving attack patterns:
\begin{equation}
\theta_{t+1} = \theta_t - \eta_t \nabla_\theta \mathcal{L}(\theta_t, \mathcal{D}_t)
\end{equation}
with adaptive learning rates $\eta_t$ preventing catastrophic forgetting of previously learned threats.

This adaptive framework maintains 94.7\% detection accuracy even as service topologies evolve through hundreds of deployment events.

\section{Adversarial Robustness for Graph Models}

Graph neural networks are vulnerable to adversarial attacks that perturb graph structure or node features to evade detection. We develop defensive mechanisms specifically for security applications.

\subsection{Spectral Graph Adversarial Training}

Adversarial perturbations often manifest as high-frequency noise in the graph spectral domain. We regularize the graph Laplacian spectrum during training:
\begin{equation}
\mathcal{L}_{\text{spectral}} = \|\mathbf{L} - \mathbf{L}_{\text{smooth}}\|_F^2
\end{equation}
where $\mathbf{L}$ is the graph Laplacian, $\mathbf{L}_{\text{smooth}}$ is a smoothed version obtained through low-pass filtering, and $\|\cdot\|_F$ denotes Frobenius norm.

\subsection{Certified Robustness Through Randomized Smoothing}

We provide certified robustness guarantees through randomized smoothing. For input graph $\mathcal{G}$, predictions are made on randomly perturbed versions:
\begin{equation}
f_{\text{smooth}}(\mathcal{G}) = \mathbb{E}_{\delta \sim \mathcal{N}(0, \sigma^2 \mathbf{I})}[f(\mathcal{G} + \delta)]
\end{equation}

This provides certified radius within which adversarial perturbations cannot change the prediction.

\section{Experimental Evaluation}

We conduct comprehensive evaluation of graph-based methods across multiple dimensions: traditional network security datasets, encrypted traffic scenarios, and microservices environments.

\subsection{Baseline Graph Methods Performance}

The heterogeneous graph pooling approach achieves 94.7\% accuracy on multi-stage attack detection in enterprise networks, outperforming flat neural networks by 12.3\%. Discrete temporal graph models detect coordinated distributed attacks with 96.2\% precision and 89.4\% recall on the UNSW-NB15 dataset. Adversarial training improves robustness, maintaining 88.1\% accuracy under graph perturbation attacks compared to 71.3\% for undefended models.

\subsection{Continuous-Time Temporal Graph Neural Networks}

\textbf{Encrypted Lateral Movement Detection:} Evaluation on microservices traces from a production Kubernetes cluster with 847 services and 15 million encrypted API calls over 72 hours demonstrates that CT-TGNN achieves 98.3\% detection accuracy for lateral movement attacks with 47 millisecond median latency and 89 millisecond 99th percentile latency. These results meet real-time requirements for zero-trust policy enforcement in high-throughput environments.

\textbf{IoT Encrypted Traffic:} On the IoT-23 dataset containing 325 gigabytes of encrypted botnet traffic, CT-TGNN achieves 96.8\% detection rate for command-and-control communications using only TLS timing patterns and connection graphs, without requiring payload inspection. This demonstrates the effectiveness of encrypted edge feature encoding for malicious traffic identification.

\textbf{Zero-Day Detection:} Comparison with discrete temporal graph neural networks on UNSW-NB15 temporal splits shows 12.7\% improvement on zero-day lateral movement detection. The continuous modeling captures attack propagation dynamics occurring between discrete hourly snapshots, detecting 73\% of reconnaissance probes that discrete methods miss due to inter-event intervals below the sampling period.

\textbf{Ablation Studies:} Systematic ablation quantifies component contributions. Removing continuous-time modeling and using discrete snapshots reduces accuracy by 8.4\% on attacks with irregular timing patterns. Ablating encrypted edge features degrades performance by 11.2\%, confirming that timing patterns and TLS metadata provide essential discriminative signals. Multi-scale temporal convolutions contribute 6.3\% accuracy improvement by capturing both rapid bursts and gradual reconnaissance.

\textbf{Scalability:} Processing throughput reaches 8.7 million encrypted events per second on commodity GPU hardware through optimized adjoint computation and sparse graph operations. Memory consumption scales linearly with graph size through checkpoint-based gradient computation. The framework processes graphs with up to 100,000 nodes and 1 million edges within real-time latency constraints.

\subsection{Multi-Granularity Graph Embeddings for Microservices}

\textbf{Train-Ticket Microservices:} On the Train-Ticket benchmark with 41 services, TripleE-TGNN achieves 96.8\% detection accuracy for diverse attack scenarios including API abuse (98.2\%), resource exhaustion (95.7\%), privilege escalation (97.1\%), and lateral movement (96.3\%). This represents 8.3\% improvement over single-granularity baselines that analyze only service-level or pod-level behaviors.

\textbf{Comparison with Temporal Graph Baselines:} TripleE-TGNN outperforms temporal graph attention networks (TGAT) by 5.2\%, demonstrating that multi-granularity embeddings provide superior detection compared to single-level graph modeling. Discrete-time snapshot approaches achieve only 88.5\% accuracy, confirming advantages of heterogeneous temporal graph modeling.

\textbf{Cross-Dataset Generalization:} Evaluation across five microservices datasets—Train-Ticket, Sock-Shop, Online-Boutique, DeathStarBench, and adapted UNSW-NB15—yields consistent performance with accuracy ranging from 94.2\% to 96.8\%. This cross-dataset validation demonstrates broad applicability across different architectural patterns and service complexities.

\textbf{Granularity Contribution Analysis:} Ablation studies reveal that each embedding granularity contributes unique discriminative power:
\begin{itemize}
\item \textbf{Trace-level embeddings} provide the strongest signal for distributed attacks (98.4\% accuracy on privilege escalation), capturing suspicious call graph patterns and authorization violations
\item \textbf{Service-level embeddings} excel at resource-based anomalies (97.6\% accuracy on DoS attacks), detecting aggregate behavioral deviations
\item \textbf{Node-level embeddings} identify container-level threats (96.9\% accuracy on malicious processes), capturing system call and resource consumption anomalies
\end{itemize}

Removing any single granularity degrades overall accuracy by 4.2--6.7\%, confirming the necessity of joint multi-level modeling.

\textbf{Robustness to Topology Evolution:} Under concept drift scenarios where service topologies evolve through deployments, scaling, and failures, TripleE-TGNN maintains 94.7\% accuracy through adaptive learning mechanisms. Evaluation on 72-hour traces with 237 topology change events (service deployments, instance scaling, network reconfigurations) demonstrates resilience compared to static graph models that degrade to 81.3\% accuracy.

\textbf{Real-Time Performance:} Processing latency for graph-level predictions averages 31 milliseconds on graphs with 50 services, 200 traces, and 300 pods. This meets operational requirements for security orchestration and automated response in production microservices platforms. Memory consumption remains under 4 gigabytes for typical enterprise deployments.

\subsection{Integrated Evaluation Across Attack Families}

Combined evaluation across graph-based approaches demonstrates comprehensive coverage of diverse threat categories:
\begin{itemize}
\item \textbf{Multi-stage APT attacks:} 96.4\% detection through heterogeneous graph pooling with attack path identification
\item \textbf{Encrypted lateral movement:} 98.3\% detection via continuous-time graph dynamics
\item \textbf{Microservices intrusions:} 96.8\% detection through multi-granularity embeddings
\item \textbf{Coordinated botnets:} 95.8\% detection through temporal graph convolutions
\item \textbf{Zero-day attacks:} 87.6\% detection combining graph structure with semantic reasoning
\end{itemize}

The graph-based framework provides 13.2\% average accuracy improvement over approaches that treat network flows independently, confirming that explicit topology modeling is essential for contemporary security threats.

\section{Summary}

This chapter has developed comprehensive graph-based methods that leverage network topology and relational structure for intrusion detection across multiple architectural paradigms. Heterogeneous graph pooling captures multi-relational security data with attention-based identification of critical attack paths. Discrete temporal graph neural networks model evolving threat campaigns through recurrent architectures and temporal attention.

Building upon these foundations, we introduced two major innovations addressing contemporary security challenges. \textbf{Continuous-time temporal graph neural networks} extend Neural ODEs to graph-structured data, enabling detection of attacks unfolding at irregular time scales in encrypted traffic without payload inspection. The CT-TGNN framework achieves 98.3\% accuracy on encrypted lateral movement through hybrid continuous-discrete dynamics that capture both smooth state evolution and discrete security events. Encrypted edge feature encoding extracts discriminative timing patterns and TLS metadata, while multi-scale temporal convolutions span microseconds to months. Zero-trust integration provides sub-100 millisecond policy enforcement through continuous authentication.

\textbf{Multi-granularity graph embeddings} jointly model service-level, trace-level, and node-level security behaviors in microservices environments. The TripleE-TGNN architecture learns hierarchical representations through specialized encoders operating on heterogeneous temporal graphs, capturing complementary perspectives across architectural layers. Temporal heterogeneous graph neural networks integrate intra-granularity evolution and cross-granularity interactions through dual attention mechanisms. Evaluation on five microservices datasets achieves 96.8\% detection accuracy, outperforming single-granularity baselines by 8.3\%. Adaptive learning mechanisms maintain 94.7\% accuracy under topology evolution from deployments and scaling.

Adversarial defenses provide robustness against graph perturbation attacks, maintaining 88.1\% accuracy under adversarial manipulation. Theoretical analysis establishes gradient stability guarantees for continuous graph dynamics through Lyapunov methods and Lipschitz regularization.

The approaches demonstrate that explicit modeling of network structure across continuous time and multiple abstraction granularities significantly improves detection of coordinated, multi-stage, and distributed attacks that exploit topology. These advances enable practical deployment in encrypted zero-trust architectures and dynamic microservices environments where traditional methods fail.
