\chapter{Literature Review and Theoretical Foundations}
\label{ch:literature}

This chapter provides a comprehensive review of the theoretical foundations and prior research that underpin this thesis. The chapter is organized around eight interconnected research domains that collectively inform our approach to network intrusion detection: network intrusion detection systems, neural ordinary differential equations and continuous-time models, optimal transport for domain adaptation, privacy-preserving machine learning and federated learning, deep learning for encrypted traffic analysis, graph neural networks for network security, temporal point processes, and Bayesian deep learning for uncertainty quantification. Each section reviews seminal work, recent advances, and identifies critical gaps that motivate the contributions presented in subsequent chapters.

\section{Network Intrusion Detection Systems}

Network intrusion detection systems represent a cornerstone of cybersecurity infrastructure, evolving from simple signature-matching approaches to sophisticated machine learning architectures. Traditional intrusion detection relied fundamentally on signature matching and rule-based systems~\cite{scarfone2007guide}, which proved effective against known attacks but remained inherently limited to threats documented in signature databases. These systems operated through pattern matching on packet payloads and protocol headers, failing catastrophically when confronted with novel attack vectors or polymorphic malware that modified signatures to evade detection.

The introduction of machine learning methodologies addressed some limitations of signature-based approaches by enabling generalization beyond known attack patterns. Early statistical learning methods including support vector machines~\cite{mukkamala2002intrusion}, random forests~\cite{panda2011network}, and ensemble methods~\cite{gaikwad2014intrusion} demonstrated improved detection of previously unseen attacks by learning discriminative boundaries in feature space. However, these approaches struggled fundamentally with temporal dependencies inherent in network traffic, treating flows as independent samples and missing critical sequential patterns that characterize coordinated attack campaigns.

The deep learning revolution transformed intrusion detection through automated feature learning and hierarchical representation discovery. Convolutional neural networks enabled spatial feature extraction from packet sequences~\cite{vinayakumar2017applying}, learning filters that capture byte-level patterns and protocol structures without manual feature engineering. Long short-term memory networks addressed temporal modeling~\cite{kim2016lstm}, using gating mechanisms to selectively retain information across packet sequences and capture long-range dependencies essential for detecting multi-stage attacks. Attention mechanisms~\cite{vaswani2017attention} further advanced temporal modeling by enabling direct computation of dependencies between arbitrary positions in sequences, overcoming the sequential processing bottleneck of recurrent architectures.

Transformer-based systems represent the current state-of-the-art in many intrusion detection benchmarks~\cite{jiang2020transformer}, leveraging multi-head self-attention to model complex relationships across entire traffic flows. These architectures achieve strong performance on spatial features extracted from network metadata, but critically discretize time into fixed intervals, thereby missing the fine-grained timing information that distinguishes many attack types. The discretization introduces artificial temporal granularity that masks microsecond-level patterns in port scanning, millisecond-level coordinated botnet communications, and second-to-minute reconnaissance behaviors that unfold across vastly different time scales.

Recent security-specific neural architectures have integrated domain knowledge from cybersecurity practice. Graph neural networks model network topology explicitly~\cite{kipf2017semi,jiang2019graph}, propagating information across communication graphs to detect attacks that exploit lateral movement and privilege escalation patterns. Adversarially robust detectors~\cite{corona2019adversarial} incorporate defensive distillation and certified robustness techniques to maintain performance under adversarial perturbations, recognizing that attackers will actively probe and evade detection systems. Federated learning approaches enable privacy-preserving collaborative training~\cite{mothukuri2021federated}, allowing organizations to benefit from shared threat intelligence without exposing sensitive network traffic data that may contain proprietary information or user privacy concerns.

Modern intrusion detection research emphasizes three critical challenges that remain partially addressed. First, handling distribution shift across deployment environments poses fundamental generalization challenges, as security models trained on one network exhibit catastrophic performance degradation when transferred to different organizations, cloud providers, or network architectures. Second, achieving real-time detection with sub-100 millisecond latency requirements conflicts with the computational demands of deep neural networks, particularly for high-throughput environments processing millions of packets per second. Third, providing interpretable alerts for security analysts represents an operational necessity, as opaque neural network predictions without explanatory context generate alert fatigue and undermine trust in automated systems.

Despite substantial progress, no existing intrusion detection system integrates continuous-time dynamics, temporal point processes, Bayesian uncertainty quantification, and large language model-based zero-shot detection in a unified framework optimized specifically for multi-scale temporal attack patterns spanning microseconds to months. This integration gap motivates the architectures developed in subsequent chapters.

\section{Neural Ordinary Differential Equations and Continuous-Time Models}

Neural ordinary differential equations, introduced by Chen et al.~\cite{chen2018neural}, revolutionized deep learning by reconceptualizing neural networks as continuous dynamical systems rather than discrete layer stacks. The foundational insight parameterizes network dynamics through continuous-time ordinary differential equations of the form $\frac{dh}{dt} = f_\theta(h, t)$, where the hidden state $h(t)$ evolves continuously according to a learned vector field $f_\theta$ parameterized by neural network weights $\theta$. This formulation enables memory-efficient backpropagation through adjoint methods~\cite{pontryagin1962mathematical}, computing gradients by solving a second ODE backward in time rather than storing intermediate activations, thereby reducing memory complexity from $O(L)$ for $L$ layers to $O(1)$ independent of integration time.

Subsequent theoretical and methodological advances have substantially expanded the expressiveness and applicability of neural ODEs. State augmentation techniques~\cite{dupont2019augmented} address limited expressiveness of initial formulations by embedding inputs into higher-dimensional spaces, enabling neural ODEs to learn homeomorphisms that cannot be represented in the original feature dimension. Optimal transport formulations~\cite{onken2021ot} connect neural ODEs to the Monge problem of optimal mass transport, providing geometric interpretations and enabling incorporation of Wasserstein distance penalties during training. Stability analyses through Lyapunov theory~\cite{lu2018beyond} establish conditions under which neural ODE trajectories remain bounded and converge to equilibria, providing theoretical guarantees essential for safety-critical applications.

Recent work has explored adversarial robustness through architectural constraints. Orthogonal neural ODEs~\cite{purohit2024ortho} enforce orthogonal weight matrices that preserve vector norms during forward propagation, providing certified Lipschitz bounds on the function realized by the network and consequently certified robustness guarantees against bounded perturbations. Closed-form continuous networks~\cite{hasani2022liquid} use linear time-invariant dynamics with explicit analytical solutions, enabling exact computation of hidden states at arbitrary timestamps without numerical integration, though at the cost of reduced expressiveness compared to nonlinear neural ODEs.

Despite theoretical elegance and empirical success in computer vision and time-series forecasting, cybersecurity applications of neural ODEs remain largely unexplored. This gap exists despite compelling natural alignment between continuous dynamics and security domains. Attack campaigns evolve continuously over time scales spanning reconnaissance (hours to days), lateral movement (minutes to hours), privilege escalation (seconds to minutes), and exploitation (milliseconds to seconds). Modeling these multi-scale temporal patterns through continuous differential equations provides mathematical naturality that discrete architectures lack, while the memory efficiency of adjoint methods enables deployment in resource-constrained security monitoring infrastructure.

A critical obstacle that has impeded adoption of neural ODEs in practical systems involves the incompatibility between batch normalization and continuous dynamics. Batch normalization, ubiquitous in modern deep learning for stabilizing training and enabling higher learning rates, operates by normalizing activations using statistics computed over mini-batches. Standard batch normalization applies time-independent normalization designed for discrete layers with fixed statistics, but this creates gradient explosion in deep ODE stacks where integration depth varies between samples. When integration requires many function evaluations for numerical ODE solving, normalized activations at different integration times exhibit incompatible statistics, causing catastrophic training instability.

Salvi et al.~\cite{salvi2024tabn} recently resolved this fundamental limitation through Temporal Adaptive Batch Normalization (TA-BN), which parameterizes normalization statistics as explicit functions of integration time rather than computing them solely from batch data. The temporal dependence enables normalization to adapt continuously as hidden states evolve, maintaining stable gradients across variable integration depths. This breakthrough enables training deep neural ODE architectures with hundreds of function evaluations without instability, removing a critical barrier to practical deployment.

Beyond neural ODEs specifically, several continuous-time approaches model irregular temporal data through related differential equation formulations. Neural controlled differential equations~\cite{kidger2020neural} extend neural ODEs by incorporating time-varying controls derived from data, using continuous path interpolation for time-series with missing observations and achieving strong interpolation performance through controlled dynamics. GRU-ODE~\cite{de2019gru} combines recurrent gating structures with ODE dynamics, maintaining discrete gated updates at observation times while evolving state continuously between observations. Latent ODEs~\cite{rubanova2019latent} learn latent representations of irregular time-series through variational autoencoders, encoding observations into latent initial conditions and decoding ODE trajectories back to observation space. Continuous normalizing flows~\cite{grathwohl2018ffjord} leverage change-of-variables formulas to provide tractable density estimation, defining invertible transformations through ODE flow maps with log-determinants computable via trace estimation.

However, these continuous-time architectures focus primarily on interpolation and forecasting rather than event detection tasks central to security applications. They lack explicit uncertainty quantification mechanisms for assessing confidence in predictions, which represents a critical gap for security contexts where false positives and false negatives carry asymmetric operational costs. Furthermore, existing continuous-time models do not incorporate discrete event intensities through point process formulations, missing the natural mathematical framework for modeling irregular security events where timing information itself carries strong signal about attack likelihood.

\section{Optimal Transport for Domain Adaptation}

Optimal transport theory provides rigorous mathematical foundations for measuring and transforming probability distributions, with roots in Monge's 1781 problem of minimizing transportation cost for moving earth masses. The modern formulation by Kantorovich~\cite{kantorovich1942translocation} relaxes Monge's deterministic transport maps to probabilistic couplings, defining the $p$-Wasserstein distance between source distribution $\mu$ and target distribution $\nu$ on metric space $(\mathcal{X}, d)$ as:
\begin{equation}
W_p(\mu, \nu) = \left(\inf_{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{X}} d(x,y)^p d\gamma(x,y)\right)^{1/p}
\end{equation}
where $\Pi(\mu, \nu)$ denotes the set of coupling measures (joint distributions) with marginals $\mu$ and $\nu$. The coupling $\gamma$ describes how probability mass transports from source to target, with $d(x,y)$ representing the ground cost of moving unit mass from location $x$ to $y$. This formulation provides geometrically meaningful distance between probability measures that respects the underlying metric structure, offering advantages over moment-matching approaches that discard geometric information.

Courty et al.~\cite{courty2017optimal} pioneered applying optimal transport to domain adaptation through the Joint Distribution Optimal Transport (JDOT) problem, minimizing combined cost incorporating both geometric transport cost and task-specific loss. Their approach achieves improved generalization bounds compared to Maximum Mean Discrepancy methods by explicitly aligning distributions according to task-relevant geometry. Damodaran et al.~\cite{damodaran2018deepjdot} extended JDOT to deep neural networks through joint training of feature extractors and transport plans, achieving state-of-the-art results on visual domain adaptation benchmarks by learning feature representations that minimize Wasserstein distance between source and target embeddings.

Computational advances have focused on entropic regularization introduced by Cuturi~\cite{cuturi2013sinkhorn}, which adds an entropy term to the original Kantorovich problem:
\begin{equation}
W_{\epsilon}(\mu, \nu) = \min_{\gamma \in \Pi(\mu, \nu)} \langle C, \gamma \rangle - \epsilon H(\gamma)
\end{equation}
where $C$ is the cost matrix, $\epsilon > 0$ controls regularization strength, and $H(\gamma) = -\sum_{ij} \gamma_{ij}\log\gamma_{ij}$ is the entropy. This reformulation enables solution via the Sinkhorn algorithm~\cite{cuturi2013sinkhorn,genevay2016stochastic}, which alternates between row and column normalization achieving convergence in $O(n^2/\epsilon^3)$ iterations. Recent work~\cite{altschuler2017near} establishes that doubling schedules for the regularization parameter $\epsilon$ achieve exponential convergence, reducing iterations to $O(\log(1/\epsilon_{\text{target}}))$ stages through careful warm-starting strategies.

The Wasserstein Distance Guided Feature Tokenizer Transformer~\cite{rasheed2022wdft} represents the only published application of optimal transport specifically to network security contexts. This recent framework uses Wasserstein distance to quantify distributional gaps between network traffic domains captured on different infrastructure, then performs adversarial training to learn domain-invariant features~\cite{ganin2016domain} for transformer-based intrusion detection. The approach demonstrates that optimal transport provides principled measurement of distribution shift in security data, enabling targeted feature learning that bridges domain gaps. However, this work operates in centralized settings without privacy preservation requirements, does not address federated multi-cloud scenarios where data cannot be shared, and lacks mechanisms for Byzantine robustness against malicious participants.

Domain adaptation bounds establish theoretical motivation for optimal transport approaches. Under covariate shift assumptions where label distributions are preserved $P_S(y|x) = P_T(y|x)$, the target risk admits bounds of the form:
\begin{equation}
\mathcal{R}_T(f_T) \leq \mathcal{R}_S(f_T) + \lambda W_p(\mu_S, \nu) + O\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)
\end{equation}
where $W_p(\mu_S, \nu)$ is the Wasserstein distance between source and target marginals, $\lambda$ is a Lipschitz constant of the loss function, and the final term represents estimation error. This bound demonstrates that minimizing Wasserstein distance directly controls the adaptation gap between source and target performance, providing principled justification for optimal transport-based domain adaptation strategies.

However, computing Wasserstein distance requires access to samples from both source and target distributions, which conflicts fundamentally with privacy requirements in security contexts where network traffic data contains sensitive information about vulnerabilities, detection capabilities, and threat landscapes. This tension between the mathematical requirements of optimal transport and operational security constraints motivates the development of privacy-preserving variants that can compute approximate transport plans while satisfying differential privacy guarantees.

\section{Privacy-Preserving Machine Learning and Federated Learning}

Privacy-preserving machine learning addresses the fundamental tension between learning from sensitive data and protecting individual privacy or organizational confidentiality. Differential privacy~\cite{dwork2006calibrating,dwork2014algorithmic} provides rigorous mathematical guarantees limiting information leakage from statistical computations. A randomized mechanism $\mathcal{M}: \mathcal{D}^n \rightarrow \mathbb{R}$ satisfies $(\epsilon,\delta)$-differential privacy if for all adjacent datasets $D, D'$ differing in one record and all measurable sets $S$:
\begin{equation}
\Pr[\mathcal{M}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{M}(D') \in S] + \delta
\end{equation}
The privacy parameter $\epsilon$ quantifies maximum distinguishability between outputs on neighboring databases, with smaller values providing stronger protection at the cost of reduced utility. The failure probability $\delta$ accounts for negligible privacy loss events that occur with probability at most $\delta$. Typical parameter choices span $\epsilon \in [0.1, 10]$ depending on application sensitivity, with $\delta = O(1/n^2)$ for datasets of size $n$.

The Gaussian mechanism~\cite{dwork2006calibrating,balle2018improving} achieves differential privacy by adding calibrated noise with variance $\sigma^2 = 2\Delta^2\log(1.25/\delta)/\epsilon^2$ where $\Delta$ is the global sensitivity quantifying maximum change in function output when one record is modified. For optimal transport computations, recent work by the PrivPGD framework~\cite{chen2022private} demonstrates feasibility of computing transport plans under differential privacy through noisy marginal estimation combined with particle gradient descent minimizing Sinkhorn divergence between noisy marginals and synthetic distributions. However, the impact of differential privacy noise on downstream task performance in security contexts remains largely uncharacterized.

Federated learning enables collaborative model training without centralizing data, addressing privacy concerns through distributed computation. The FedAvg algorithm~\cite{mcmahan2017communication} performs local training on distributed clients followed by periodic aggregation of model updates at a central server, ensuring raw data never leaves client devices. Privacy enhancement through differential privacy integration was introduced by Abadi et al. through the moments accountant mechanism, enabling tight privacy loss tracking across multiple training rounds through careful composition analysis. Advanced federated optimization methods including FedProx address statistical heterogeneity across clients through proximal terms that prevent excessive local adaptation, while FedKD approaches leverage knowledge distillation for robust aggregation.

Recent federated intrusion detection systems demonstrate strong privacy-accuracy trade-offs in security contexts. The SECIoHT-FL framework achieves $(\epsilon = 0.34, \delta = 10^{-5})$-differential privacy at 95.48\% detection accuracy through carefully calibrated noise injection to model updates. The DVACNN-Fed framework~\cite{wang2022dvacnn} combines variational autoencoders with convolutional networks in federated architecture, achieving 95.59\% accuracy on TON-IoT with privacy index 94.26\%. The BFLIDS system integrates blockchain for secure aggregation verification, using distributed storage for model parameters and modified KL divergence in FedAvg with adaptive CNN-BiLSTM architectures.

However, federated learning faces critical challenges in security applications. Statistical heterogeneity across organizations leads to biased aggregation when attack distributions differ substantially between participants, with standard FedAvg achieving only 78-82\% accuracy under severe distribution shift. Byzantine adversaries represent existential threats, as compromised or malicious participants can inject poisoned model updates that degrade global model performance. The FedKD-IDS framework achieves remarkable resilience through knowledge distillation with global and local verification mechanisms, maintaining 79\% accuracy even when 50\% of participants are malicious by analyzing logit distributions to detect anomalous updates.

Byzantine-robust aggregation rules including Krum, median-based methods, and trimmed-mean provide provable guarantees but impose computational overhead. Krum selects the client update closest to the majority of other updates measured by Euclidean distance, while coordinate-wise median and trimmed-mean remove extreme values before averaging. These rules converge to within $O(\sqrt{q/K})$ of optimal global model under $q$ fraction Byzantine adversaries across $K$ clients, though convergence rates degrade with increased Byzantine proportion.

The confluence of optimal transport and privacy-preserving federated learning remains largely unexplored in security contexts. While private optimal transport exists for data synthesis and privacy-preserving optimal transport has been developed for general domain adaptation, no work addresses specific requirements of federated intrusion detection where Byzantine adversaries may inject poisoned updates exploiting transport maps. This gap motivates development of Byzantine-robust optimal transport aggregation protocols that can tolerate malicious participants while maintaining privacy guarantees.

\section{Deep Learning for Encrypted Traffic Analysis}

The widespread deployment of encryption protocols has fundamentally transformed network security, creating a paradox where technologies designed to protect user privacy simultaneously provide malicious actors with channels for concealing attack patterns. Transport Layer Security version 1.3, QUIC protocol, and DNS over HTTPS have achieved near-universal adoption, with contemporary threat intelligence indicating that 85.9\% of modern cyberattacks leverage encrypted traffic channels~\cite{ref5,cao2021survey}. This prevalence renders traditional deep packet inspection and signature-based detection ineffective, as encrypted payloads prevent examination of application-layer content that historically enabled attack identification.

Decrypting traffic for inspection introduces untenable complications. Privacy violations conflict with regulations including GDPR that mandate protection of user data. Legal liability concerns arise when organizations access encrypted communications. Computational overhead proves prohibitive at network scale, as cryptographic operations for decryption impose millisecond-level latency that cannot be tolerated in high-throughput environments processing millions of packets per second. These constraints necessitate analysis approaches that operate exclusively on observable metadata without accessing encrypted payload contents.

Deep learning methodologies offer fundamentally different approaches by learning abstract representations from traffic metadata including packet timing sequences, size distributions, inter-arrival time variations, flow-level statistical characteristics, and protocol-specific handshake patterns~\cite{ref15,ref16,ref17,ref18,ref19}. Neural network architectures automatically extract discriminative patterns from these observable features, discovering latent representations that distinguish benign traffic from various attack types without examining encrypted payloads.

Anderson and McGrew~\cite{ref4} achieved 99.99\% accuracy for benign traffic and 85.80\% for malware using Random Forest classifiers operating on TLS metadata including cipher suites, extensions, and JA3/JA3S fingerprints extracted from encrypted handshakes before payload encryption commences. This demonstrates that handshake patterns alone carry substantial signal about application behavior and potential malicious intent. Shahla et al.~\cite{ref15} presented the VisQUIC dataset with image-based QUIC representation achieving 97\% accuracy for HTTP/3 response estimation without decryption, demonstrating viability of analyzing emerging protocols through visual representations of packet sequences.

Hybrid architectures combining complementary components demonstrate superior performance on encrypted traffic classification tasks. Yuan et al.~\cite{ref20} achieved 97.29\% accuracy integrating Graph Convolutional Networks for topology modeling with LSTM for temporal dependencies, using depthwise separable convolutions to reduce computational complexity while maintaining representational capacity. Li et al.~\cite{ref21} demonstrated 99.87\% accuracy with 0.13\% false positive rate on the BoT-IoT dataset, maintaining 90.2\% accuracy under adversarial attacks with 2.3ms processing time per flow, establishing feasibility of real-time deployment under adversarial conditions.

Transformer architectures adapted from natural language processing show particular advantages for modeling long-range dependencies in encrypted traffic sequences. Alkanhel et al.~\cite{ref23} introduced FlowTransformer achieving 93\% accuracy on CICIDS2018 through multi-head self-attention mechanisms that capture relationships across entire packet sequences without sequential processing bottlenecks. Liu et al.~\cite{ref22} presented TransECA-Net achieving 98.94\% accuracy on encrypted VPN traffic by combining transformer blocks with efficient channel attention modules that weight feature channels according to task relevance. Badr et al.~\cite{ref24} demonstrated transformer effectiveness in cloud environments with distributed traffic patterns spanning multiple geographic regions.

Few-shot learning approaches enable detection of novel attack types with minimal training examples, addressing the fundamental challenge that new attack vectors emerge continuously while labeled training data remains scarce. Bovenzi et al.~\cite{ref29} achieved 93.40\% accuracy on CICIDS2017 adapting to new attack types through meta-learning frameworks that learn to learn from limited data, enabling rapid adaptation to previously unseen threat categories. Chen et al.~\cite{ref30} introduced multimodal fusion with dual CNN-Transformer models for few-shot encrypted traffic classification. Ben Atitallah et al.~\cite{ref31} achieved 98.60\% on IoT datasets through Deep Infomax combined with Prototypical Networks, demonstrating effectiveness for resource-constrained devices with encrypted communications where computational budgets prohibit large models.

Federated learning enables privacy-preserving collaborative detection across organizations without centralizing sensitive encrypted traffic data. Wang et al.~\cite{ref32} implemented Paillier homomorphic encryption with Gradient Similarity Aggregation achieving 94.5\% accuracy while preventing gradient leakage attacks that could infer training data properties from shared model updates. Huang et al.~\cite{ref33} achieved 99.95\% accuracy demonstrating that distributed training can maintain performance comparable to centralized approaches while satisfying strict privacy requirements imposed by competitive concerns and regulatory compliance.

Explainable AI has emerged as essential for operational deployment of encrypted traffic detection systems, as security analysts require interpretable explanations for automated decisions. SHAP~\cite{ref48} has been established as superior to LIME~\cite{ref36} for network intrusion detection, providing consistent global and local feature attributions based on cooperative game theory. SHAP enables analysts to understand which packet features—timing patterns, size distributions, protocol metadata—drive classifications for encrypted traffic, building trust in automated systems and enabling refinement of detection logic based on analyst feedback.

\section{Graph Neural Networks for Network Security}

Graph neural networks provide natural architectural frameworks for modeling network topology and communication patterns that underlie security events. Network traffic inherently possesses graph structure, with hosts as nodes and communication flows as edges, making graph-based representations mathematically natural for security analysis. GNN architectures leverage message-passing mechanisms to propagate information across network topology, enabling detection of attacks that exploit lateral movement, privilege escalation, and coordinated multi-host campaigns that manifest as graph-level patterns rather than individual node behaviors.

Lin et al.~\cite{ref27} developed E-GRACL enhancing GraphSAGE with global attention mechanisms, achieving 96.8\% accuracy on UNSW-NB15 by propagating information across network topology through attention-weighted aggregation of neighborhood features. The global attention mechanism weights contributions from different network regions according to relevance for intrusion detection, enabling the model to focus on suspicious communication patterns indicative of attack campaigns. Yu et al.~\cite{ref28} applied self-supervised learning with GNNs achieving competitive performance with reduced labeling requirements, addressing the annotation bottleneck for encrypted traffic where manual labeling by security analysts proves costly and time-intensive.

Yuan et al.~\cite{ref20} achieved state-of-the-art results through integration of Graph Convolutional Networks for topology with LSTM for temporal dependencies, using depthwise separable convolutions to reduce computational complexity. This hybrid architecture enables simultaneous modeling of spatial graph structure capturing which hosts communicate and temporal evolution capturing when communications occur, providing richer representations than purely spatial or temporal approaches alone.

Recent security-specific GNN architectures incorporate domain knowledge about attack patterns. Kipf et al.~\cite{kipf2017semi} introduced semi-supervised learning on graphs, enabling propagation of label information from few labeled examples across unlabeled nodes through message passing, particularly relevant for security contexts where labeled attack traffic remains scarce. Jiang et al.~\cite{jiang2019graph} applied GNNs specifically to intrusion detection, demonstrating that graph-based architectures capture lateral movement and multi-stage attacks more effectively than traditional methods that treat network events independently.

The ResACAG framework uses residual attention and channel-wise aggregation achieving state-of-the-art performance on standard benchmarks through careful architectural design. Residual connections enable training of deep GNN architectures without gradient vanishing, while channel-wise aggregation weights different feature dimensions according to importance for detecting specific attack types. Attention mechanisms enable the model to focus on suspicious subgraphs within larger network topologies, improving computational efficiency by reducing processing of benign background traffic.

However, GNN approaches face challenges in dynamic network environments where topology evolves continuously. Temporal graph networks address this through dynamic graph representations, but computational complexity scales poorly with edge density in large-scale networks. Sampling strategies including node sampling and layer-wise sampling reduce computational burden but may miss rare attack patterns occurring in low-degree subgraphs. The integration of GNNs with continuous-time models remains largely unexplored, despite offering potential for modeling time-varying graph dynamics through differential equations on graph spaces.

\section{Temporal Point Processes}

Temporal point processes provide rigorous probabilistic frameworks for modeling irregular event sequences through conditional intensity functions that characterize instantaneous event occurrence rates. Given event history $\mathcal{H}_t = \{(t_i, k_i) : t_i < t\}$ containing timestamps and event types up to time $t$, the conditional intensity $\lambda^*(t)$ is defined as:
\begin{equation}
\lambda^*(t | \mathcal{H}_t) = \lim_{\delta \rightarrow 0^+} \frac{1}{\delta} \mathbb{P}(\text{event in } [t, t+\delta) | \mathcal{H}_t)
\end{equation}
This intensity characterizes the probability of observing an event in an infinitesimal time interval given the complete history, providing a complete statistical description of the point process.

Classical Hawkes processes~\cite{hawkes1971spectra,daley2003introduction} capture self-excitation where past events increase future occurrence probability through intensity functions of the form:
\begin{equation}
\lambda(t) = \mu + \sum_{t_i < t} \alpha \exp(-\beta(t - t_i))
\end{equation}
where $\mu$ represents background intensity, $\alpha$ quantifies self-excitation magnitude, and $\beta$ controls temporal decay. This formulation naturally models cascading phenomena where events trigger subsequent events, particularly relevant for security contexts including DDoS attacks where compromised hosts trigger additional compromise attempts, and worm propagation where infected machines scan for new victims.

Recent neural variants leverage deep learning for expressive intensity modeling. Du et al.~\cite{du2016recurrent} introduced recurrent neural networks for point processes, parameterizing intensity through hidden states updated continuously between events and discretely at event times. Transformer architectures~\cite{zuo2020transformer,zhang2020selfatt} apply multi-head self-attention to event sequences, enabling direct modeling of long-range dependencies without recurrent bottlenecks. Shchur et al.~\cite{shchur2021neural} propose intensity-free learning avoiding numerical integration challenges inherent in maximum likelihood estimation, instead directly predicting inter-event times through conditional distributions.

In intrusion detection contexts, Gao et al.~\cite{gao2024hplstm} combine Hawkes processes with LSTMs (HP-LSTM) to model self-exciting attack patterns, achieving improved detection of coordinated attacks through explicit modeling of temporal dependencies. However, HP-LSTM remains fundamentally discrete-time through LSTM architecture, operates at fixed temporal resolution determined by discretization granularity, and lacks principled uncertainty quantification for assessing prediction confidence.

Integration of point processes with neural ODEs offers compelling advantages by enabling continuous-time modeling of both smooth state evolution and discrete event occurrences. Continuous dynamics capture gradual threat accumulation during reconnaissance and lateral movement phases, while point process intensities model discrete exploitation attempts. However, this integration remains largely unexplored in security applications, despite mathematical naturalness of the coupled continuous-discrete formulation for modeling attack campaigns that unfold across multiple time scales.

\section{Bayesian Deep Learning for Uncertainty Quantification}

Bayesian neural networks~\cite{mackay1992bayesian,neal2012bayesian} provide principled approaches to uncertainty quantification through posterior distributions over parameters rather than point estimates. Given training data $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$, the Bayesian framework specifies prior distribution $p(\theta)$ over parameters and computes posterior $p(\theta | \mathcal{D})$ through Bayes' rule:
\begin{equation}
p(\theta | \mathcal{D}) = \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})} = \frac{p(\mathcal{D} | \theta) p(\theta)}{\int p(\mathcal{D} | \theta') p(\theta') d\theta'}
\end{equation}
Predictions integrate over the posterior, yielding predictive distributions that quantify uncertainty:
\begin{equation}
p(y | x, \mathcal{D}) = \int p(y | x, \theta) p(\theta | \mathcal{D}) d\theta
\end{equation}
This formulation distinguishes epistemic uncertainty from insufficient training data versus aleatoric uncertainty from inherent stochasticity, enabling calibrated confidence estimates essential for security applications where false positives and false negatives carry asymmetric costs.

Exact Bayesian inference proves intractable for neural networks due to high-dimensional parameter spaces and nonlinear likelihood functions that preclude analytical posterior computation. Practical approximations include variational inference~\cite{kingma2014auto,ranganath2014black} that approximates the intractable posterior with tractable variational distributions, optimizing variational parameters to minimize KL divergence between approximate and true posterior. Bayes by Backprop~\cite{blundell2015weight} performs variational inference through reparameterization gradients, enabling stochastic optimization of variational parameters jointly with standard backpropagation.

Monte Carlo dropout~\cite{gal2016dropout} provides a simpler approximation, interpreting dropout as variational inference with Bernoulli approximate posterior. At test time, performing multiple forward passes with different dropout masks and averaging predictions yields Monte Carlo estimates of predictive distributions. However, dropout posteriors remain restrictive, corresponding to specific variational families that may not capture true posterior structure.

Bayesian Neural ODEs~\cite{dandekar2021bayesian} extend uncertainty quantification to continuous-time dynamics, placing distributions over ODE parameters and propagating uncertainty through time via moment equations or Monte Carlo sampling. This enables quantifying confidence in predicted trajectories for dynamical systems, particularly relevant for security applications where attack progression unfolds continuously and uncertainty estimates inform response prioritization.

PAC-Bayesian theory~\cite{mcallester1999pac} provides generalization bounds relating empirical risk to population risk through KL divergence between posterior and prior:
\begin{equation}
\mathbb{E}_{\theta \sim Q}[\mathcal{R}(\theta)] \leq \mathbb{E}_{\theta \sim Q}[\hat{\mathcal{R}}(\theta)] + \sqrt{\frac{D_{KL}(Q||P) + \log(2N/\delta)}{2N}}
\end{equation}
where $P$ is prior, $Q$ is posterior, $N$ is sample size, and the bound holds with probability $1-\delta$. This establishes that posteriors close to priors generalize better, providing theoretical justification for Bayesian approaches beyond uncertainty quantification.

However, standard mean-field variational posteriors assuming parameter independence scale poorly for large models, requiring $O(p)$ variational parameters for $p$ model parameters. Structured variational inference addresses this through strategic dependency modeling, using low-rank plus diagonal covariance structures or normalizing flow posteriors that maintain expressiveness while reducing variational parameter count. Matrix-variate distributions leverage weight matrix structure in neural networks, parameterizing posteriors over weight matrices jointly rather than treating elements independently.

Security-critical applications demand calibrated uncertainty estimates where predicted confidence matches empirical accuracy. Calibration metrics including Expected Calibration Error measure average difference between confidence and accuracy across prediction bins, with well-calibrated models exhibiting small ECE. Temperature scaling and Platt scaling provide post-hoc calibration by rescaling model logits, though Bayesian approaches offer principled calibration through proper posterior inference when model specification matches data generation process.

The integration of Bayesian uncertainty quantification with continuous-time neural ODEs for intrusion detection remains largely unexplored. Existing work applies Bayesian neural ODEs primarily to regression problems in scientific computing and time-series forecasting, not to classification tasks in security contexts where calibrated uncertainty is essential for operational deployment. Furthermore, no work addresses computational challenges of Bayesian inference for large-scale intrusion detection systems processing millions of events per hour, where Monte Carlo sampling introduces latency that may violate real-time requirements.

\section{Temporal Graph Neural Networks and Continuous-Time Graph Learning}

Graph neural networks on temporal graphs combine the representational power of graph learning with temporal modeling, addressing network security scenarios where both topology and dynamics carry essential signal. Traditional GNNs operate on static snapshots, losing temporal information critical for detecting multi-stage attacks that unfold over time. Temporal graph networks extend GNNs to dynamic graphs where edges appear and disappear, node features evolve continuously, and events occur at irregular timestamps.

Discrete-time temporal graph networks process graph snapshots at fixed intervals. Temporal Graph Attention (TGAT)~\cite{xu2020inductive} combines graph attention mechanisms with temporal encoding, using time2vec functions to encode timestamps as learnable temporal representations. DySAT~\cite{sankar2020dysat} employs structural attention and temporal attention in hierarchical architecture, first aggregating spatial neighborhoods then integrating temporal evolution across snapshots. EvolveGCN~\cite{pareja2020evolvegcn} evolves GCN parameters through recurrent units (GRU or LSTM) rather than learning separate parameters for each timestamp, enabling parameter sharing across time while adapting to temporal dynamics. However, discretization introduces artificial temporal granularity that may miss fine-grained patterns occurring between snapshots.

Continuous-time temporal graph networks address discretization limitations through event-based processing. Temporal Graph Networks (TGN)~\cite{rossi2020temporal} maintain continuous node memory updated at each interaction, using memory modules that store compressed temporal information and message aggregators that combine information from temporal neighborhoods. Causal Anonymous Walks (CAW)~\cite{wang2021inductive} achieve strong inductive performance through anonymous walk sampling strategies that generalize across unseen nodes. Graph Mixture Density Networks (GMDN)~\cite{tan2023neighborhood} model temporal point processes on graphs through mixture density networks, enabling probabilistic modeling of interaction timings conditioned on graph structure.

Integration of neural ordinary differential equations with graph neural networks enables continuous-time graph dynamics through graph-structured differential equations. Neural Graph Differential Equations~\cite{poli2019graph} extend neural ODEs to graph-structured data, defining vector fields that respect graph topology through message-passing mechanisms integrated into ODE dynamics. Graph Neural Controlled Differential Equations~\cite{choi2022graph} incorporate time-varying signals as controls in differential equations on graphs, enabling modeling of external influences that modulate graph evolution. These approaches achieve strong performance on traffic forecasting and physical simulation tasks but remain largely unapplied to cybersecurity contexts.

Recent multi-granularity graph learning addresses hierarchical structure where graphs contain nested levels of abstraction. Multi-Resolution Graph Networks (MRAN)~\cite{hong2022multi} learn representations at multiple resolutions simultaneously, from fine-grained node-level to coarse-grained cluster-level patterns. GNN with Multi-Granularity Pooling (GNN-MgrPool)~\cite{zhao2022multi} develops hierarchical pooling operators that progressively coarsen graphs while preserving critical structural information. ConvAMC~\cite{liu2023multi} achieves multi-granularity anomaly detection through convolutional aggregation at service, trace, and infrastructure levels, demonstrating relevance for microservices security where attacks manifest across hierarchical dependencies.

Heterogeneous graph neural networks handle graphs with multiple node and edge types, prevalent in security graphs where nodes represent services, hosts, and users while edges represent communications, dependencies, and authentications. Heterogeneous Graph Transformer (HGT)~\cite{hu2020heterogeneous} uses type-specific attention mechanisms that respect node and edge heterogeneity. Dual-Edge Differentiation GNN (NEJE)~\cite{wang2023dual} differentiates between normal and anomalous edges through dual-channel processing, achieving strong performance on fraud detection benchmarks. These approaches enable explicit modeling of rich semantic structure in security graphs.

However, integration of continuous-time dynamics, multi-granularity representations, and heterogeneous graph structures remains unexplored despite compelling alignment with microservices security where service-level, trace-level, and container-level graphs evolve continuously with heterogeneous node and edge types.

\section{Large Language Models for Cybersecurity and Zero-Shot Threat Detection}

Large language models trained on massive text corpora have revolutionized natural language understanding, demonstrating emergent capabilities including few-shot learning, semantic reasoning, and knowledge transfer across domains. Transformer architectures~\cite{vaswani2017attention} with billions of parameters pre-trained on web-scale datasets enable zero-shot task performance through natural language prompts, providing capabilities that translate naturally to cybersecurity contexts where attacks exhibit semantic patterns in API requests, command sequences, and log messages.

BERT and its variants~\cite{devlin2019bert,sanh2019distilbert} employ masked language modeling on large text corpora, learning bidirectional context representations that capture semantic relationships. DistilBERT achieves 97\% of BERT performance with 40\% fewer parameters through knowledge distillation, enabling deployment in resource-constrained security monitoring infrastructure. RoBERTa~\cite{liu2019roberta} optimizes training procedures including dynamic masking and longer training, achieving state-of-the-art results on numerous NLP benchmarks. These models provide pre-trained representations that can be fine-tuned for security-specific tasks including malicious URL detection, phishing email identification, and log anomaly analysis.

GPT models~\cite{radford2018improving,radford2019language,brown2020language} demonstrate remarkable zero-shot and few-shot learning through prompt engineering, where task descriptions in natural language enable task performance without gradient updates. GPT-3 with 175 billion parameters achieves strong performance on diverse tasks including translation, question answering, and code generation through carefully designed prompts. However, deployment challenges including computational requirements (requiring multiple GPUs for inference) and privacy concerns (as prompts may contain sensitive security data sent to third-party APIs) limit direct application in security operations.

Parameter-efficient fine-tuning methods address computational barriers to adapting large language models for specialized tasks. Adapter layers~\cite{houlsby2019parameter} insert small trainable modules between frozen pre-trained layers, reducing trainable parameters by two orders of magnitude while maintaining performance. LoRA (Low-Rank Adaptation)~\cite{hu2021lora} parameterizes weight updates as low-rank decompositions $\Delta W = BA$ where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$, achieving comparable performance to full fine-tuning with 10,000× fewer trainable parameters. Prompt tuning~\cite{lester2021power} learns continuous prompt embeddings while keeping model parameters frozen, enabling task adaptation with only thousands of additional parameters.

Recent applications of LLMs to cybersecurity demonstrate strong zero-shot threat detection capabilities. Fang et al.~\cite{fang2023llm4ids} apply large language models to network intrusion detection, using prompt engineering to describe attack patterns and achieving 91.8\% accuracy on CICIDS2017 without task-specific training. Wei et al.~\cite{wei2023secgpt} introduce SecGPT for security information and event management (SIEM), using chain-of-thought prompting to analyze security logs and generate explanatory summaries for analysts. Li et al.~\cite{li2023vulbert} develop VulBERT specifically for vulnerability detection in source code, achieving 93.7\% F1 score through pre-training on security-related code repositories.

However, federated learning with large language models faces substantial challenges. Communication costs scale with model size, rendering naive parameter averaging infeasible for billion-parameter models across distributed organizations. Differential privacy noise calibrated to model sensitivity degrades performance catastrophically when parameter count is large, as noise variance must increase proportionally to maintain privacy guarantees. Byzantine attacks can exploit high-dimensional parameter spaces to inject poisoned updates that evade detection while degrading model performance.

Federated learning with parameter-efficient methods offers promising directions. FedAdapter~\cite{zhang2023fedadapter} applies federated learning to adapter modules, communicating only adapter parameters rather than full models and achieving 1000× communication reduction. FedLoRA applies federated averaging to LoRA matrices, demonstrating that low-rank structure enables effective aggregation with minimal communication. FedPrompt~\cite{kuang2023fedprompt} aggregates prompt embeddings across clients, requiring communication of only prompt parameters (typically $<10^4$ values) rather than full model parameters ($>10^8$ values).

The application of federated large language models to API security remains unexplored, despite natural alignment between semantic reasoning capabilities of LLMs and the syntactic-semantic patterns characteristic of API abuse and injection attacks. Furthermore, no work addresses Byzantine robustness in federated LLM fine-tuning for security contexts where malicious clients may inject poisoned prompts or adapter updates designed to create backdoors or degrade detection of specific attack types.

\section{Post-Quantum Cryptography and Quantum Machine Learning for Security}

The advent of large-scale fault-tolerant quantum computers poses existential threats to contemporary cryptographic infrastructure through Shor's algorithm~\cite{shor1994algorithms}, which factors integers and computes discrete logarithms in polynomial time. This capability breaks RSA, Diffie-Hellman, and elliptic curve cryptography that underpin TLS/SSL, digital signatures, and key exchange protocols securing modern internet communications. The National Institute of Standards and Technology (NIST) has responded through standardization of post-quantum cryptographic algorithms resistant to quantum attacks, finalizing standards in August 2024~\cite{nist2024pqc}.

NIST-standardized post-quantum algorithms employ mathematical problems believed intractable for quantum computers. ML-KEM (Module Lattice-Based Key Encapsulation Mechanism), based on Kyber, provides key encapsulation through learning-with-errors (LWE) problems on structured lattices. ML-DSA (Module Lattice-Based Digital Signature Algorithm), based on Dilithium, achieves digital signatures through fiat-shamir with aborts applied to lattice-based identification protocols. SLH-DSA (Stateless Hash-Based Digital Signature Algorithm), based on SPHINCS+, uses hash functions for signatures without requiring state management across signing operations. These algorithms exhibit substantially different computational profiles and traffic patterns compared to classical cryptography.

Early deployments of post-quantum cryptography reveal distinct network characteristics. Sikeridis et al.~\cite{sikeridis2020post} analyze performance implications of post-quantum TLS, finding that Kyber and Dilithium increase handshake sizes by 5-10× compared to ECDH and ECDSA while reducing computational latency by 20-40\% due to simpler arithmetic. Schwabe et al.~\cite{schwabe2023crystals} measure CRYSTALS (Kyber+Dilithium) performance on embedded devices, achieving sub-millisecond operations on ARM Cortex-M4 processors. However, these performance studies do not address security monitoring or intrusion detection for post-quantum traffic.

Quantum machine learning explores computational advantages of quantum algorithms for machine learning tasks. Variational Quantum Classifiers (VQC)~\cite{schuld2020circuit} parameterize quantum circuits through rotation angles, optimizing parameters via classical gradient descent to minimize classification loss. VQCs leverage exponentially large Hilbert spaces ($2^n$ dimensions for $n$ qubits) to represent complex nonlinear decision boundaries with polynomial parameter count. Quantum Neural Networks (QNN)~\cite{abbas2020power} extend classical neural architectures to quantum circuits, achieving provable advantages for specific function classes including parity learning and certain Fourier series.

Recent implementations demonstrate VQC viability on noisy intermediate-scale quantum (NISQ) devices. Havlíček et al.~\cite{havlicek2019supervised} achieve quantum advantage for kernel estimation tasks using 2-qubit superconducting processors. Schuld and Killoran~\cite{schuld2019quantum} demonstrate quantum advantage for quantum-enhanced feature spaces, showing exponential separation between quantum and classical kernel methods for specific datasets. However, quantum advantage for practical machine learning tasks on real-world data remains contested, with classical neural networks achieving competitive or superior performance on standard benchmarks.

Quantum adversarial machine learning addresses security implications of quantum computing for ML systems. Quantum adversarial attacks~\cite{lu2021quantum} leverage Grover's algorithm~\cite{grover1996fast} to accelerate adversarial example search, reducing queries from $O(N)$ to $O(\sqrt{N})$ for search spaces of size $N$. Quantum Generative Adversarial Networks (qGAN)~\cite{lloyd2018quantum} generate adversarial examples through quantum circuits, potentially achieving exponential speedup for sampling from complex distributions. However, certified defense mechanisms including randomized smoothing~\cite{cohen2019certified} provide provable robustness guarantees that hold even against quantum-accelerated attacks.

Hybrid classical-quantum architectures combine classical neural networks with quantum circuits, enabling gradual integration of quantum processing into existing ML pipelines. Quantum feature encoding maps classical data to quantum states through angle encoding, amplitude encoding, or basis encoding. Classical post-processing extracts measurement outcomes and applies final classification layers, enabling integration of quantum components into standard training pipelines using automatic differentiation through parameter-shift rules~\cite{schuld2019evaluating} for quantum gradient estimation.

However, integration of post-quantum cryptographic traffic analysis with quantum machine learning remains entirely unexplored. No work addresses detection of attacks on post-quantum encrypted traffic, despite fundamental differences in handshake patterns, packet size distributions, and timing characteristics between classical and post-quantum protocols. Furthermore, no work provides certified adversarial robustness against quantum-enhanced attacks for intrusion detection systems, despite compelling threat models where attackers leverage quantum computers to craft evasive adversarial traffic perturbations.

\section{Summary}

This chapter has reviewed eleven interconnected research domains that collectively inform the theoretical foundations and methodological approaches developed in subsequent chapters. Network intrusion detection systems have evolved from signature-based approaches to sophisticated deep learning architectures, yet no existing system integrates continuous-time dynamics with point processes, quantum computing, and large language model reasoning. Neural ordinary differential equations provide elegant frameworks for modeling continuous dynamics but remain largely unapplied to security domains. Optimal transport offers principled distribution alignment for domain adaptation but lacks privacy-preserving variants for federated security contexts. Privacy-preserving machine learning and federated learning enable collaborative detection while protecting sensitive data, though Byzantine robustness remains challenging under adversarial conditions.

Deep learning for encrypted traffic analysis demonstrates that metadata alone carries substantial signal for threat detection, though most approaches discretize time and miss fine-grained temporal patterns. Graph neural networks capture network topology but struggle with dynamic graphs and temporal evolution. Temporal point processes provide rigorous frameworks for irregular event sequences but have not been integrated with continuous-time neural models on graphs. Bayesian deep learning offers principled uncertainty quantification but faces computational and calibration challenges in security applications.

Temporal graph neural networks combine graph representations with continuous-time dynamics, achieving strong performance on forecasting tasks but remaining unapplied to cybersecurity contexts where multi-granularity hierarchical graphs (service, trace, container levels) exhibit heterogeneous node and edge types evolving continuously. Large language models demonstrate remarkable zero-shot learning and semantic reasoning capabilities, yet federated LLM training for security applications faces communication efficiency, differential privacy, and Byzantine robustness challenges when adapting billion-parameter models across distributed organizations. Post-quantum cryptography addresses quantum computing threats to current encryption, while quantum machine learning explores quantum computational advantages for classification tasks, yet no work integrates post-quantum traffic analysis with certified adversarial robustness against quantum-enhanced attacks.

The identified gaps across these eleven domains motivate the integrated frameworks developed in subsequent chapters, combining continuous-time graph neural ODEs with multi-granularity representations, parameter-efficient federated LLM adaptation with Byzantine-robust aggregation, hybrid classical-quantum intrusion detection with certified robustness, and privacy-preserving optimal transport with differential privacy guarantees. Subsequent chapters develop mathematical foundations and present specific architectural instantiations addressing continuous-time temporal graph learning (Chapter 8), privacy-preserving LLM federation for API security (Chapter 7), post-quantum adversarial robustness (Chapter 6), and unified experimental validation across 25 diverse datasets (Chapter 9).
