\chapter{Mathematical Proofs and Derivations}
\label{app:proofs}

This appendix provides detailed mathematical proofs for the key theorems presented in the main text.

\section{Proof of Theorem 3.1: Adjoint Gradient Stability}

\begin{theorem*}[Adjoint Gradient Stability for TA-BN-ODE]
Under Lipschitz continuity assumptions on the ODE function and time-dependent normalization statistics, the adjoint method for TA-BN-ODE produces bounded gradients.
\end{theorem*}

\begin{proof}
Let $f_\theta(h, t)$ denote the ODE function with parameters $\theta$ and $\gamma(t), \beta(t), \mu(t), \sigma(t)$ represent time-dependent normalization parameters. The forward dynamics satisfy:
\begin{equation*}
\frac{dh(t)}{dt} = f_\theta(h(t), t; \gamma(t), \beta(t), \mu(t), \sigma(t))
\end{equation*}

The adjoint state $a(t) = \partial \mathcal{L}/\partial h(t)$ evolves according to:
\begin{equation*}
\frac{da(t)}{dt} = -a(t)^T \frac{\partial f_\theta}{\partial h}(h(t), t)
\end{equation*}

Assuming $f_\theta$ is $L_f$-Lipschitz in $h$, we have:
\begin{equation*}
\left\|\frac{\partial f_\theta}{\partial h}\right\| \leq L_f
\end{equation*}

Therefore:
\begin{align*}
\frac{d\|a(t)\|}{dt} &= \frac{a(t)^T}{\|a(t)\|} \frac{da(t)}{dt}\\
&= -\frac{a(t)^T}{\|a(t)\|} a(t)^T \frac{\partial f_\theta}{\partial h}\\
&\leq \|a(t)\| \left\|\frac{\partial f_\theta}{\partial h}\right\|\\
&\leq L_f \|a(t)\|
\end{align*}

By Gronwall's inequality:
\begin{equation*}
\|a(t_0)\| \leq \|a(t_1)\| \exp(L_f (t_1 - t_0))
\end{equation*}

For time-dependent normalization with bounded derivatives $\|\partial \gamma(t)/\partial t\| \leq C_\gamma$, $\|\partial \beta(t)/\partial t\| \leq C_\beta$, the augmented Lipschitz constant becomes $\tilde{L}_f = L_f + C_\gamma + C_\beta$, maintaining bounded adjoint evolution.

The parameter gradients:
\begin{equation*}
\frac{\partial \mathcal{L}}{\partial \theta} = -\int_{t_1}^{t_0} a(t)^T \frac{\partial f_\theta}{\partial \theta} dt
\end{equation*}
remain bounded since $\|a(t)\|$ is bounded and $\partial f_\theta/\partial \theta$ is bounded by network weight constraints.
\end{proof}

\section{Proof of Theorem 3.2: PAC-Bayesian Risk Bound}

\begin{theorem*}[PAC-Bayesian Risk Bound]
For structured variational posterior $q(\theta)$, with probability at least $1 - \delta$ over training data $\mathcal{D}$:
\begin{equation*}
\mathbb{E}_{\theta \sim q}[\mathcal{R}(\theta)] \leq \mathbb{E}_{\theta \sim q}[\hat{\mathcal{R}}(\theta)] + \sqrt{\frac{\mathrm{KL}(q \| p) + \log(2n/\delta)}{2n}}
\end{equation*}
where $\mathcal{R}$ is true risk, $\hat{\mathcal{R}}$ is empirical risk, $p$ is prior, and $n$ is sample size.
\end{theorem*}

\begin{proof}
By McAllester's PAC-Bayesian theorem, for any distribution $q$ over hypotheses:
\begin{equation*}
\Pr_{\mathcal{D}}\left[\mathbb{E}_{\theta \sim q}[\mathcal{R}(\theta)] > \mathbb{E}_{\theta \sim q}[\hat{\mathcal{R}}(\theta)] + \sqrt{\frac{\mathrm{KL}(q \| p) + \log(2n/\delta)}{2n}}\right] \leq \delta
\end{equation*}

For structured mean-field variational posterior:
\begin{equation*}
q(\theta) = \prod_{i=1}^m q_i(\theta_i | \phi_i)
\end{equation*}

The KL divergence decomposes as:
\begin{equation*}
\mathrm{KL}(q \| p) = \sum_{i=1}^m \mathrm{KL}(q_i \| p_i)
\end{equation*}

Each factor $q_i(\theta_i | \phi_i) = \mathcal{N}(\mu_i, \sigma_i^2)$ with prior $p_i = \mathcal{N}(0, \tau^2)$ yields:
\begin{equation*}
\mathrm{KL}(q_i \| p_i) = \frac{1}{2}\left(\frac{\mu_i^2 + \sigma_i^2}{\tau^2} - 1 - \log\frac{\sigma_i^2}{\tau^2}\right)
\end{equation*}

Summing over all parameters and substituting into the PAC-Bayesian bound completes the proof.
\end{proof}

\section{Proof of Theorem 4.1: Privacy-Preserving Optimal Transport}

\begin{theorem*}[Differential Privacy for Noisy Sinkhorn]
Adding Gaussian noise $\mathcal{N}(0, \sigma^2 I)$ to empirical marginals with $\sigma^2 = 2\Delta^2\log(1.25/\delta)/\epsilon^2$ and $\Delta = 2/n$ achieves $(\epsilon, \delta)$-differential privacy for Sinkhorn-based optimal transport.
\end{theorem*}

\begin{proof}
The empirical marginal $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}$ has global sensitivity:
\begin{equation*}
\Delta = \max_{D, D'} \|\hat{\mu}_D - \hat{\mu}_{D'}\|_1 = \frac{2}{n}
\end{equation*}
for adjacent datasets $D, D'$ differing in one record.

The Gaussian mechanism adds noise $\xi \sim \mathcal{N}(0, \sigma^2 I)$ to produce noisy marginal $\tilde{\mu} = \hat{\mu} + \xi$.

By the Gaussian mechanism theorem (Dwork et al.), this achieves $(\epsilon, \delta)$-differential privacy when:
\begin{equation*}
\sigma \geq \frac{\Delta}{\epsilon}\sqrt{2\log(1.25/\delta)}
\end{equation*}

Sinkhorn iterations operate on noisy marginals $\tilde{\mu}, \tilde{\nu}$ through:
\begin{align*}
u^{(k+1)} &= \tilde{\mu} \oslash (K v^{(k)})\\
v^{(k+1)} &= \tilde{\nu} \oslash (K^T u^{(k+1)})
\end{align*}
where $K_{ij} = \exp(-C_{ij}/\lambda)$ and $\oslash$ denotes element-wise division.

The transport plan $\gamma^* = \text{diag}(u^*) K \text{diag}(v^*)$ inherits privacy from noisy marginals through post-processing property: any function of private data remains private with the same parameters.
\end{proof}

\section{Proof of Theorem 4.2: Byzantine-Robust Convergence}

\begin{theorem*}[Convergence Under Byzantine Attacks]
Trimmed mean aggregation with trimming fraction $\beta > q$ converges as:
\begin{equation*}
\mathbb{E}[\mathcal{L}(\theta^{(R)})] - \mathcal{L}(\theta^*) \leq O\left(\frac{1}{R} + \frac{q}{\sqrt{K}}\right)
\end{equation*}
where $q < 0.5$ is Byzantine fraction, $R$ is rounds, $K$ is clients.
\end{theorem*}

\begin{proof}
Let $\theta_1^{(r)}, \ldots, \theta_K^{(r)}$ denote client updates at round $r$, with $qK$ Byzantine clients.

Define the trimmed mean for each dimension $j$:
\begin{equation*}
\bar{\theta}^{(r)}[j] = \frac{1}{K - 2\beta K} \sum_{k \in \mathcal{K}_{\text{trim}}(j)} \theta_k^{(r)}[j]
\end{equation*}

Under $L$-smoothness and $\mu$-strong convexity:
\begin{equation*}
\mathbb{E}[\mathcal{L}(\bar{\theta}^{(r)})] \leq \mathcal{L}(\theta^*) + \frac{L}{2}\mathbb{E}[\|\bar{\theta}^{(r)} - \theta^*\|^2]
\end{equation*}

For honest clients following gradient descent:
\begin{equation*}
\mathbb{E}[\|\theta_k^{(r)} - \theta^*\|^2] \leq \left(1 - \frac{\mu\eta}{2}\right)^r \|\theta_0 - \theta^*\|^2 + \frac{\sigma^2}{\mu K}
\end{equation*}

Trimming removes Byzantine updates, yielding:
\begin{equation*}
\mathbb{E}[\|\bar{\theta}^{(r)} - \theta^*\|^2] \leq \frac{1}{(1-q)K} \sum_{k \text{ honest}} \mathbb{E}[\|\theta_k^{(r)} - \theta^*\|^2] + O\left(\frac{q^2}{K}\right)
\end{equation*}

Substituting and telescoping over $R$ rounds yields the stated convergence rate.
\end{proof}

\section{Additional Derivations}

\subsection{Sinkhorn Iteration Convergence}

The Sinkhorn algorithm solves the entropic regularized optimal transport:
\begin{equation*}
\min_{\gamma \in \Pi(\mu, \nu)} \langle C, \gamma \rangle - \lambda H(\gamma)
\end{equation*}

Setting Lagrangian derivatives to zero yields:
\begin{equation*}
\gamma_{ij}^* = u_i \exp(-C_{ij}/\lambda) v_j
\end{equation*}

The scaling vectors $u, v$ satisfy:
\begin{align*}
u_i &= \frac{\mu_i}{\sum_j \exp(-C_{ij}/\lambda) v_j}\\
v_j &= \frac{\nu_j}{\sum_i u_i \exp(-C_{ij}/\lambda)}
\end{align*}

Defining $K_{ij} = \exp(-C_{ij}/\lambda)$, the Sinkhorn iterations become:
\begin{align*}
u^{(k+1)} &= \mu \oslash (K v^{(k)})\\
v^{(k+1)} &= \nu \oslash (K^T u^{(k+1)})
\end{align*}

Convergence follows from contraction mapping properties in the log domain.

\subsection{Evidence Lower Bound Derivation}

For Bayesian neural networks with latent variables $\theta$:
\begin{align*}
\log p(\mathcal{D}) &= \log \int p(\mathcal{D} | \theta) p(\theta) d\theta\\
&= \log \int \frac{p(\mathcal{D} | \theta) p(\theta)}{q(\theta)} q(\theta) d\theta\\
&\geq \int q(\theta) \log \frac{p(\mathcal{D} | \theta) p(\theta)}{q(\theta)} d\theta\\
&= \mathbb{E}_{q(\theta)}[\log p(\mathcal{D} | \theta)] - \mathrm{KL}(q(\theta) \| p(\theta))\\
&= \text{ELBO}(q)
\end{align*}

where the inequality follows from Jensen's inequality. Maximizing the ELBO provides a tractable lower bound on the log marginal likelihood.

\section{Foundational Theorems for Adversarial Resilience}

This section provides formal mathematical foundations establishing the adversarial resilience properties emphasized in this dissertation.

\subsection{Robustness-Accuracy Trade-off}

\begin{theorem*}[Robustness-Accuracy Trade-off]
For any classifier $f: \mathcal{X} \rightarrow \mathcal{Y}$ and dataset $\mathcal{D}$, the sum of standard accuracy $\text{Acc}(f)$ and robust accuracy $R_{\epsilon,p}(f,\mathcal{D})$ is fundamentally bounded by:
\begin{equation*}
\text{Acc}(f) + R_{\epsilon,p}(f,\mathcal{D}) \leq 1 + \mathbb{E}_{(x,y) \sim \mathcal{D}}[\text{Lip}(f,x) \cdot \epsilon]
\end{equation*}
where $\text{Lip}(f,x)$ is the local Lipschitz constant of $f$ at $x$, and $\epsilon$ is the perturbation budget under $\ell_p$ norm.
\end{theorem*}

\begin{proof}
Let $f$ be any classifier and $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ be the test dataset. Standard accuracy is:
\begin{equation*}
\text{Acc}(f) = \frac{1}{n}\sum_{i=1}^n \mathbb{I}[f(x_i) = y_i]
\end{equation*}

Robust accuracy under $\ell_p$ perturbations with budget $\epsilon$ is:
\begin{equation*}
R_{\epsilon,p}(f,\mathcal{D}) = \frac{1}{n}\sum_{i=1}^n \mathbb{I}\left[\min_{\|\delta\|_p \leq \epsilon} f(x_i + \delta) = y_i\right]
\end{equation*}

For each point $x_i$, if $f(x_i) = y_i$ (correctly classified), robust accuracy requires $f(x_i + \delta) = y_i$ for all $\|\delta\|_p \leq \epsilon$. By Lipschitz continuity:
\begin{equation*}
\|f(x_i + \delta) - f(x_i)\| \leq \text{Lip}(f,x_i) \|\delta\|_p \leq \text{Lip}(f,x_i) \epsilon
\end{equation*}

When $\text{Lip}(f,x_i) \epsilon \geq \text{margin}(f,x_i,y_i)$ (decision boundary margin), adversarial perturbations can cross the decision boundary, making robust classification impossible.

Summing over all points and normalizing:
\begin{equation*}
\text{Acc}(f) + R_{\epsilon,p}(f,\mathcal{D}) \leq 1 + \mathbb{E}_{(x,y) \sim \mathcal{D}}[\text{Lip}(f,x) \cdot \epsilon]
\end{equation*}

This establishes the fundamental trade-off: improving robustness (larger robust radius) requires reducing local Lipschitz constants or accepting lower standard accuracy.
\end{proof}

\subsection{Stochastic Robustness Enhancement}

\begin{theorem*}[Stochastic Robustness Enhancement]
For a deterministic classifier $f_{\text{det}}$ and its stochastic counterpart $f_{\text{stoch}}$ with noise injection $\xi \sim \mathcal{N}(0, \sigma^2 I)$, the expected robust accuracy satisfies:
\begin{equation*}
\mathbb{E}[R_{\epsilon}(f_{\text{stoch}})] \geq R_{\epsilon}(f_{\text{det}}) + \Omega(\sigma)
\end{equation*}
for appropriately chosen noise level $\sigma$.
\end{theorem*}

\begin{proof}
Consider the stochastic classifier $f_{\text{stoch}}(x) = f_{\text{det}}(x + \xi)$ where $\xi \sim \mathcal{N}(0, \sigma^2 I)$. For adversarial perturbation $\delta$ with $\|\delta\| \leq \epsilon$:
\begin{align*}
\mathbb{P}[f_{\text{stoch}}(x + \delta) = y] &= \mathbb{P}[f_{\text{det}}(x + \delta + \xi) = y]\\
&= \mathbb{E}_{\xi}[\mathbb{I}[f_{\text{det}}(x + \delta + \xi) = y]]
\end{align*}

The noise $\xi$ creates a probabilistic shield, making it harder for adversaries to find consistent adversarial directions. For fixed $\delta$, the effective perturbation $\delta + \xi$ has increased variance:
\begin{equation*}
\text{Var}[\delta + \xi] = \sigma^2 I
\end{equation*}

By concentration of measure in high dimensions, for $d$-dimensional input space:
\begin{equation*}
\|\delta + \xi\|^2 \approx \|\delta\|^2 + d\sigma^2
\end{equation*}

This effective perturbation enlargement creates a gap between deterministic and stochastic robust accuracy. Specifically, adversarial perturbations that successfully fool $f_{\text{det}}$ at distance $\epsilon$ now require overcoming additional variance $\sigma^2$, leading to:
\begin{equation*}
\mathbb{E}[R_{\epsilon}(f_{\text{stoch}})] \geq R_{\epsilon}(f_{\text{det}}) + \Omega\left(\frac{\sigma}{\epsilon}\right)
\end{equation*}

For appropriate noise level $\sigma = O(\epsilon)$, this provides a constant factor improvement: $\Omega(\sigma)$.
\end{proof}

\subsection{Uncertainty Decomposition}

\begin{theorem*}[Uncertainty Decomposition into Aleatoric and Epistemic Components]
For a Bayesian neural network with posterior $p(\theta | \mathcal{D})$ over parameters $\theta$ and predictive distribution $p(y | x, \mathcal{D})$, the total predictive variance decomposes as:
\begin{equation*}
\text{Var}[y | x, \mathcal{D}] = \mathbb{E}_{\theta \sim p(\theta | \mathcal{D})}[\text{Var}[y | x, \theta]] + \text{Var}_{\theta \sim p(\theta | \mathcal{D})}[\mathbb{E}[y | x, \theta]]
\end{equation*}
where the first term is \textbf{aleatoric uncertainty} (irreducible data noise) and the second is \textbf{epistemic uncertainty} (model uncertainty reducible with more data).
\end{theorem*}

\begin{proof}
By the law of total variance:
\begin{equation*}
\text{Var}[y | x, \mathcal{D}] = \mathbb{E}_{\theta}[\text{Var}[y | x, \theta]] + \text{Var}_{\theta}[\mathbb{E}[y | x, \theta]]
\end{equation*}

The first term represents uncertainty inherent in the data generation process, independent of model parameters. For fixed $\theta$, $\text{Var}[y | x, \theta]$ captures noise in the label $y$ given features $x$—this is \textbf{aleatoric uncertainty}.

The second term represents variability in predictions across different parameter values sampled from the posterior $p(\theta | \mathcal{D})$. This reflects our uncertainty about which model parameters are correct—this is \textbf{epistemic uncertainty}.

Expanding the second term:
\begin{align*}
\text{Var}_{\theta}[\mathbb{E}[y | x, \theta]] &= \mathbb{E}_{\theta}[\mathbb{E}[y | x, \theta]^2] - \mathbb{E}_{\theta}[\mathbb{E}[y | x, \theta]]^2\\
&= \mathbb{E}_{\theta}[\mu(x, \theta)^2] - \mathbb{E}_{\theta}[\mu(x, \theta)]^2
\end{align*}
where $\mu(x, \theta) = \mathbb{E}[y | x, \theta]$.

This decomposition is critical for security applications: aleatoric uncertainty indicates inherently ambiguous network traffic requiring human review, while high epistemic uncertainty signals distribution shift or adversarial manipulation requiring model updates.
\end{proof}

\subsection{MC-Dropout Convergence}

\begin{theorem*}[Monte Carlo Dropout Convergence]
For a neural network with dropout and $M$ Monte Carlo forward passes, the estimated predictive mean $\hat{\mu}_M(x)$ and variance $\hat{\sigma}^2_M(x)$ converge to their true values at rate $O(1/\sqrt{M})$:
\begin{equation*}
|\hat{\mu}_M(x) - \mu(x)| = O_p(1/\sqrt{M}), \quad |\hat{\sigma}^2_M(x) - \sigma^2(x)| = O_p(1/\sqrt{M})
\end{equation*}
\end{theorem*}

\begin{proof}
Dropout with rate $p$ induces a distribution over network architectures. Let $\mathcal{M}$ denote the random subnetwork selected by dropout mask $\mathcal{M} \sim \text{Bernoulli}(1-p)^d$. The predictive distribution is:
\begin{equation*}
p(y | x, \mathcal{D}) = \mathbb{E}_{\mathcal{M}}[p(y | x, \mathcal{M}, \mathcal{D})]
\end{equation*}

With $M$ Monte Carlo samples $\{\mathcal{M}_i\}_{i=1}^M$, the estimated mean is:
\begin{equation*}
\hat{\mu}_M(x) = \frac{1}{M}\sum_{i=1}^M f(x; \mathcal{M}_i)
\end{equation*}

By the Central Limit Theorem, for i.i.d. samples:
\begin{equation*}
\sqrt{M}(\hat{\mu}_M(x) - \mu(x)) \xrightarrow{d} \mathcal{N}(0, \sigma^2_f)
\end{equation*}
where $\sigma^2_f = \text{Var}_{\mathcal{M}}[f(x; \mathcal{M})]$.

Therefore:
\begin{equation*}
|\hat{\mu}_M(x) - \mu(x)| = O_p(1/\sqrt{M})
\end{equation*}

Similarly, the variance estimator:
\begin{equation*}
\hat{\sigma}^2_M(x) = \frac{1}{M}\sum_{i=1}^M (f(x; \mathcal{M}_i) - \hat{\mu}_M(x))^2
\end{equation*}
converges at the same rate by the continuous mapping theorem.

For practical security applications, $M = 50$ provides $1/\sqrt{50} \approx 14\%$ relative error, balancing accuracy and computational cost for real-time intrusion detection.
\end{proof}

\subsection{Stochastic Attention Robustness}

\begin{theorem*}[Stochastic Attention Robustness]
For transformer attention with stochastic noise injection $\xi \sim \mathcal{N}(0, \sigma^2 I)$ into query-key products, adversarial perturbations $\delta$ with $\|\delta\| \leq \epsilon$ satisfy:
\begin{equation*}
\mathbb{E}_{\xi}[\|\text{Attention}(Q + \delta_Q, K + \delta_K) - \text{Attention}(Q, K)\|_F] \leq \frac{\epsilon}{\sigma}\exp\left(-\frac{\sigma^2}{2\epsilon^2}\right)
\end{equation*}
for appropriately chosen noise level $\sigma = \Theta(\epsilon)$.
\end{theorem*}

\begin{proof}
Standard attention computes:
\begin{equation*}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation*}

With stochastic noise injection:
\begin{equation*}
\text{Attention}_{\text{stoch}}(Q, K, V) = \text{softmax}\left(\frac{(Q + \xi_Q)(K + \xi_K)^T}{\sqrt{d_k}}\right)V
\end{equation*}
where $\xi_Q, \xi_K \sim \mathcal{N}(0, \sigma^2 I)$.

Adversarial perturbations $(Q + \delta_Q, K + \delta_K)$ with $\|\delta_Q\|, \|\delta_K\| \leq \epsilon$ attempt to shift attention weights. The noisy attention score becomes:
\begin{equation*}
S_{ij} = \frac{(q_i + \delta_{q_i} + \xi_{q_i})(k_j + \delta_{k_j} + \xi_{k_j})^T}{\sqrt{d_k}}
\end{equation*}

The adversarial shift is:
\begin{equation*}
\Delta S_{ij} = \frac{(q_i + \xi_{q_i})\delta_{k_j}^T + \delta_{q_i}(k_j + \xi_{k_j})^T + \delta_{q_i}\delta_{k_j}^T}{\sqrt{d_k}}
\end{equation*}

Taking expectation over noise:
\begin{align*}
\mathbb{E}_{\xi}[\Delta S_{ij}] &= \frac{q_i\delta_{k_j}^T + \delta_{q_i}k_j^T + \delta_{q_i}\delta_{k_j}^T}{\sqrt{d_k}}\\
&\leq \frac{2\epsilon\|q_i\| + \epsilon^2}{\sqrt{d_k}}
\end{align*}

However, the variance introduced by noise:
\begin{equation*}
\text{Var}_{\xi}[\Delta S_{ij}] = \frac{\sigma^2(\|\delta_{q_i}\|^2 + \|\delta_{k_j}\|^2)}{\sqrt{d_k}} \leq \frac{2\sigma^2\epsilon^2}{\sqrt{d_k}}
\end{equation*}

For $\sigma = \Theta(\epsilon)$, the variance dominates the mean, making consistent adversarial attention manipulation difficult. Applying concentration inequalities:
\begin{equation*}
\mathbb{P}[|\Delta S_{ij}| \geq t\sigma] \leq 2\exp\left(-\frac{t^2}{4}\right)
\end{equation*}

Setting $t = \epsilon/\sigma$ and integrating over attention matrix yields the stated bound.
\end{proof}

