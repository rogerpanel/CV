{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOo6WXLrzcHFCKoQ7+DmKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerpanel/CV/blob/main/Evasion_XIV_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "upgraded with DQN"
      ],
      "metadata": {
        "id": "zmh3PuIXYp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorporating RL - DQN\n",
        "\n",
        "# pip install tensorflow tensorflow-addons gym numpy\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import gc\n",
        "import tensorflow.keras.mixed_precision as mixed_precision\n",
        "import random\n"
      ],
      "metadata": {
        "id": "g62nCZADcMcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "mdD-wFSQcjDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Enable mixed precision training\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# PREPROCESSING\n",
        "\n",
        "# Function to save dataset\n",
        "def save_dataset(X_train, X_test, y_train, y_test, scaler, label_encoder, file_path):\n",
        "    try:\n",
        "        np.savez(file_path,\n",
        "                 X_train=X_train, X_test=X_test,\n",
        "                 y_train=y_train, y_test=y_test,\n",
        "                 scaler_mean=scaler.mean_, scaler_scale=scaler.scale_,\n",
        "                 label_encoder_classes=label_encoder.classes_)\n",
        "        print(f\"Dataset saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving dataset: {e}\")\n",
        "\n",
        "# Function to load dataset\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        data = np.load(file_path, allow_pickle=True)\n",
        "        X_train, X_test = data['X_train'], data['X_test']\n",
        "        y_train, y_test = data['y_train'], data['y_test']\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.mean_ = data['scaler_mean']\n",
        "        scaler.scale_ = data['scaler_scale']\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder.classes_ = data['label_encoder_classes']\n",
        "\n",
        "        print(f\"Dataset loaded from {file_path}\")\n",
        "        return X_train, X_test, y_train, y_test, scaler, label_encoder\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Function to read data in chunks and handle data types\n",
        "def read_data_in_chunks(file_path, chunk_size=10000):\n",
        "    for chunk in pd.read_csv(file_path, dtype='unicode', chunksize=chunk_size, low_memory=False):\n",
        "        yield chunk\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(file_path):\n",
        "    X, y = [], []\n",
        "\n",
        "    for chunk in read_data_in_chunks(file_path):\n",
        "        y_chunk = chunk.iloc[:, -1]\n",
        "        X_chunk = chunk.iloc[:, :-1]\n",
        "\n",
        "        y.extend(y_chunk)\n",
        "        X.extend(X_chunk.values)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Preprocess entire data\n",
        "def preprocess_full_dataset(file_path, save_path=None):\n",
        "    X, y = preprocess_data(file_path)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    if save_path:\n",
        "        save_dataset(X_train, X_test, y_train, y_test, scaler, label_encoder, save_path)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler, label_encoder\n",
        "\n",
        "# Load preprocessed dataset if exists, otherwise preprocess and save it\n",
        "file_path = \"/mnt/data/KDDTrain+.txt\"\n",
        "save_path = \"/mnt/data/processed_data.npz\"\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    X_train, X_test, y_train, y_test, scaler, label_encoder = load_dataset(save_path)\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test, scaler, label_encoder = preprocess_full_dataset(file_path, save_path)\n"
      ],
      "metadata": {
        "id": "1PD4BQSNcjet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline NETWORK IDS MODEL"
      ],
      "metadata": {
        "id": "58QINeo9cwo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "def build_nn_model(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Train the baseline model\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "nn_model = build_nn_model(input_dim, num_classes)\n",
        "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, X, y, batch_size=256, shuffle=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.X))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X_batch = self.X[batch_indices]\n",
        "        y_batch = self.y[batch_indices]\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "train_generator = DataGenerator(X_train, y_train, batch_size=256)\n",
        "test_generator = DataGenerator(X_test, y_test, batch_size=256, shuffle=False)\n",
        "\n",
        "nn_model.fit(train_generator, epochs=50, callbacks=[early_stopping], validation_data=test_generator)\n",
        "\n",
        "# Evaluate the baseline model on the test data\n",
        "y_pred = np.argmax(nn_model.predict(X_test), axis=1)\n",
        "print(\"Baseline Neural Network Model\")\n",
        "print(classification_report(y_test, y_pred, labels=np.unique(y_train), target_names=label_encoder.classes_))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "xJT0JTi-dAas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# AUTOENCODER IDS MODEL"
      ],
      "metadata": {
        "id": "CSXuDofvdTLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the autoencoder model\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(128, activation='relu')(input_layer)\n",
        "    encoded = Dense(64, activation='relu')(encoded)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    bottleneck = Dense(16, activation='relu')(encoded)\n",
        "    decoded = Dense(32, activation='relu')(bottleneck)\n",
        "    decoded = Dense(64, activation='relu')(decoded)\n",
        "    decoded = Dense(128, activation='relu')(decoded)\n",
        "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return autoencoder\n",
        "\n",
        "autoencoder = build_autoencoder(input_dim)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train the autoencoder model\n",
        "autoencoder.fit(train_generator, epochs=50, callbacks=[early_stopping], validation_data=test_generator)\n",
        "\n",
        "# Use the autoencoder to transform the original data\n",
        "X_train_autoencoded = autoencoder.predict(X_train)\n",
        "X_test_autoencoded = autoencoder.predict(X_test)\n",
        "\n",
        "# Build a new IDS model using the autoencoded data\n",
        "nn_model_autoencoded = build_nn_model(input_dim, num_classes)\n",
        "nn_model_autoencoded.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "train_generator_autoencoded = DataGenerator(X_train_autoencoded, y_train, batch_size=256)\n",
        "test_generator_autoencoded = DataGenerator(X_test_autoencoded, y_test, batch_size=256, shuffle=False)\n",
        "nn_model_autoencoded.fit(train_generator_autoencoded, epochs=50, callbacks=[early_stopping], validation_data=test_generator_autoencoded)\n",
        "\n"
      ],
      "metadata": {
        "id": "v1NS0OOldSTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the autoencoded model on the autoencoded test data\n",
        "y_pred_autoencoded = np.argmax(nn_model_autoencoded.predict(X_test_autoencoded), axis=1)\n",
        "print(\"Neural Network Model with Autoencoder - Autoencoded Test Data\")\n",
        "print(classification_report(y_test, y_pred_autoencoded, labels=np.unique(y_train), target_names=label_encoder.classes_))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_autoencoded)}\")\n"
      ],
      "metadata": {
        "id": "yS6q7-FOdeOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL DQN AGENT"
      ],
      "metadata": {
        "id": "yf36N9xzdiXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "# Instantiate the DQNAgent\n",
        "state_size = input_dim  # This should match the input feature dimension of the IDS\n",
        "action_size = num_classes  # This should match the number of unique classes in the target\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Train the DQN agent\n",
        "EPISODES = 1000\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    state = np.reshape(X_train[0], [1, state_size])  # Initialize with the first sample\n",
        "    for time in range(500):  # Limit the number of steps in each episode\n",
        "        action = agent.act(state)\n",
        "        next_state = np.reshape(X_train[(time + 1) % len(X_train)], [1, state_size])\n",
        "        reward = 1 if np.argmax(y_train[time]) == action else -1  # Reward mechanism\n",
        "        done = time == 499\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(f\"episode: {e}/{EPISODES}, score: {time}, e: {agent.epsilon:.2}\")\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "\n",
        "# Save the trained agent\n",
        "agent.save(\"dqn_agent.h5\")\n",
        "\n",
        "# Test the DQN agent\n",
        "correct = 0\n",
        "total = len(X_test)\n",
        "\n",
        "for i in range(total):\n",
        "    state = np.reshape(X_test[i], [1, state_size])\n",
        "    action = agent.act(state)\n",
        "    if np.argmax(y_test[i]) == action:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"DQN Agent Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "izCDtgZNdhml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ADVSERIAL ATTACK"
      ],
      "metadata": {
        "id": "9Q6-G0cWd10Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FGSM Attack\n",
        "def fgsm_attack(model, X, y, epsilon=0.1):\n",
        "    X_adv = X.copy()\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(X_adv)\n",
        "        prediction = model(X_adv)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, prediction)\n",
        "    gradient = tape.gradient(loss, X_adv)\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    X_adv = X_adv + epsilon * signed_grad\n",
        "    X_adv = tf.clip_by_value(X_adv, -1, 1)\n",
        "    return X_adv.numpy()\n",
        "\n",
        "# PGD Attack\n",
        "def pgd_attack(model, X, y, epsilon=0.1, alpha=0.01, num_iterations=40):\n",
        "    X_adv = X.copy()\n",
        "    for i in range(num_iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(X_adv)\n",
        "            prediction = model(X_adv)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y, prediction)\n",
        "        gradient = tape.gradient(loss, X_adv)\n",
        "        signed_grad = tf.sign(gradient)\n",
        "        X_adv = X_adv + alpha * signed_grad\n",
        "        X_adv = tf.clip_by_value(X_adv, X - epsilon, X + epsilon)\n",
        "        X_adv = tf.clip_by_value(X_adv, -1, 1)\n",
        "    return X_adv.numpy()\n"
      ],
      "metadata": {
        "id": "m0fUVk6Hd2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For GAN examples"
      ],
      "metadata": {
        "id": "Nk-TmHEBeMBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the generator model for the GAN\n",
        "def build_generator(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, input_dim=input_dim, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(output_dim, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the discriminator model for the GAN\n",
        "def build_discriminator(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(512, input_dim=input_dim, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Compile GAN\n",
        "def compile_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential([\n",
        "        generator,\n",
        "        discriminator\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002))\n",
        "    return model\n",
        "\n",
        "# Train the GAN\n",
        "def train_gan(generator, discriminator, gan, data, epochs=10000, batch_size=64):\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
        "        real_data = data[idx]\n",
        "        noise = np.random.normal(0, 1, (batch_size, generator.input_shape[1]))\n",
        "        generated_data = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, generator.input_shape[1]))\n",
        "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f\"{epoch} [D loss: {d_loss_real[0]}] [G loss: {g_loss}]\")\n",
        "    return generator\n",
        "\n",
        "\n",
        "# Prepare data for GAN training\n",
        "generator = build_generator(input_dim, input_dim)\n",
        "discriminator = build_discriminator(input_dim)\n",
        "gan = compile_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN on normal data\n",
        "generator = train_gan(generator, discriminator, gan, X_train)\n",
        "\n",
        "# Generate adversarial examples using GAN\n",
        "noise = np.random.normal(0, 1, (len(X_test), input_dim))\n",
        "X_test_gan_adv = generator.predict(noise)\n"
      ],
      "metadata": {
        "id": "nUPbxsujeStr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on baseline model and Robust model"
      ],
      "metadata": {
        "id": "eVv3MDDJebHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to evaluate a model on adversarial examples\n",
        "def evaluate_model(model, X, y, attack_fn, **kwargs):\n",
        "    X_adv = attack_fn(model, X, y, **kwargs)\n",
        "    y_pred = np.argmax(model.predict(X_adv), axis=1)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    print(f\"Accuracy on adversarial examples: {accuracy:.2%}\")\n",
        "    print(classification_report(y, y_pred, target_names=label_encoder.classes_))\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the baseline model on FGSM adversarial examples\n",
        "print(\"Evaluating Baseline Model on FGSM Adversarial Examples\")\n",
        "evaluate_model(nn_model, X_test, y_test, fgsm_attack, epsilon=0.1)\n",
        "\n",
        "# Evaluate the baseline model on PGD adversarial examples\n",
        "print(\"Evaluating Baseline Model on PGD Adversarial Examples\")\n",
        "evaluate_model(nn_model, X_test, y_test, pgd_attack, epsilon=0.1, alpha=0.01, num_iterations=40)\n",
        "\n",
        "# Evaluate the robust adversarial model (RL DQN) on FGSM adversarial examples\n",
        "print(\"Evaluating RL DQN Model on FGSM Adversarial Examples\")\n",
        "evaluate_model(agent.model, X_test, y_test, fgsm_attack, epsilon=0.1)\n",
        "\n",
        "# Evaluate the robust adversarial model (RL DQN) on PGD adversarial examples\n",
        "print(\"Evaluating RL DQN Model on PGD Adversarial Examples\")\n",
        "evaluate_model(agent.model, X_test, y_test, pgd_attack, epsilon=0.1, alpha=0.01, num_iterations=40)\n",
        "\n",
        "# Evaluate the baseline model on GAN adversarial examples\n",
        "y_pred_gan_adv = np.argmax(nn_model.predict(X_test_gan_adv), axis=1)\n",
        "print(\"Baseline Model - GAN Adversarial Examples\")\n",
        "print(classification_report(y_test, y_pred_gan_adv, target_names=label_encoder.classes_))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gan_adv)}\")\n",
        "\n",
        "# Evaluate the robust adversarial model (RL DQN) on GAN adversarial examples\n",
        "y_pred_gan_adv_dqn = np.argmax(agent.model.predict(X_test_gan_adv), axis=1)\n",
        "print(\"RL DQN Model - GAN Adversarial Examples\")\n",
        "print(classification_report(y_test, y_pred_gan_adv_dqn, target_names=label_encoder.classes_))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gan_adv_dqn)}\")"
      ],
      "metadata": {
        "id": "OmG70v-Fe0Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvtTaIgGYiqJ"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ]
}