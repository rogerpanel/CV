{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyParvSSe/ClaaLzE5PVqsdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerpanel/CV/blob/main/Transformer_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdVLJ7iDSp_U",
        "outputId": "cacf88ea-66ac-41d7-8c93-1b2b406d5e95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikuAHgbBS3pU",
        "outputId": "d9012926-b7b5-48f2-aa17-7840c2061878"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get ViT-PyTorch\n",
        "!pip install --upgrade pytorch-pretrained-vit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwSZbFQsnntU",
        "outputId": "f1d127fa-35c9-41c4-9717-24602362cbf3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-vit in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-vit) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-pretrained-vit) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-pretrained-vit) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-pretrained-vit) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-pretrained-vit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-pretrained-vit) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_6yFs9ECd-H",
        "outputId": "bf2f7e55-97a7-455f-9c36-b2b24ac3170c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import Libraries\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from sklearn.manifold import TSNE\n",
        "# import libraries\n",
        "from torchvision.transforms import ToTensor\n",
        "from pytorch_pretrained_vit import ViT"
      ],
      "metadata": {
        "id": "_D93r2aYSiH3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants\n",
        "AUDIO_SAMPLE_RATE = 44100  # Sample rate of the original audio file\n",
        "FRAME_LENGTH = 2048\n",
        "HOP_LENGTH = 512\n",
        "\n",
        "# Function to extract hidden audio information from stego-images in embedded batches\n",
        "def extract_audio_from_stego_images(stego_images, patch_size, audio_length, batch_size):\n",
        "    num_images = len(stego_images)\n",
        "    extracted_audio_batches = []\n",
        "\n",
        "    # Create a new ViT model with the desired image size\n",
        "    model_name = 'google/vit-base-patch16-224'\n",
        "    model = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "    for i in range(0, num_images, batch_size):\n",
        "        batch_images = stego_images[i:i+batch_size]\n",
        "\n",
        "        # Resize stego images to match the expected input size of the ViT model\n",
        "        resized_stego_images = []\n",
        "        for image in batch_images:\n",
        "            resized_image = cv2.resize(image, (224, 224))\n",
        "            resized_stego_images.append(resized_image)\n",
        "\n",
        "        # Convert stego images to tensor\n",
        "        tensor_stego_images = torch.stack([ToTensor()(image) for image in resized_stego_images])\n",
        "\n",
        "        # Pass stego images through the ViT model to obtain audio embeddings\n",
        "        audio_embeddings = model.forward(tensor_stego_images).last_hidden_state.squeeze().detach().numpy()\n",
        "\n",
        "        # Reshape audio embeddings to match the patch arrangement\n",
        "        audio_embeddings = audio_embeddings.reshape(audio_embeddings.shape[0], -1)\n",
        "\n",
        "        # Extract the hidden audio information from the audio embeddings\n",
        "        extracted_audio_batch = []\n",
        "        for j in range(audio_embeddings.shape[0]):\n",
        "            audio_embedding = audio_embeddings[j]\n",
        "            patch_audio = audio_embedding[:audio_length]\n",
        "            extracted_audio_batch.append(patch_audio)\n",
        "\n",
        "        # Concatenate the extracted audio patches for the batch\n",
        "        extracted_audio_batch = np.concatenate(extracted_audio_batch)\n",
        "        extracted_audio_batches.append(extracted_audio_batch)\n",
        "\n",
        "    # Concatenate the extracted audio patches for all batches\n",
        "    extracted_audio = np.concatenate(extracted_audio_batches)\n",
        "\n",
        "    return extracted_audio\n",
        "\n",
        "# Function to resize or truncate the reconstructed audio to match the length of the original audio\n",
        "def reconstruct_audio(extracted_audio, original_audio_length):\n",
        "    reconstructed_audio = extracted_audio[:original_audio_length]\n",
        "    return reconstructed_audio\n",
        "\n",
        "# Example usage\n",
        "# Load the stego images\n",
        "stego_images = np.load('/content/image_npy_1.npy')\n",
        "\n",
        "# Convert stego images to RGB color space\n",
        "rgb_stego_images = []\n",
        "for image in stego_images:\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    rgb_stego_images.append(rgb_image)\n",
        "\n",
        "# Convert the RGB stego images back to a NumPy array\n",
        "rgb_stego_images = np.array(rgb_stego_images)\n",
        "\n",
        "# Assuming the patch size is equal to the height of the stego image\n",
        "patch_size = rgb_stego_images.shape[1]\n",
        "\n",
        "# Assuming the audio length is based on the number of frames in the stego image\n",
        "audio_length = rgb_stego_images.shape[0] * FRAME_LENGTH\n",
        "\n",
        "# Set the batch size for processing stego images\n",
        "batch_size = 10\n",
        "\n",
        "# Extract hidden audio information from stego-images\n",
        "extracted_audio = extract_audio_from_stego_images(rgb_stego_images, patch_size, audio_length, batch_size)\n",
        "\n",
        "# Load the original audio file\n",
        "original_audio, _ = librosa.load('/content/1. male1a.wav', sr=AUDIO_SAMPLE_RATE)\n",
        "\n",
        "# Resize or truncate the reconstructed audio to match the length of the original audio\n",
        "reconstructed_audio = reconstruct_audio(extracted_audio, len(original_audio))\n",
        "\n",
        "# Assuming you have defined the evaluate_decoding function for accuracy evaluation\n",
        "def evaluate_decoding(original_audio, reconstructed_audio):\n",
        "    mse = np.mean((original_audio - reconstructed_audio) ** 2)\n",
        "    psnr = librosa.core.power_to_db(np.max(original_audio) / mse)\n",
        "    ssim = ...\n",
        "    return mse, psnr, ssim\n",
        "\n",
        "# Evaluate the accuracy and reliability of the decoding process\n",
        "mse, psnr, ssim = evaluate_decoding(original_audio, reconstructed_audio)\n",
        "\n",
        "# Save the reconstructed audio\n",
        "output_audio_file = '/content/output_audio.wav'\n",
        "sf.write(output_audio_file, reconstructed_audio, AUDIO_SAMPLE_RATE, 'PCM_24')\n",
        "print(\"Reconstructed audio saved as\", output_audio_file)\n",
        "\n",
        "from google.colab import files\n",
        "files.download (output_audio_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "DYUpvmh-As4u",
        "outputId": "2cc452df-a95f-4939-8589-2fb9541bfcc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed audio saved as /content/output_audio.wav\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be97dc57-b97c-48a0-8d39-79e0a94d0d38\", \"output_audio.wav\", 250496)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}