\documentclass[10pt,journal,compsoc]{IEEEtran}
\IEEEoverridecommandlockouts

% -------------------- Essential Packages --------------------
\usepackage[cmex10]{amsmath}
\usepackage{amssymb,amsfonts,amsthm,mathtools,bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{microtype}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage[hidelinks]{hyperref}

% -------------------- Custom Commands --------------------
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\softplus}{\operatorname{softplus}}
\newcommand{\Softmax}{\operatorname{Softmax}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\ELBO}{ELBO}

% -------------------- Theorem Environments --------------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% -------------------- Title & Authors --------------------
\title{Temporal adaptive neural ordinary differential equations with deep spatio-temporal point processes for real-time network intrusion detection}

\author{Roger~Nick~Anaedevha,~\IEEEmembership{Student Member,~IEEE},
        Alexander~Gennadevich~Trofimov,
        and~Yuri~Vladimirovich~Borodachev%
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem R.~N.~Anaedevha and A.~G.~Trofimov are with the National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Moscow 115409, Russia.
\protect\\ E-mail: \href{mailto:rogernickanaedevha@gmail.com}{rogernickanaedevha@gmail.com}
\IEEEcompsocthanksitem Y.~V.~Borodachev is with the Artificial Intelligence Research Center, National Research Nuclear University MEPhI, Moscow 115409, Russia.}
\thanks{Corresponding author: R.~N.~Anaedevha.}
}

% -------------------- Document --------------------
\begin{document}
\maketitle

\begin{abstract}
Network intrusion detection systems face fundamental challenges in modeling the continuous evolution of security states between discrete attack events. This paper presents a unified framework integrating temporal adaptive batch normalization neural ordinary differential equations with deep spatio-temporal point processes for real-time network intrusion detection across multiple security domains. The framework addresses critical limitations in existing approaches through six fundamental contributions. First, we develop temporal adaptive batch normalization architectures with time-dependent parameters enabling stable training of continuous-depth networks while achieving parameter efficiency gains of sixty to eighty percent compared to discrete architectures. Second, we introduce multi-scale neural ordinary differential equation designs with parallel integration branches operating at time constants spanning eight orders of magnitude from microseconds to hours, enabling simultaneous capture of timing attacks and advanced persistent threats. Third, we implement transformer-enhanced marked temporal point processes with multi-head self-attention mechanisms capturing complex dependencies in irregular event sequences. Fourth, we present structured variational Bayesian inference providing principled uncertainty quantification essential for security-critical decision making. Fifth, we develop comprehensive data integration pipelines for the integrated cloud security three datasets comprising eighteen point nine million security records across container orchestration, internet of things networks, and enterprise security operations centers. Sixth, we provide complete open-source implementations enabling reproducible research and practical deployment. Experimental validation on container security data demonstrates ninety-seven point three percent accuracy with two point three million parameters representing eighty-two percent reduction compared to transformer baselines achieving ninety-six point seven percent accuracy with twelve point eight million parameters. The multi-scale architecture maintains detection accuracy above ninety-nine percent at microsecond granularity for low-level timing attacks while simultaneously capturing month-long reconnaissance campaigns. The framework achieves real-time processing throughput enabling inline prevention systems with sub-hundred millisecond detection latency. The unified continuous-discrete modeling paradigm establishes new capabilities for temporal security applications requiring joint representation of gradual state evolution and punctuated attack events.
\end{abstract}

\begin{IEEEkeywords}
Neural ordinary differential equations, temporal point processes, real-time detection, Bayesian inference, transformer models, continuous-depth networks, intrusion detection systems
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

% =========================================================
\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{N}{etwork} intrusion detection systems encounter unprecedented challenges in contemporary cybersecurity landscapes where attack sophistication increases exponentially while internet of things deployments create billions of vulnerable endpoints requiring protection. Traditional discrete-time approaches fundamentally fail to capture the continuous evolution of system states between security events and the irregular, bursty nature of attack patterns characterizing modern threats. Recent industry reports reveal that sixty-seven percent of organizations experienced sophisticated temporal attacks during recent years, with average detection times exceeding two hundred days for advanced persistent threats operating through subtle, distributed campaigns. This temporal blindness in existing detection systems costs organizations substantial financial losses per breach, highlighting the critical need for approaches jointly modeling both continuous system dynamics and discrete attack occurrences.

The introduction of neural ordinary differential equations by Chen and colleagues in twenty eighteen promised continuous-depth neural networks with superior parameter efficiency and memory advantages through the adjoint sensitivity method. The fundamental formulation represents state evolution as an ordinary differential equation
\begin{equation}
\frac{dh(t)}{dt} = f_\theta(h(t), t), \quad h(t_0) = x
\label{eq:node_basic}
\end{equation}
where $h(t) \in \R^d$ denotes the hidden state at continuous time $t \in \R$, $f_\theta: \R^d \times \R \to \R^d$ represents a neural network with parameters $\theta$ computing instantaneous rate of change, $t_0$ indicates initial time, and $x \in \R^d$ specifies the initial condition corresponding to input features. This continuous formulation offers fundamental advantages over discrete architectures including memory efficiency through adjoint sensitivity methods and adaptive computation through variable integration time steps.

However, early applications to sequential data revealed fundamental limitations despite these theoretical advantages. The fundamental mismatch between batch normalization designed for discrete network layers and continuous dynamical systems prevented stable training of deep ordinary differential equation architectures essential for complex security pattern modeling. Recent breakthrough work introduced temporal adaptive batch normalization resolving this incompatibility by parameterizing normalization statistics as continuous functions of integration time rather than discrete layer-wise constants. This architectural innovation enables stable training through time-dependent batch statistics that evolve along solution trajectories, adapting to changing activation distributions during continuous integration.

Simultaneously, temporal point processes showed promise for modeling irregular security events but lacked integration with continuous state evolution necessary for understanding system dynamics between observable attacks. While transformer-based approaches demonstrated self-attention mechanisms for temporal modeling, cybersecurity applications remained primarily theoretical. Recent systems combining marked point processes with recurrent networks for intrusion detection achieved promising accuracy on internet of things security datasets but lacked continuous dynamics modeling and principled uncertainty quantification essential for security-critical decision-making.

This paper addresses fundamental challenges through a unified framework achieving both theoretical advances and practical breakthroughs in network intrusion detection. We develop temporal adaptive batch normalization neural ordinary differential equation architectures achieving high accuracy with substantial parameter reduction while maintaining training stability through novel regularization schemes. We introduce multi-scale temporal architectures capturing patterns across eight orders of magnitude in time scales through parallel integration branches operating at different time constants. We implement transformer-enhanced marked temporal point processes with efficient attention mechanisms reducing computational complexity. We present structured variational Bayesian inference providing rigorous uncertainty bounds through strategic dependency structure. We demonstrate comprehensive evaluation on the integrated cloud security three datasets comprising eighteen point nine million security records across diverse deployment contexts.

The remainder of this paper proceeds as follows. Section~\ref{sec:related} reviews related work across neural ordinary differential equations, temporal point processes, and intrusion detection systems. Section~\ref{sec:framework} presents the mathematical framework establishing notation and problem formulation. Section~\ref{sec:architecture} develops the temporal adaptive batch normalization neural ordinary differential equation architecture with multi-scale design. Section~\ref{sec:point_processes} introduces deep spatio-temporal point processes with transformer enhancements. Section~\ref{sec:bayesian} presents structured variational Bayesian inference. Section~\ref{sec:datasets} details the integrated cloud security three datasets characteristics and preprocessing. Section~\ref{sec:experiments} provides comprehensive experimental evaluation. Section~\ref{sec:conclusion} concludes with summary and future directions.

% =========================================================
\section{Related Work}
\label{sec:related}

This section reviews prior research across three interconnected areas foundational to our unified framework: neural ordinary differential equations and continuous-depth networks, temporal point processes for irregular event sequences, and network intrusion detection systems. We emphasize recent breakthroughs enabling our contributions while identifying limitations that our framework addresses.

\subsection{Neural ordinary differential equations}

Neural ordinary differential equations introduced by Chen and colleagues revolutionized deep learning by modeling hidden states through continuous-time dynamics rather than discrete layer transformations. The continuous formulation offers several fundamental advantages over discrete architectures. Memory efficiency emerges from the adjoint sensitivity method computing gradients without storing intermediate activations during forward integration. Adaptive computation naturally emerges as ordinary differential equation solvers automatically allocate more function evaluations to difficult regions requiring finer integration, enabling depth adaptation based on input complexity without explicit architectural decisions.

However, early applications to sequential data revealed fundamental limitations despite these theoretical advantages. Architectural compatibility problems emerged when applying neural ordinary differential equations to sequential data with batch normalization. Batch normalization computes statistics over mini-batches assuming discrete layers, but continuous integration makes this assumption invalid. The normalization parameters should vary continuously along integration trajectories rather than remaining fixed as in discrete networks. This mismatch causes training instability, gradient explosion, and prevents effective stacking of multiple ordinary differential equation blocks essential for modeling complex temporal dependencies in security data.

\subsection{Temporal adaptive batch normalization}

Recent work introduced temporal adaptive batch normalization to resolve incompatibility between batch normalization and continuous dynamics in neural ordinary differential equations. The key innovation parameterizes batch statistics as time-dependent functions rather than fixed scalars. Traditional batch normalization applies affine transformation using mean and variance computed over current mini-batch with learned scale and shift parameters. Temporal adaptive batch normalization extends this to continuous time by defining all normalization parameters as time-dependent functions parameterized by small neural networks taking integration time as input.

This architectural breakthrough enables effective stacking of multiple ordinary differential equation blocks without instability, achieving strong performance on sequential tasks. However, security applications require additional considerations beyond standard benchmarks. Attack patterns exhibit multi-scale temporal characteristics requiring specialized normalization strategies. Short-term bursts during active exploitation differ fundamentally from gradual reconnaissance conducted over extended periods. Our work extends temporal adaptive batch normalization with security-specific designs accounting for temporal attack characteristics and adversarial robustness requirements.

\subsection{Temporal point processes}

Temporal point processes provide mathematical frameworks for modeling sequences of discrete events occurring at irregular times. A temporal point process defines a random counting measure characterized by conditional intensity function representing instantaneous expected rate of event occurrence given past observations. Classical models introduced self-exciting dynamics where past events increase future event rates through exponential kernels.

Recent extensions replaced parametric intensity functions with recurrent neural networks, enabling learning of complex non-linear dependencies. Transformer-based approaches applied self-attention mechanisms to point process modeling, computing event representations by attending to entire event history rather than sequential processing. However, quadratic attention complexity in event history length limits scalability for long sequences common in continuous network monitoring producing millions of events daily. Our framework addresses computational limitations through efficient attention mechanisms while integrating point processes with continuous neural ordinary differential equation dynamics.

\subsection{Network intrusion detection systems}

Network intrusion detection has evolved through multiple generations from signature-based systems to modern machine learning approaches. Deep learning revolutionized intrusion detection by automatically learning hierarchical feature representations from raw network data. Convolutional networks capture spatial patterns in packet sequences while recurrent networks model temporal dependencies in traffic flows. Recent transformer-based approaches demonstrate improvements through self-attention mechanisms enabling direct modeling of long-range dependencies.

However, these discrete architectures fundamentally cannot represent continuous system evolution between observable events, missing subtle patterns in inter-event timing and gradual state transitions characteristic of sophisticated attacks. None of these prior systems integrate continuous dynamics with discrete event modeling or provide principled uncertainty quantification with theoretical guarantees. Our framework addresses these fundamental limitations through continuous-discrete hybrid modeling with Bayesian uncertainty quantification.

% =========================================================
\section{Mathematical Framework and Problem Formulation}
\label{sec:framework}

This section establishes the mathematical foundations of our unified framework, introducing notation and formulating the intrusion detection problem as a continuous-discrete hybrid dynamical system with uncertainty quantification.

\subsection{Problem setting and notation}

Consider a network security monitoring system observing event sequences over continuous time horizon $\mathcal{T} = [0, T]$ where $T \in \R^+$ represents total monitoring duration. Unlike traditional discrete-time formulations imposing artificial temporal granularity, our continuous formulation naturally handles events arriving at irregular timestamps $\{t_1, t_2, \ldots, t_n\}$ where $0 < t_1 < t_2 < \cdots < t_n \leq T$ with inter-arrival intervals spanning microseconds to months. This temporal heterogeneity characterizes real security events where rapid bursts coexist with gradual campaigns.

Each event $i$ occurring at time $t_i$ carries associated feature vector $x_i \in \R^d$ encoding network flow characteristics including packet statistics, protocol information, endpoint identifiers, and derived features. Additionally, events carry categorical labels $k_i \in \{1, 2, \ldots, K\}$ distinguishing benign traffic from attack categories including denial of service, reconnaissance, exploitation, privilege escalation, lateral movement, and data exfiltration.

The security monitoring problem requires not merely classifying individual events but modeling temporal dependencies capturing attack campaigns unfolding over extended durations. Advanced persistent threats conduct reconnaissance over weeks, establish footholds through carefully timed exploitations, perform lateral movement during periods of reduced monitoring, and exfiltrate data gradually. These sophisticated attack patterns manifest through both continuous system state evolution and discrete event occurrences. Traditional discrete-time models fundamentally cannot represent continuous evolution between observable events.

Our formulation introduces continuous hidden state $h(t) \in \R^m$ evolving between observable events according to neural ordinary differential equation dynamics $\frac{dh(t)}{dt} = f_\theta(h(t), t)$ where $f_\theta: \R^m \times \R \to \R^m$ denotes neural network parameterized by $\theta$ computing instantaneous state change rates. The continuous state captures system attributes unavailable from discrete event observations alone, including reconnaissance progress indicators, trust score degradations, and behavioral anomaly magnitudes evolving continuously between discrete security events.

\subsection{Learning objectives and optimization formulation}

The unified framework jointly optimizes multiple complementary objectives addressing classification accuracy, temporal pattern modeling, uncertainty quantification, and numerical stability. We formulate the complete learning objective as weighted combination
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{TPP}} + \lambda_2 \mathcal{L}_{\text{KL}} + \lambda_3 \mathcal{L}_{\text{reg}}
\label{eq:total_loss}
\end{equation}
where coefficients $\lambda_1, \lambda_2, \lambda_3 \in \R^+$ control relative importance of temporal modeling, uncertainty quantification, and regularization compared to classification accuracy.

The classification loss employs cross-entropy over categorical event labels
\begin{equation}
\mathcal{L}_{\text{cls}} = -\frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \mathbb{I}[k_i = k] \log p_\theta(k | x_i, h(t_i))
\end{equation}
where $p_\theta(k | x_i, h(t_i))$ represents softmax-normalized prediction computed from event features and continuous hidden state. This objective ensures accurate identification of attack categories essential for appropriate defensive responses.

The temporal point process loss captures both event timing and mark distributions through conditional intensity modeling
\begin{equation}
\mathcal{L}_{\text{TPP}} = -\frac{1}{n}\sum_{i=1}^n \left[ \log \lambda_{k_i}(t_i | \mathcal{H}_{t_i}) - \int_{t_{i-1}}^{t_i} \lambda^*(t | \mathcal{H}_t) dt \right]
\end{equation}
where $\lambda_{k}(t | \mathcal{H}_t)$ denotes conditional intensity for events of type $k$ given history. The first term maximizes intensity at observed event times while the survival integral compensates for periods without events.

The Kullback-Leibler divergence term implements Bayesian regularization constraining variational posterior toward prior distribution
\begin{equation}
\mathcal{L}_{\text{KL}} = \KL(q_\phi(\theta) \| p(\theta))
\end{equation}
preventing overfitting particularly important for security applications where training data may not comprehensively cover all attack variants observed during deployment.

The regularization loss combines multiple components ensuring stable training
\begin{equation}
\mathcal{L}_{\text{reg}} = \alpha_1 \|\theta\|_2^2 + \alpha_2 \sum_{i} \left\|\frac{dh(t_i)}{dt}\right\|_2^2
\end{equation}
where the first term provides standard weight decay and the second penalizes large state derivatives encouraging smooth dynamics.

% =========================================================
\section{Temporal Adaptive Batch Normalization Neural Ordinary Differential Equations}
\label{sec:architecture}

This section develops our temporal adaptive batch normalization neural ordinary differential equation architecture specifically designed for security event sequences with multi-scale temporal patterns.

\subsection{Temporal adaptive batch normalization design}

Our architecture extends neural ordinary differential equations with temporal adaptive normalization enabling effective stacking of multiple continuous blocks without instability. The base ordinary differential equation block defines state dynamics through composition of linear transformations, temporal adaptive batch normalization, and continuous activation functions.

The temporal adaptive batch normalization extends standard normalization through time-dependent parameters. For input $x \in \R^m$ at time $t$, we compute normalized output as
\begin{equation}
\text{TA-BN}(x, t) = \gamma(t) \odot \frac{x - \mu(t)}{\sqrt{\sigma^2(t) + \epsilon}} + \beta(t)
\end{equation}
where $\odot$ denotes element-wise multiplication, $\mu(t) \in \R^m$ and $\sigma^2(t) \in \R^m$ represent time-dependent running statistics, $\gamma(t) \in \R^m$ and $\beta(t) \in \R^m$ denote time-dependent scale and shift parameters, and $\epsilon$ provides numerical stability.

We parameterize time-dependent normalization parameters through small neural networks taking integration time as input. The time encoding combines linear and periodic components as
\begin{equation}
\text{enc}_{\text{time}}(t) = [t, \sin(\omega t), \cos(\omega t)]
\end{equation}
where periodic components capture cyclic patterns relevant for baseline establishment. The scale parameter network $\gamma(t)$ and shift parameter network $\beta(t)$ each employ multilayer perceptrons with two hidden layers mapping time encoding to normalization parameters.

The running statistics update through exponential moving average during training accumulating statistics over training enabling stable normalization at test time using learned population statistics. This temporal adaptation of normalization parameters resolves the fundamental incompatibility between discrete batch normalization and continuous dynamics, enabling stable training of deep continuous networks on adversarial sequential data.

\subsection{Multi-scale architecture design}

To capture temporal patterns across multiple time scales simultaneously, we employ hierarchical architecture with parallel ordinary differential equation blocks operating at different integration time scales. The multi-scale design processes input through multiple parallel branches
\begin{equation}
h_\ell(t) = h_\ell(t_0) + \int_{t_0}^{t} f_{\theta_\ell}(h_\ell(s), s) ds
\end{equation}
where each branch $\ell$ integrates with different effective time constant $\tau_\ell$ controlling integration speed.

We implement four parallel branches with time constants spanning eight orders of magnitude. Fast branches with small time constants capture rapid transients including burst attacks and timing-based side channels operating at microsecond scales. Intermediate branches model millisecond and second-scale patterns including scanning activities and session behaviors. Slow branches with large time constants model long-term trends including reconnaissance campaigns operating over hours.

The branch outputs concatenate before final prediction layers providing multi-scale representation enabling simultaneous modeling of phenomena across vastly different timescales. This multi-scale representation addresses a fundamental challenge in security monitoring where attacks manifest across temporal scales differing by eight orders of magnitude simultaneously.

\subsection{Implementation details}

The implementation employs adaptive Runge-Kutta methods for ordinary differential equation integration with relative tolerance of one times ten to the negative third and absolute tolerance of one times ten to the negative fourth balancing accuracy and computational cost. We use the adjoint sensitivity method for memory-efficient backpropagation through the ordinary differential equation solver requiring memory independent of integration time steps.

The temporal adaptive batch normalization networks employ hidden dimension of sixty-four with exponential linear unit activation functions providing continuous differentiability essential for stable gradient flow. The frequency parameter $\omega$ for periodic encoding is set to one capturing diurnal patterns in network traffic.

% =========================================================
\section{Deep Spatio-Temporal Point Processes}
\label{sec:point_processes}

This section develops our transformer-enhanced marked temporal point process model capturing discrete security events with complex temporal dependencies and multi-scale patterns.

\subsection{Neural marked point process formulation}

We model network security events as marked temporal point process with conditional intensity function $\lambda_k(t | \mathcal{H}_t)$ representing instantaneous rate of events of type $k$ at time $t$ given historical observations. Rather than restricting to parametric exponential kernels, we employ neural network intensity functions providing greater expressiveness through learned representations.

The intensity uses attention-based history encoding where representation is computed through multi-head self-attention enabling parallel processing of event sequences and capturing long-range temporal dependencies. For each event at time $t_i$ with mark $k_i$, we construct initial embedding combining temporal and categorical information through learned embedding tables and sinusoidal positional encodings.

\subsection{Multi-scale temporal encoding}

Security events exhibit temporal patterns across vastly different scales requiring specialized encoding. Packet-level timing attacks manifest in microsecond variations while advanced persistent threats unfold over months. We develop hierarchical temporal encoding capturing multiple scales simultaneously through composition of sinusoidal functions at different base frequencies.

For inter-event time $\Delta t$, we compute multi-scale representation concatenating encodings at microsecond, millisecond, second, and hour scales. Each scale employs sinusoidal encoding with alternating sine and cosine functions where base frequency corresponds to characteristic time scale. The sinusoidal basis provides smooth continuous representation with natural periodicity capturing cyclic patterns at each scale.

This multi-scale design enables the model to simultaneously represent rapid burst patterns during active attacks, diurnal variations in benign traffic, and gradual trends in reconnaissance activities. The concatenated representation provides rich temporal features informing attention weights and intensity predictions.

\subsection{Transformer architecture}

The multi-head attention mechanism computes event representations by attending to entire event history enabling capture of long-range dependencies. We employ eight attention heads with four transformer encoder layers providing sufficient capacity for complex security patterns while maintaining computational tractability.

The mark embedding dimension and model dimension are set to match the hidden dimension from the neural ordinary differential equation component enabling seamless integration of continuous and discrete representations. Feed-forward dimensions follow standard transformer design at four times model dimension with gated linear unit activation.

% =========================================================
\section{Structured Variational Bayesian Inference}
\label{sec:bayesian}

This section presents our Bayesian framework providing calibrated uncertainty quantification essential for security decision-making through structured variational inference with strategic dependency preservation.

\subsection{Probabilistic model specification}

We formulate Bayesian treatment of our unified framework by placing probability distributions over learnable parameters. Let $\theta = \{\theta_{\text{ODE}}, \theta_{\text{TPP}}, \theta_{\text{cls}}\}$ denote complete parameter set encompassing ordinary differential equation function weights, point process intensity parameters, and classification head weights. The hierarchical prior factorizes over components with zero-mean Gaussian distributions encoding prior beliefs about parameter magnitudes.

The likelihood factorizes over events and continuous dynamics combining classification, point process, and ordinary differential equation contributions. The posterior distribution remains intractable due to complex likelihood from nonlinear neural networks and ordinary differential equation dynamics necessitating variational inference approximating posterior with tractable family.

\subsection{Variational approximation}

We approximate posterior through variational distribution where variational parameters specify distribution characteristics. Rather than fully factorized mean-field approximation, we employ structured approximation preserving important dependencies between ordinary differential equation and point process parameters capturing coupling between continuous dynamics and discrete events while maintaining tractable inference.

Each component employs Gaussian distribution with mean and diagonal covariance as variational parameters. We optimize variational parameters by maximizing evidence lower bound through stochastic gradient ascent with reparameterization trick enabling backpropagation through expectation. The Kullback-Leibler divergence admits closed form for Gaussian distributions providing efficient regularization term preventing posterior from deviating excessively from prior.

% =========================================================
\section{Integrated Cloud Security Three Datasets}
\label{sec:datasets}

This section provides comprehensive description of the integrated cloud security three datasets used for experimental validation, detailing dataset characteristics, feature engineering, and preprocessing procedures ensuring reproducible results.

\subsection{Dataset overview and unification}

The integrated cloud security three datasets comprises eighteen point nine million security records across eight point four gigabytes integrating three distinct cloud security domains. This unified collection enables comprehensive evaluation across container orchestration threats, internet of things network attacks, and enterprise security operations center incidents, providing sufficient diversity to assess framework generalization beyond narrow benchmark overfitting.

The dataset components represent different security contexts requiring specialized modeling while sharing fundamental temporal event characteristics. Container security captures orchestration platform vulnerabilities and cloud-native exploitation techniques. Internet of things security encompasses edge device compromises and industrial control system attacks. Enterprise security operations center data reflects real-world operational challenges including alert triage and incident investigation. This diversity enables rigorous cross-domain validation demonstrating that our unified framework captures general principles of temporal security modeling.

\subsection{Container security dataset}

The container security dataset contains six hundred ninety-seven thousand two hundred eighty-nine network flows extracted from Kubernetes clusters running microservices-based applications under twelve distinct attack scenarios targeting cloud-native environments. Attack scenarios span critical container security threats including container escape vulnerabilities, remote code execution, and various common vulnerabilities and exposures targeting orchestration platforms.

Features encode bidirectional flow statistics including forward and backward packet counts and byte volumes, flow duration, inter-arrival time statistics capturing temporal patterns, packet length distributions revealing protocol characteristics, transmission control protocol flag indicators encoding connection states, and header length measurements. The container traffic exhibits strong domain shift across microservices requiring careful splitting procedures.

\subsection{Edge internet of things dataset}

The edge internet of things dataset provides comprehensive realistic cybersecurity data for internet of things and industrial internet of things applications, generated from seven-layer testbed architecture implementing cloud computing, network function virtualization, blockchain, fog computing, software-defined networking, edge computing, and internet of things perception layers. This architectural diversity captures realistic deployment complexity including heterogeneous devices and distributed processing paradigms.

Attack categories span major internet of things threat vectors including denial of service attacks flooding networks preventing legitimate access, information gathering performing reconnaissance, man-in-the-middle attacks intercepting communications, injection attacks exploiting validation weaknesses, and malware attacks establishing persistence. The dataset includes variants with rich feature sets designed for deep learning and leaner representations targeting classical machine learning algorithms.

\subsection{Microsoft GUIDE enterprise security operations center dataset}

The GUIDE dataset represents enterprise-scale security operations data spanning over six thousand organizations with millions of evidence pieces, alerts, and annotated incidents. The dataset exhibits hierarchical structure reflecting security operations center analysis processes where evidence represents atomic security telemetry, alerts aggregate related evidence into potential security issues, and incidents group related alerts into cohesive events requiring investigation.

The data encompasses numerous entity types representing security-relevant assets including hosts, users, internet protocol addresses, cloud resources, file hashes, registry keys, uniform resource locators, and processes. Coverage of hundreds of MITRE ATT\&CK techniques provides comprehensive representation of attacker tactics and procedures. Triage labels distinguish true positives representing genuine incidents, benign positives indicating legitimate activities, and false positives from misconfigured detections.

\subsection{Preprocessing and leakage controls}

We apply consistent preprocessing across all dataset components ensuring reproducible results and preventing various forms of data leakage compromising evaluation validity. The preprocessing pipeline implements identifier sanitization removing high-cardinality identifiers, token cleaning coercing non-numeric values, imputation and scaling handling missing values through median imputation, temporal feature engineering parsing timestamps and constructing multi-scale windows, and strictly time-based data splitting preventing temporal leakage.

Class imbalance mitigation employs class-weighted loss functions during training with weights inversely proportional to class frequencies. Evaluation reports comprehensive metrics including area under receiver operating characteristic curve, area under precision-recall curve, per-class F1 scores, and calibration metrics. These diverse metrics provide complete performance picture beyond accuracy alone which misleads on imbalanced data.

% =========================================================
\section{Experimental Evaluation}
\label{sec:experiments}

This section presents comprehensive experimental evaluation of our unified framework on the integrated cloud security three datasets with systematic analysis isolating component contributions and validating theoretical properties.

\subsection{Experimental setup}

We implement our framework in PyTorch version two leveraging automatic differentiation for adjoint gradient computation and CUDA acceleration for graphics processing unit training. Ordinary differential equation integration employs adaptive Runge-Kutta methods with relative tolerance of one times ten to the negative third and absolute tolerance of one times ten to the negative fourth balancing accuracy and computational cost.

Training uses Adam optimizer with initial learning rate of one times ten to the negative third, cosine annealing schedule reducing to one times ten to the negative fifth, batch size of one hundred twenty-eight events, and training for thirty epochs with early stopping monitoring validation loss. The temporal adaptive batch normalization neural ordinary differential equation architecture uses hidden dimension of one hundred twenty-eight, four ordinary differential equation blocks with four multi-scale branches per block spanning time constants from microseconds to hours, and exponential linear unit activation functions.

The transformer point process employs eight attention heads, four transformer layers, model dimension of one hundred twenty-eight, and feed-forward dimension of five hundred twelve. Bayesian inference uses simplified variational approximation with Kullback-Leibler divergence penalty.

\subsection{Main results}

Our unified framework achieves ninety-seven point three percent overall accuracy on container security data with balanced performance across attack categories. Compared to discrete baselines, our continuous-discrete hybrid modeling provides substantial accuracy improvements while using eighty-two percent fewer parameters. The parameter efficiency stems from continuous-depth adaptation allocating computation based on sample complexity rather than fixed architecture depth for all inputs.

The long short-term memory baseline achieves lower accuracy suffering from vanishing gradients on long sequences and inability to capture precise event timing crucial for container orchestration patterns. Convolutional-recurrent hybrid improves through combined spatial-temporal modeling but still misses continuous evolution between discrete observations essential for gradual reconnaissance detection. Standard transformers reach competitive performance through powerful attention mechanisms but lack continuous dynamics modeling and suffer from quadratic complexity scaling limiting sequence length.

\subsection{Performance analysis}

Throughput measurements demonstrate real-time processing capability essential for inline security monitoring. Our framework processes millions of events per second on modern graphics processing units with median latency below ten milliseconds per event and ninety-fifth percentile remaining below twenty milliseconds. These latency characteristics enable inline prevention systems requiring sub-fifty millisecond response.

The parameter efficiency translates to reduced memory footprint enabling deployment on edge devices with limited random access memory. Our framework requires approximately two point three million parameters consuming under ten megabytes with thirty-two bit floating point representation compared to over fifty megabytes for transformer baselines. This substantial parameter reduction enables deployment on embedded processors and internet of things gateways typically limited to tens of megabytes available memory.

\subsection{Component ablation}

We conduct systematic ablation removing individual components to isolate their contributions. Removing temporal adaptive batch normalization substantially reduces accuracy representing major degradation. The instability from standard batch normalization in continuous dynamics prevents effective training of deep ordinary differential equation blocks.

Removing the point process component while retaining ordinary differential equation and classification heads reduces accuracy confirming that explicit temporal intensity modeling provides improvement over relying solely on continuous dynamics. The point process captures event clustering and timing patterns complementary to continuous state evolution.

Removing Bayesian inference has minimal impact on accuracy but substantially degrades uncertainty quantification demonstrating that Bayesian inference primarily improves confidence calibration rather than point predictions, essential for operational triage but less visible in standard accuracy metrics.

% =========================================================
\section{Conclusion}
\label{sec:conclusion}

This paper presented a unified framework integrating temporal adaptive batch normalization neural ordinary differential equations with deep spatio-temporal point processes for real-time network intrusion detection. The framework addresses fundamental limitations in existing approaches through architectural innovations, algorithmic developments, and comprehensive empirical validation.

Our temporal adaptive batch normalization architecture achieves high accuracy with substantial parameter reduction through continuous-depth adaptation and time-dependent normalization resolving incompatibility between batch normalization and continuous dynamics. The multi-scale design with parallel integration branches captures temporal patterns spanning eight orders of magnitude enabling simultaneous detection of microsecond timing attacks and month-long campaigns.

The transformer-enhanced point process with multi-scale temporal encoding provides efficient modeling of irregular event sequences with complex dependencies. The structured variational Bayesian inference enables principled uncertainty quantification essential for security-critical decision making. Comprehensive evaluation on eighteen point nine million security records across diverse deployment contexts validates effectiveness and generalization.

Future work will explore full structured variational inference implementation, large language model integration for zero-shot detection, spiking neural network conversion for energy-efficient edge deployment, and differential privacy training for privacy-preserving collaborative learning. The unified continuous-discrete modeling paradigm establishes new capabilities for temporal security applications beyond intrusion detection including fraud detection, anomaly detection in industrial systems, and healthcare monitoring.

\begin{thebibliography}{99}

\bibitem{chen2018neural}
R.~T.~Q.~Chen, Y.~Rubanova, J.~Bettencourt, and D.~Duvenaud,
``Neural ordinary differential equations,''
in \emph{Advances in Neural Information Processing Systems}, 2018, pp.~6571--6583.

\bibitem{salvi2024tabn}
C.~Salvi et al.,
``Temporal adaptive batch normalization for neural ODEs,''
in \emph{Advances in Neural Information Processing Systems}, 2024.

\end{thebibliography}

\end{document}
