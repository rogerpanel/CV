{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPCIjXAk2e5D4qsQgwvtvxY"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# -*- coding: utf-8 -*-\n\"\"\"\nIntegrated DQN-Actor-Critic Architecture for Poisoning Attack Detection\nAuthor: Your Name\nDate: 2024\n\"\"\"\n\n\"\"\"# Cell 1: Imports and Mathematical Framework\n\n## Mathematical Framework for DQN-Actor-Critic Based Poisoning Detection\n\n1. Feature Space Definition:\n   F = {f âˆˆ â„áµˆ | f is a network flow feature vector}\n   where d is the dimension of extracted features\n\n2. Data Distribution:\n   P(x) = Normal network traffic distribution\n   Q(x) = Poisoned traffic distribution\n   KL(P||Q) = Measure of distribution divergence\n\n3. Attack Space:\n   A = {a | a is a poisoning attack vector}\n   Impact(a) = Î£ ||f_original - f_poisoned||â‚‚\n\n4. Detection Functions:\n   DQN: Q(s,a) = ð”¼[R + Î³ max Q(s',a') | s,a]\n   Actor: Ï€(a|s) = P(action=a | state=s)\n   Critic: V(s) = ð”¼[Î£ Î³áµ—R_t | sâ‚€=s]\n\n5. Combined Detection Score:\n   D(x) = Î±Â·Q(x) + (1-Î±)Â·Ï€(x) + Î²Â·KL(P||Q_x)\n   where Q_x is the estimated distribution at point x\n\"\"\"","metadata":{"id":"eMjQlShIb6Dr"}},{"cell_type":"markdown","source":"## TPU Verification","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef setup_kaggle_environment():\n    \"\"\"Setup Kaggle environment and verify connectivity\"\"\"\n    print(\"Setting up Kaggle environment...\")\n\n    # Check if we're in Kaggle environment\n    IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n    if IN_KAGGLE:\n        print(\"Running in Kaggle environment\")\n\n        # Install required packages if needed\n        try:\n            subprocess.check_call(['pip', 'install', '--quiet', 'torch_xla'])\n            print(\"Installed torch_xla successfully\")\n        except:\n            print(\"torch_xla already installed or installation failed\")\n\n        # Reset notebook connection if needed\n        try:\n            from IPython.display import clear_output\n            clear_output(wait=True)\n            print(\"Cleared notebook output\")\n        except:\n            pass\n\n        # Set Kaggle-specific environment variables\n        os.environ['XLA_USE_BF16'] = \"1\"\n        os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\n        # Verify TPU availability\n        try:\n            import torch_xla.core.xla_model as xm\n            print(\"TPU backend initialized successfully\")\n        except ImportError:\n            print(\"Could not initialize TPU backend\")\n\n    else:\n        print(\"Not running in Kaggle environment\")\n\n    return IN_KAGGLE\n\n# Initialize environment\nIN_KAGGLE = setup_kaggle_environment()\n\n# Restart kernel function\ndef restart_kernel():\n    \"\"\"Helper function to restart kernel if needed\"\"\"\n    from IPython.display import Javascript\n    display(Javascript('IPython.notebook.kernel.restart()'))\n\nprint(\"Environment setup complete. If you see connection errors, run restart_kernel()\")\n\n","metadata":{"id":"fs42wweDPw2t","executionInfo":{"status":"ok","timestamp":1732040976233,"user_tz":-180,"elapsed":447,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GPU verification","metadata":{"id":"N88B-9zoRAT2"}},{"cell_type":"code","source":"def check_gpu():\n    \"\"\"Check and setup GPU if available\"\"\"\n    if torch.cuda.is_available():\n        # Print GPU info\n        print(f\"CUDA is available:\")\n        print(f\"- GPU Device: {torch.cuda.get_device_name(0)}\")\n        print(f\"- CUDA Version: {torch.version.cuda}\")\n        print(f\"- Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB\")\n\n        # Set device\n        device = torch.device(\"cuda:0\")\n\n        # Enable optimizations\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n\n        return device\n    else:\n        print(\"No GPU detected. Using CPU.\")\n        return torch.device(\"cpu\")","metadata":{"id":"-t3ODEIIRCjT","executionInfo":{"status":"ok","timestamp":1732040977533,"user_tz":-180,"elapsed":468,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installs","metadata":{}},{"cell_type":"code","source":"# Usage in notebook\n!pip install torch_xla\n!pip install cloud-tpu-client\n!pip install cloud-tpu-client==0.10 torch_xla==2.0 torch==2.0.0 torchvision==0.15.1 -f https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Liberaries","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\ntry:\n    import torch_xla.debug.metrics as met\nexcept ImportError:\n    print(\"Note: TPU metrics module not available\")\n# imoisoningdetectionmetricort requests\n# requests.get('http://metadata.google.internal/computeMetadata/v1/instance/name', headers={'Metadata-Flavor': 'Google'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard imports\n\nimport gc\nimport time\nimport random\nimport logging\nimport traceback\nfrom typing import Dict, List, Tuple\nfrom collections import deque, defaultdict\n\n# Data processing\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nfrom sklearn.model_selection import train_test_split\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Machine Learning\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.utils import Sequence\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom typing import Dict, Any, List, Union\n\n# Utilities\nimport psutil\nfrom tqdm import tqdm\nimport contextlib\nfrom torch.nn.functional import sigmoid\n\nimport torch.cuda\nimport torch.backends.cudnn\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.metrics import (confusion_matrix, roc_auc_score, precision_score, \n                           recall_score, f1_score, average_precision_score,\n                           brier_score_loss, matthews_corrcoef, cohen_kappa_score)\n","metadata":{"id":"ZczGsNejsD_Z","executionInfo":{"status":"ok","timestamp":1732040986608,"user_tz":-180,"elapsed":9077,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        import torch_xla\n        import torch_xla.core.xla_model as xm\n        import torch_xla.distributed.parallel_loader as pl\n        import torch_xla.distributed.xla_multiprocessing as xmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add TPU call","metadata":{}},{"cell_type":"code","source":"# TPU specific imports with error handling\ndef setup_tpu_imports():\n    try:\n        import torch_xla\n        import torch_xla.core.xla_model as xm\n        import torch_xla.distributed.parallel_loader as pl\n        import torch_xla.distributed.xla_multiprocessing as xmp\n        \n        # Verify TPU is available\n        devices = xm.get_xla_supported_devices()\n        if devices:\n            print(f\"TPU devices available: {len(devices)}\")\n            return True, (torch_xla, xm, pl, xmp)\n        else:\n            print(\"No TPU devices found, falling back to CPU/GPU\")\n            return False, None\n    except ImportError as e:\n        print(f\"TPU imports failed: {str(e)}\")\n        print(\"Falling back to CPU/GPU\")\n        return False, None\n    except Exception as e:\n        print(f\"Unexpected error during TPU setup: {str(e)}\")\n        return False, None\n\ndef initialize_device():\n    \"\"\"Initialize compute device with proper error handling\"\"\"\n    # Try TPU first\n    tpu_available, tpu_modules = setup_tpu_imports()\n    if tpu_available:\n        try:\n            xm = tpu_modules[1]\n            device = xm.xla_device()\n            print(f\"Using TPU device: {device}\")\n            return device, tpu_modules\n        except Exception as e:\n            print(f\"TPU initialization failed: {str(e)}\")\n    \n    # Fall back to GPU if available\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n        return device, None\n    \n    # Fall back to CPU\n    print(\"Using CPU\")\n    return torch.device(\"cpu\"), None\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Device Manager\n","metadata":{}},{"cell_type":"code","source":"class DeviceManager:\n    \"\"\"Centralized device management for consistent handling\"\"\"\n    @staticmethod\n    def initialize_device():\n        try:\n            # Try TPU first\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n            use_tpu = True\n            print(\"Using TPU device\")\n        except:\n            # Fallback to GPU/CPU\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            use_tpu = False\n            print(f\"Using device: {device}\")\n        return device, use_tpu\n\n    @staticmethod\n    def to_tensor(data, device):\n        \"\"\"Safely convert data to tensor on specified device\"\"\"\n        try:\n            if isinstance(data, np.ndarray):\n                return torch.from_numpy(data).to(device)\n            elif isinstance(data, torch.Tensor):\n                return data.to(device)\n            elif isinstance(data, (list, tuple)):\n                return torch.tensor(data).to(device)\n            else:\n                raise TypeError(f\"Unsupported data type: {type(data)}\")\n        except Exception as e:\n            print(f\"Error converting to tensor: {str(e)}\")\n            return None\n\n    @staticmethod\n    def sync_device(device):\n        \"\"\"Synchronize device operations\"\"\"\n        if str(device).startswith('xla'):\n            import torch_xla.core.xla_model as xm\n            xm.mark_step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Handling","metadata":{"id":"pnEBLV4Dk6T-"}},{"cell_type":"code","source":"# At start of notebook\n!nvidia-smi\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1732040986608,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"},"user_tz":-180},"id":"3iT3EB2ukM9v","outputId":"e36936cf-9cf5-4613-8003-619b0e9ad0d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LabelHandler:\n    \"\"\"Handle both binary and multi-class labels for poisoning detection\"\"\"\n    def __init__(self):\n        self.label_mapping = {}  # Store original label meanings\n        self.attack_types = {}   # Store attack type categories\n        self.binary_mapping = {} # Map between binary and multi-class\n\n    def process_labels(self, labels: np.ndarray, label_names: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Process labels to maintain both binary and multi-class information\n\n        Args:\n            labels: Original multi-class labels\n            label_names: Optional list of label names/descriptions\n\n        Returns:\n            Tuple of (binary_labels, multi_class_labels)\n        \"\"\"\n        unique_labels = np.unique(labels)\n\n        # Store mapping if not already created\n        if not self.label_mapping:\n            self.label_mapping = {\n                idx: name if label_names is not None and idx < len(label_names) else f\"Class_{idx}\"\n                for idx in unique_labels\n            }\n\n            # Create binary mapping (0 for normal, 1 for any attack)\n            self.binary_mapping = {\n                idx: 0 if idx == 0 else 1  # Assuming 0 is normal traffic\n                for idx in unique_labels\n            }\n\n            # Store attack types separately\n            self.attack_types = {\n                idx: label\n                for idx, label in self.label_mapping.items()\n                if idx != 0  # Exclude normal traffic\n            }\n\n        # Create both label versions\n        binary_labels = np.array([self.binary_mapping[l] for l in labels])\n        multi_labels = labels.copy()\n\n        return binary_labels, multi_labels\n\n    def get_attack_info(self, attack_id: int) -> Dict[str, Union[str, bool, int]]:\n        \"\"\"Get information about a specific attack type\"\"\"\n        if attack_id not in self.label_mapping:\n            return {\n                'attack_name': f'Unknown_Attack_{attack_id}',\n                'is_attack': True,\n                'attack_id': attack_id,\n                'binary_class': self.binary_mapping.get(attack_id, 1)\n            }\n\n        return {\n            'attack_name': self.label_mapping[attack_id],\n            'is_attack': self.binary_mapping.get(attack_id, 1) == 1,\n            'attack_id': attack_id,\n            'binary_class': self.binary_mapping.get(attack_id, 1)\n        }\n\n    def get_attack_stats(self, multi_labels: np.ndarray) -> Dict[str, Dict[str, Union[int, float, bool]]]:\n        \"\"\"Get statistics about attack distribution\"\"\"\n        unique, counts = np.unique(multi_labels, return_counts=True)\n        total = len(multi_labels)\n\n        stats = {}\n        for u, c in zip(unique, counts):\n            attack_info = self.get_attack_info(u)\n            stats[attack_info['attack_name']] = {\n                'count': int(c),\n                'percentage': float(c/total),\n                'is_attack': attack_info['is_attack']\n            }\n\n        return stats\n\n    def print_distribution(self, labels: np.ndarray):\n        \"\"\"Print distribution of attacks in dataset\"\"\"\n        stats = self.get_attack_stats(labels)\n\n        print(\"\\nAttack Distribution:\")\n        print(\"-\" * 50)\n        print(f\"{'Attack Type':<30} {'Count':>8} {'Percentage':>12}\")\n        print(\"-\" * 50)\n\n        for attack_name, info in stats.items():\n            print(f\"{attack_name:<30} {info['count']:>8} {info['percentage']:>11.2f}%\")\n\n    def __str__(self) -> str:\n        return f\"LabelHandler with {len(self.attack_types)} attack types\"\n\n    def __repr__(self) -> str:\n        return f\"LabelHandler(n_attacks={len(self.attack_types)}, n_labels={len(self.label_mapping)})\"\n\n","metadata":{"id":"lfMDodREk8aD","executionInfo":{"status":"ok","timestamp":1732040987166,"user_tz":-180,"elapsed":565,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attack type mapping","metadata":{}},{"cell_type":"code","source":"# Add after LabelHandler class (around line 300)\nclass AttackTypeMapper:\n    \"\"\"Maps complete attack types for each dataset\"\"\"\n    @staticmethod \n    def get_mappings():\n        return {\n            'cic': {\n                0: 'Normal/Benign',\n                1: 'DDoS',\n                2: 'DoS', \n                3: 'Reconnaissance',\n                4: 'Backdoor',\n                5: 'SQL_Injection',\n                6: 'Password_Attack',\n                7: 'XSS',\n                8: 'Man_in_the_Middle',\n                9: 'Scanning'\n            },\n            'ton': {\n                0: 'Normal/Benign',\n                1: 'Scanning',\n                2: 'DoS',\n                3: 'DDoS', \n                4: 'Ransomware',\n                5: 'Backdoor',\n                6: 'Data_Theft',\n                7: 'Keylogging',\n                8: 'OS_Fingerprint',\n                9: 'Service_Scan',\n                10: 'Data_Exfiltration',\n                11: 'SQL_Injection',\n                12: 'MITM',\n                13: 'Spam',\n                14: 'XSS',\n                15: 'Cryptojacking',\n                16: 'Command_Injection',\n                17: 'Rootkit',\n                18: 'Trojan',\n                19: 'Worm',\n                20: 'Botnet',\n                21: 'Malware',\n                22: 'Vulnerability_Scan',\n                23: 'Password_Attack',\n                24: 'Privilege_Escalation',\n                25: 'Protocol_Manipulation',\n                26: 'Remote_Shell',\n                27: 'SSL_Attack',\n                28: 'Tunneling',\n                29: 'Web_Attack',\n                30: 'Zero_Day',\n                31: 'APT',\n                32: 'Code_Execution',\n                33: 'Brute_Force'\n            },\n            'cse': {\n                0: 'Normal/Benign',\n                1: 'Bot',\n                2: 'Brute_Force',\n                3: 'DoS_Hulk',\n                4: 'DoS_GoldenEye', \n                5: 'DoS_Slowloris',\n                6: 'DoS_Slowhttptest',\n                7: 'FTP_Patator',\n                8: 'Heartbleed',\n                9: 'Infiltration',\n                10: 'SQL_Injection'\n            }\n        }\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## System setup","metadata":{"id":"F6DJ7Ruik_Qg"}},{"cell_type":"code","source":"# System setup function\ndef setup_system():\n    \"\"\"Setup system checks and cleanup before training\"\"\"\n    print(\"\\n=== System Setup ===\")\n    \n    # Verify TPU first\n    if TPUHandler.is_tpu_available():\n        print(\"TPU support verified\")\n    else:\n        print(\"TPU not available, using alternative device\")\n\n    # Clear memory\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n        print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n        print(f\"CUDA Capability: {torch.cuda.get_device_capability()}\")\n    else:\n        print(\"Using CPU for training\")\n\n    # Check available memory\n    process = psutil.Process()\n    print(f\"Initial memory usage: {process.memory_info().rss/1024/1024:.2f}MB\")\n\n    # Initialize logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('training.log'),\n            logging.StreamHandler()\n        ]\n    )\n\n    # Verify system requirements\n    memory_gb = psutil.virtual_memory().total / (1024**3)\n    if memory_gb < 8:  # Minimum 8GB required\n        raise RuntimeError(f\"Insufficient memory: {memory_gb:.1f}GB < 8GB required\")\n\n    # Set random seeds for reproducibility\n    random.seed(42)\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n\n    print(\"System setup completed successfully\")\n    return True\n\n# Memory monitoring class\nclass MemoryMonitor:\n    \"\"\"Monitor system memory usage during training\"\"\"\n    def __init__(self, log_interval=60):  # Log every 60 seconds\n        self.log_interval = log_interval\n        self.last_log_time = time.time()\n\n        # Setup logging\n        logging.basicConfig(\n            filename='memory_usage.log',\n            level=logging.INFO,\n            format='%(asctime)s - %(message)s'\n        )\n\n    def check_memory(self):\n        current_time = time.time()\n        if current_time - self.last_log_time >= self.log_interval:\n            process = psutil.Process()\n            memory_info = process.memory_info()\n            system_memory = psutil.virtual_memory()\n\n            logging.info(\n                f\"Memory Usage - RSS: {memory_info.rss/1024/1024:.2f}MB, \"\n                f\"VMS: {memory_info.vms/1024/1024:.2f}MB, \"\n                f\"System Memory Used: {system_memory.percent}%\"\n            )\n\n            self.last_log_time = current_time\n            return system_memory.percent > 90  # Warning threshold\n        return False\n\nclass GPUMemoryManager:\n    @staticmethod\n    def print_memory_stats():\n        \"\"\"Print memory usage for either GPU or CPU\"\"\"\n        if torch.cuda.is_available():\n            print(\"\\nGPU Memory Usage:\")\n            print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n            print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n        else:\n            process = psutil.Process()\n            print(\"\\nCPU Memory Usage:\")\n            print(f\"RSS: {process.memory_info().rss/1e9:.2f}GB\")\n            print(f\"VMS: {process.memory_info().vms/1e9:.2f}GB\")\n\n    @staticmethod\n    def clear_memory():\n        \"\"\"Clear memory cache\"\"\"\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    @staticmethod\n    def get_memory_usage():\n        \"\"\"Get current memory usage as percentage\"\"\"\n        if torch.cuda.is_available():\n            return torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory\n        else:\n            process = psutil.Process()\n            return process.memory_percent()\n\n","metadata":{"id":"UTqhJQ6e8JUb","executionInfo":{"status":"ok","timestamp":1732040987167,"user_tz":-180,"elapsed":12,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TPU Handler","metadata":{}},{"cell_type":"code","source":"class TPUHandler:\n    \"\"\"Handles TPU initialization and management\"\"\"\n    @staticmethod\n    def setup_tpu():\n        \"\"\"Initialize TPU with proper error handling\"\"\"\n        try:\n            import torch_xla\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n            print(\"TPU device initialized successfully\")\n            return device, True\n        except (ImportError, AttributeError) as e:\n            print(f\"TPU initialization failed: {str(e)}\")\n            # Fallback to GPU/CPU\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Falling back to: {device}\")\n            return device, False\n        except Exception as e:\n            print(f\"Unexpected error during TPU setup: {str(e)}\")\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Falling back to: {device}\")\n            return device, False\n\n    @staticmethod\n    def is_tpu_available():\n        \"\"\"Check if TPU is available\"\"\"\n        try:\n            import torch_xla.core.xla_model as xm\n            return True\n        except:\n            return False\n\n    @staticmethod\n    def sync_device(device):\n        \"\"\"Synchronize TPU operations if needed\"\"\"\n        if str(device).startswith('xla'):\n            try:\n                import torch_xla.core.xla_model as xm\n                xm.mark_step()\n            except Exception as e:\n                print(f\"TPU sync failed: {str(e)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model configuration","metadata":{"id":"7aeQrTt-cQDC"}},{"cell_type":"code","source":"class ModelConfig:\n    def __init__(self):\n        \"\"\"Initialize model configuration with proper device handling\"\"\"\n        # Initialize device first\n        self.device, self.use_tpu = DeviceManager.initialize_device()\n\n        # Set device-specific configurations\n        if self.use_tpu:\n            self.batch_size = 512 if self.use_tpu else 256\n            self.gradient_accumulation_steps = 2\n            self.num_workers = 8\n            self.use_amp = False\n        elif torch.cuda.is_available():\n            self.batch_size = 512\n            self.num_workers = 4\n            self.use_amp = True\n            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n        else:\n            self.batch_size = 128\n            self.num_workers = min(2, os.cpu_count())\n            self.use_amp = False\n            print(\"Using CPU\")\n\n        # Model Architecture\n        self.feature_dim = 128\n        self.hidden_dim = 256\n        self.num_actions = 2\n        self.num_heads = 4\n        self.dropout_rate = 0.2\n\n        # Training Parameters\n        self.learning_rate = 1e-4\n        self.sequence_length = 10\n        self.num_epochs = 100\n        self.gamma = 0.99\n\n        # DQN Specific\n        self.epsilon_start = 1.0\n        self.epsilon_end = 0.01\n        self.epsilon_decay = 0.995\n        \n        # Integration parameters\n        self.dqn_weight = 0.6\n        self.ac_weight = 0.4\n        \n        # Memory and Buffer\n        self.replay_buffer_size = 10000\n        self.min_replay_size = 1000\n        \n        # Optimization\n        self.gradient_clip = 1.0\n        self.warmup_steps = 1000\n        self.target_update_freq = 10\n        \n        # Early Stopping\n        self.patience = 5\n        self.min_delta = 0.001\n        \n        # Directories\n        self.checkpoint_dir = 'checkpoints'\n        self.log_dir = 'logs'\n        \n        # Setup device-specific optimizations\n        self._setup_device_specific()\n        \n        # Print initial configuration\n        self.print_config()\n\n    def _setup_device_specific(self):\n        \"\"\"Setup device-specific optimizations\"\"\"\n        if self.use_tpu:\n            # TPU-specific optimizations\n            pass\n        elif self.device.type == \"cuda\":\n            # GPU optimizations\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n            torch.backends.cudnn.benchmark = True\n            self.pin_memory = True\n        else:\n            # CPU optimizations\n            self.pin_memory = False\n            if hasattr(torch, 'set_num_threads'):\n                torch.set_num_threads(self.num_workers)\n\n    def print_config(self):\n        \"\"\"Print the current configuration\"\"\"\n        print(\"\\nModel Configuration:\")\n        print(f\"- Device: {self.device}\")\n        print(f\"- TPU Available: {self.use_tpu}\")\n        print(f\"- Feature Dimension: {self.feature_dim}\")\n        print(f\"- Hidden Dimension: {self.hidden_dim}\")\n        print(f\"- Batch Size: {self.batch_size}\")\n        print(f\"- Number of Workers: {self.num_workers}\")\n        print(f\"- Learning Rate: {self.learning_rate}\")\n        print(f\"- Number of Epochs: {self.num_epochs}\")\n        print(f\"- DQN Weight: {self.dqn_weight}\")\n        print(f\"- Actor-Critic Weight: {self.ac_weight}\")\n        print(f\"- AMP Enabled: {self.use_amp}\")\n\n        if self.device.type == \"cuda\":\n            print(f\"- CUDA Capability: {torch.cuda.get_device_capability()}\")\n            print(f\"- GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n\n    def get_device_info(self) -> Dict[str, Any]:\n        \"\"\"Get detailed device information\"\"\"\n        info = {\n            'device_type': self.device.type,\n            'use_tpu': self.use_tpu,\n            'use_amp': self.use_amp,\n            'batch_size': self.batch_size,\n            'num_workers': self.num_workers\n        }\n\n        if self.device.type == \"cuda\":\n            info.update({\n                'cuda_capability': torch.cuda.get_device_capability(),\n                'gpu_name': torch.cuda.get_device_name(0),\n                'gpu_memory': torch.cuda.get_device_properties(0).total_memory\n            })\n\n        return info\n    ","metadata":{"id":"PRwDyLgdcJc_","executionInfo":{"status":"ok","timestamp":1732040987167,"user_tz":-180,"elapsed":12,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Safe Serialization","metadata":{}},{"cell_type":"code","source":"class ModelSerializer:\n    \"\"\"Safe model serialization and deserialization\"\"\"\n    @staticmethod\n    def save_model(model, path, additional_info=None):\n        try:\n            save_dict = {\n                'model_state_dict': model.state_dict(),\n                'config': ModelSerializer._prepare_config(additional_info)\n            }\n            torch.save(save_dict, path)\n            return True\n        except Exception as e:\n            print(f\"Error saving model: {str(e)}\")\n            return False\n\n    @staticmethod\n    def _prepare_config(config):\n        \"\"\"Prepare configuration for serialization\"\"\"\n        if config is None:\n            return {}\n            \n        def convert_to_serializable(obj):\n            if isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif isinstance(obj, dict):\n                return {k: convert_to_serializable(v) for k, v in obj.items()}\n            elif isinstance(obj, list):\n                return [convert_to_serializable(i) for i in obj]\n            elif isinstance(obj, (int, float, str, bool)):\n                return obj\n            else:\n                return str(obj)\n                \n        return convert_to_serializable(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading and Preprocessing","metadata":{"id":"PN4r4SVOcaOb"}},{"cell_type":"code","source":"## DATASET SPECIFIC PROCESSOR\n\nclass DatasetSpecificProcessor:\n    \"\"\"Processes features specific to each dataset type\"\"\"\n    def __init__(self, dataset_type: str):\n        self.dataset_type = dataset_type.lower()\n        self.protocol_features = {}\n        self.temporal_windows = {}\n        self.flow_statistics = {}\n\n    def process_features(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process features based on dataset type\"\"\"\n        # Ensure data is properly formatted as DataFrame with string column names\n        if not isinstance(data, pd.DataFrame):\n            data = pd.DataFrame(data)\n\n        # Convert numeric column indices to string names if needed\n        if all(isinstance(col, int) for col in data.columns):\n            data.columns = [f'feature_{i}' for i in range(len(data.columns))]\n\n        if self.dataset_type == 'cic':\n            return self._process_cic_features(data)\n        elif self.dataset_type == 'ton':\n            return self._process_ton_features(data)\n        elif self.dataset_type == 'cse':\n            return self._process_cse_features(data)\n        else:\n            raise ValueError(f\"Unknown dataset type: {self.dataset_type}\")\n\n    def _process_cic_features(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process CIC-IoT specific features\"\"\"\n        processed_data = data.copy()\n\n        # Find protocol-related columns (case insensitive)\n        protocol_columns = [\n            col for col in processed_data.columns\n            if isinstance(col, str) and 'protocol' in col.lower()\n        ]\n\n        # Process protocol features if any exist\n        if protocol_columns:\n            # One-hot encoding for protocol columns\n            processed_data = pd.get_dummies(\n                processed_data,\n                columns=protocol_columns,\n                prefix=['protocol']\n            )\n\n        # Calculate packet and connection rates for numeric columns\n        numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n        if 'packet_count' in numeric_cols and 'duration' in numeric_cols:\n            processed_data['packet_rate'] = (\n                processed_data['packet_count'] /\n                processed_data['duration'].clip(lower=1e-6)\n            )\n\n        if 'connection_count' in numeric_cols and 'duration' in numeric_cols:\n            processed_data['connection_rate'] = (\n                processed_data['connection_count'] /\n                processed_data['duration'].clip(lower=1e-6)\n            )\n\n        return processed_data\n\n    def _process_ton_features(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process TON-IoT specific features\"\"\"\n        processed_data = data.copy()\n        window_size = 10\n\n        # Process numeric features\n        numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n        for col in numeric_cols:\n            # Calculate temporal mean and variance safely\n            try:\n                processed_data[f'{col}_temporal_mean'] = processed_data[col].rolling(\n                    window=window_size, min_periods=1\n                ).mean()\n                processed_data[f'{col}_temporal_var'] = processed_data[col].rolling(\n                    window=window_size, min_periods=1\n                ).var()\n            except Exception as e:\n                print(f\"Warning: Could not process column {col}: {str(e)}\")\n\n        # Find and process service-related columns\n        service_columns = [\n            col for col in processed_data.columns\n            if isinstance(col, str) and 'service' in col.lower()\n        ]\n\n        if service_columns:\n            for service in service_columns:\n                try:\n                    # Convert to numeric if needed\n                    if not pd.api.types.is_numeric_dtype(processed_data[service]):\n                        processed_data[service] = pd.to_numeric(\n                            processed_data[service], errors='coerce'\n                        )\n                    # Calculate service rate\n                    processed_data[f'{service}_rate'] = processed_data[service].rolling(\n                        window=window_size, min_periods=1\n                    ).mean()\n                except Exception as e:\n                    print(f\"Warning: Could not process service column {service}: {str(e)}\")\n\n        return processed_data\n\n    def _process_cse_features(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process CSE-CIC specific features\"\"\"\n        processed_data = data.copy()\n        window_size = 10\n\n        # Process numeric features only\n        numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n\n        # Process flow-based features\n        if 'bytes_transferred' in numeric_cols and 'duration' in numeric_cols:\n            processed_data['flow_byte_rate'] = (\n                processed_data['bytes_transferred'] /\n                processed_data['duration'].clip(lower=1e-6)\n            )\n\n        # Process packet statistics\n        if 'packet_size' in numeric_cols:\n            processed_data['packet_size_mean'] = processed_data['packet_size'].rolling(\n                window=window_size, min_periods=1\n            ).mean()\n            processed_data['packet_size_std'] = processed_data['packet_size'].rolling(\n                window=window_size, min_periods=1\n            ).std()\n\n        # Process flow statistics\n        if 'flow_duration' in numeric_cols:\n            processed_data['flow_rate'] = 1.0 / processed_data['flow_duration'].clip(lower=1e-6)\n            processed_data['flow_rate_mean'] = processed_data['flow_rate'].rolling(\n                window=window_size, min_periods=1\n            ).mean()\n\n        return processed_data\n\n\nclass PoisoningFeatureExtractor:\n    \"\"\"Optimized poisoning feature extractor with full feature set\"\"\"\n    def __init__(self, window_size: int = 100, feature_dim: int = None):\n        self.window_size = window_size\n        self.feature_history = deque(maxlen=window_size)\n        self.distribution_history = deque(maxlen=window_size)\n        self.feature_dim = feature_dim\n        self.eps = 1e-8\n        \n    def extract_poisoning_features(self, data: pd.DataFrame) -> Dict[str, np.ndarray]:\n        \"\"\"Extract all poisoning-specific features with optimized computation\"\"\"\n        try:\n            # Ensure data is properly formatted\n            if not isinstance(data, pd.DataFrame):\n                data = pd.DataFrame(data)\n                \n            # Initialize feature dictionary\n            features = {}\n            \n            # Extract all features with proper error handling\n            features.update(self._detect_distribution_drift(data))\n            features.update(self._check_temporal_consistency(data))\n            features.update(self._analyze_protocol_behavior(data))\n            features.update(self._detect_traffic_anomalies(data))\n            \n            return features\n            \n        except Exception as e:\n            print(f\"Warning: Error in feature extraction: {str(e)}\")\n            return {\n                'distribution_drift': np.array([0.0]),\n                'temporal_consistency': np.array([0.0]),\n                'protocol_frequency': np.array([0.0]),\n                'traffic_anomaly': np.array([0.0])\n            }\n\n    def _detect_distribution_drift(self, data: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Optimized distribution drift detection\"\"\"\n        try:\n            numeric_data = data.select_dtypes(include=[np.number])\n            \n            if len(self.feature_history) == 0:\n                self.feature_history.append(numeric_data)\n                return {'distribution_drift': np.array([0.0])}\n                \n            # Efficient mean computation\n            current_mean = numeric_data.mean()\n            previous_mean = np.nanmean([x.mean() for x in self.feature_history], axis=0)\n            \n            # Optimized covariance computation\n            current_values = numeric_data.values\n            previous_values = np.vstack([x.values for x in self.feature_history])\n            \n            # Add stability term to covariance matrices\n            current_cov = np.cov(current_values.T) + self.eps * np.eye(current_values.shape[1])\n            previous_cov = np.cov(previous_values.T) + self.eps * np.eye(previous_values.shape[1])\n            \n            # Calculate drift score\n            mean_diff = np.linalg.norm(current_mean - previous_mean)\n            cov_diff = np.linalg.norm(current_cov - previous_cov, ord='fro')\n            drift_score = (mean_diff + cov_diff) / (1.0 + self.eps)\n            \n            self.feature_history.append(numeric_data)\n            \n            return {'distribution_drift': np.array([drift_score])}\n            \n        except Exception as e:\n            print(f\"Warning: Error in drift detection: {str(e)}\")\n            return {'distribution_drift': np.array([0.0])}\n\n    def _check_temporal_consistency(self, data: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Optimized temporal consistency check\"\"\"\n        try:\n            numeric_data = data.select_dtypes(include=[np.number])\n            \n            if len(self.feature_history) == 0:\n                return {'temporal_consistency': np.array([0.0])}\n                \n            previous_data = self.feature_history[-1]\n            common_cols = set(numeric_data.columns) & set(previous_data.columns)\n            \n            if not common_cols:\n                return {'temporal_consistency': np.array([0.0])}\n                \n            # Vectorized consistency calculation\n            consistency_scores = np.array([\n                np.abs(numeric_data[col].values - previous_data[col].values).mean()\n                for col in common_cols\n            ])\n            \n            return {'temporal_consistency': np.array([consistency_scores.mean()])}\n            \n        except Exception as e:\n            print(f\"Warning: Error in temporal consistency: {str(e)}\")\n            return {'temporal_consistency': np.array([0.0])}\n\n    def _analyze_protocol_behavior(self, data: pd.DataFrame) -> Dict[str, np.ndarray]:\n        \"\"\"Optimized protocol behavior analysis\"\"\"\n        try:\n            protocol_columns = [\n                col for col in data.columns\n                if isinstance(col, str) and 'protocol' in col.lower()\n            ]\n            \n            if not protocol_columns:\n                return {'protocol_frequency': np.array([0.0])}\n                \n            # Vectorized protocol frequency calculation\n            protocol_data = data[protocol_columns]\n            protocol_freqs = protocol_data.mean().values\n            \n            # Store in distribution history for trend analysis\n            self.distribution_history.append(protocol_freqs)\n            \n            return {'protocol_frequency': protocol_freqs}\n            \n        except Exception as e:\n            print(f\"Warning: Error in protocol analysis: {str(e)}\")\n            return {'protocol_frequency': np.array([0.0])}\n\n    def _detect_traffic_anomalies(self, data: pd.DataFrame) -> Dict[str, np.ndarray]:\n        \"\"\"Optimized traffic anomaly detection\"\"\"\n        try:\n            numeric_data = data.select_dtypes(include=[np.number])\n            \n            if 'packet_count' not in numeric_data.columns:\n                return {'traffic_anomaly': np.array([0.0])}\n                \n            packet_counts = numeric_data['packet_count'].values\n            expected_count = np.mean(packet_counts)\n            \n            # Vectorized anomaly score calculation\n            anomaly_scores = np.abs(\n                (packet_counts - expected_count) /\n                np.maximum(expected_count, self.eps)\n            )\n            \n            return {'traffic_anomaly': anomaly_scores}\n            \n        except Exception as e:\n            print(f\"Warning: Error in anomaly detection: {str(e)}\")\n            return {'traffic_anomaly': np.array([0.0])}\n\n    def get_feature_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about extracted features\"\"\"\n        return {\n            'window_size': len(self.feature_history),\n            'distribution_window': len(self.distribution_history),\n            'feature_dimensions': self.feature_dim,\n            'total_samples_processed': sum(len(x) for x in self.feature_history)\n        }\n\n    def reset(self):\n        \"\"\"Reset extractor state\"\"\"\n        self.feature_history.clear()\n        self.distribution_history.clear()\n\n    \nclass PoisoningGenerator:\n    \"\"\"Generates synthetic poisoning samples\"\"\"\n    def __init__(self, epsilon: float = 0.1, flip_rate: float = 0.1):\n        self.epsilon = epsilon\n        self.flip_rate = flip_rate\n\n    def generate_gradient_based_poisoning(self, data: torch.Tensor, loss_fn: callable) -> torch.Tensor:\n        \"\"\"Generate gradient-based poisoning samples\"\"\"\n        data.requires_grad = True\n        loss = loss_fn(data)\n        gradient = torch.autograd.grad(loss, data)[0]\n        poisoned_data = data + self.epsilon * torch.sign(gradient)\n        return poisoned_data.detach()\n\n    def generate_label_flipping_attacks(self, labels: np.ndarray) -> np.ndarray:\n        \"\"\"Generate label flipping attacks\"\"\"\n        flipped_labels = labels.copy()\n        flip_mask = np.random.random(len(labels)) < self.flip_rate\n        flipped_labels[flip_mask] = 1 - labels[flip_mask]\n        return flipped_labels\n\n    def generate_backdoor_triggers(self, data: np.ndarray, trigger_pattern: np.ndarray) -> np.ndarray:\n        \"\"\"Generate backdoor triggers\"\"\"\n        poisoned_data = data.copy()\n        backdoor_mask = np.random.random(len(data)) < self.flip_rate\n        poisoned_data[backdoor_mask] += trigger_pattern\n        return poisoned_data\n\n    def generate_clean_label_poisoning(self, data: np.ndarray, boundary_shift: np.ndarray) -> np.ndarray:\n        \"\"\"Generate clean label poisoning\"\"\"\n        poisoned_data = data.copy()\n        poison_mask = np.random.random(len(data)) < self.flip_rate\n        poisoned_data[poison_mask] += boundary_shift\n        return poisoned_data\n\n\n## DATASET LOADER\n\n\n\nclass EnhancedDatasetLoader:\n    \"\"\"Enhanced dataset loader with dataset-specific processing and poisoning detection\"\"\"\n    def __init__(self, dataset_type: str, config: ModelConfig = None):\n        self.dataset_type = dataset_type.lower()\n        self.config = config or ModelConfig()\n\n        # Label column mappings\n        self.label_columns = {\n            'cic': 'Label',\n            'ton': 'label',\n            'cse': ' Label'  # Note the space before Label for CSE dataset\n        }\n\n        # Initialize processors\n        self.feature_scaler = StandardScaler()\n        self.label_encoder = LabelEncoder()\n        self.label_handler = LabelHandler()\n        self.dataset_processor = DatasetSpecificProcessor(self.dataset_type)\n        self.poisoning_extractor = PoisoningFeatureExtractor()\n        self.poisoning_generator = PoisoningGenerator()\n        self.data_stabilizer = DataStabilizer()\n\n        # Add clip values\n        self.max_value = 1e10\n        self.min_value = -1e10\n\n        # Validation thresholds\n        self.validation_thresholds = {\n            'mean_threshold': 0.1,\n            'std_threshold': 0.1,\n            'correlation_threshold': 0.95,\n            'min_class_ratio': 0.01\n        }\n\n        # Statistics tracking\n        self.stats = defaultdict(list)\n        self.feature_columns = None\n        self.removed_columns = set()\n        self._cached_label_column = None \n\n        print(f\"\\nInitialized Enhanced Dataset Loader for {self.dataset_type.upper()}\")\n\n    def _get_label_column(self, chunk: pd.DataFrame) -> str:\n        \"\"\"Get appropriate label column with caching\"\"\"\n        if self._cached_label_column is None:\n            # First try the predefined mapping\n            default_label = self.label_columns.get(self.dataset_type)\n            if default_label in chunk.columns:\n                self._cached_label_column = default_label\n                return default_label\n\n            # Try common variations\n            common_labels = ['Label', 'label', ' Label', 'type', 'class']\n            for label in common_labels:\n                if label in chunk.columns:\n                    self._cached_label_column = label\n                    return label\n\n            # If still not found, look for any column containing 'label'\n            label_cols = [col for col in chunk.columns if 'label' in col.lower()]\n            if label_cols:\n                self._cached_label_column = label_cols[0]\n                return label_cols[0]\n\n            raise ValueError(f\"No label column found for {self.dataset_type} dataset\")\n\n        return self._cached_label_column        \n    \n        \n    def _get_label_column(self, chunk: pd.DataFrame) -> str:\n        \"\"\"Get appropriate label column based on dataset type and available columns\"\"\"\n        # First try the predefined mapping\n        default_label = self.label_columns.get(self.dataset_type)\n        if default_label in chunk.columns:\n            return default_label\n\n        # Try common variations\n        common_labels = ['Label', 'label', ' Label', 'type', 'class']\n        for label in common_labels:\n            if label in chunk.columns:\n                return label\n\n        # If still not found, look for any column containing 'label' (case insensitive)\n        label_cols = [col for col in chunk.columns if 'label' in col.lower()]\n        if label_cols:\n            return label_cols[0]\n\n        raise ValueError(f\"No label column found for {self.dataset_type} dataset\")\n\n    def _process_chunk(self, chunk: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Process data chunk with appropriate label handling\"\"\"\n        try:\n            # Get label column\n            label_col = self._get_label_column(chunk)\n            print(f\"Found label column: {label_col}\")\n\n            if label_col not in chunk.columns:\n                raise ValueError(f\"Label column '{label_col}' not found\")\n\n            # Convert numeric columns\n            feature_cols = [col for col in chunk.columns if col != label_col]\n            numeric_chunk = pd.DataFrame()\n\n            for col in feature_cols:\n                try:\n                    numeric_chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n                except Exception as e:\n                    print(f\"Warning: Could not convert column {col}: {str(e)}\")\n                    numeric_chunk[col] = 0\n\n            # Handle missing values\n            numeric_chunk = numeric_chunk.fillna(0)\n\n            # Extract features and preprocess\n            features = self._preprocess_features(numeric_chunk.values)\n            labels = chunk[label_col].values\n\n            # Verify data\n            if features is None or len(features) == 0:\n                raise ValueError(\"No valid features extracted from chunk\")\n\n            return features, labels\n\n        except Exception as e:\n            print(f\"Error processing chunk: {str(e)}\")\n            return None, None\n\n\n    def _preprocess_features(self, features: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess features to handle infinities and large values\"\"\"\n        try:\n            if features is None or len(features) == 0:\n                return None\n\n            # Replace inf values\n            features = np.nan_to_num(features, nan=0.0, posinf=self.max_value, neginf=self.min_value)\n\n            # Clip extreme values\n            features = np.clip(features, self.min_value, self.max_value)\n\n            # Check for invalid values\n            if not np.all(np.isfinite(features)):\n                print(\"Warning: Invalid values found after preprocessing\")\n                features = np.nan_to_num(features, nan=0.0)\n\n            return features\n\n        except Exception as e:\n            print(f\"Error preprocessing features: {str(e)}\")\n            return None\n\n    def process_large_dataset(self, data, batch_size=10000):\n        chunks = np.array_split(data, len(data) // batch_size + 1)\n        processed_chunks = []\n\n        for chunk in chunks:\n            # Process chunk with memory cleanup\n            processed = self._process_chunk(chunk)\n            processed_chunks.append(processed)\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        return np.concatenate(processed_chunks)\n\n    \n    def load_and_process_dataset(self, file_path: str):\n\n        try:\n            print(f\"\\nProcessing {self.dataset_type.upper()} dataset: {file_path}\")\n\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n\n            chunks = []\n            labels = []\n            poisoning_features = defaultdict(list)\n            \n                \n        # Replace direct processing with chunked processing\n        if len(chunks) > 10000:  # Large dataset threshold\n            X = self.process_large_dataset(np.vstack(chunks))\n        else:\n            X = np.vstack(chunks)\n\n\n            # Read and process chunks\n            for chunk in pd.read_csv(file_path, chunksize=10000):\n                features, chunk_labels = self._process_chunk(chunk)\n\n                if features is not None and len(features) > 0:\n                    try:\n                        processed_chunk = self.dataset_processor.process_features(pd.DataFrame(features))\n                        processed_features = self._preprocess_features(processed_chunk.values)\n\n                        if processed_features is not None and len(processed_features) > 0:\n                            chunks.append(processed_features)\n                            labels.extend(chunk_labels)\n\n                            try:\n                                poison_features = self.poisoning_extractor.extract_poisoning_features(processed_chunk)\n                                for key, value in poison_features.items():\n                                    if value is not None and len(value) > 0:\n                                        poisoning_features[key].append(value)\n                            except Exception as e:\n                                print(f\"Warning: Error extracting poisoning features: {str(e)}\")\n\n                    except Exception as e:\n                        print(f\"Warning: Error processing chunk features: {str(e)}\")\n\n            if not chunks:\n                raise ValueError(\"No valid data chunks processed\")\n\n            # Combine and process data\n            X = np.vstack(chunks)\n            y = np.array(labels)\n\n            # Scale features\n            X_scaled = self.feature_scaler.fit_transform(X)\n\n            # Process labels\n            y_encoded = self.label_encoder.fit_transform(y)\n            binary_labels, multi_labels = self.label_handler.process_labels(y_encoded)\n\n            # Combine poisoning features\n            combined_poison_features = {}\n            for key, values in poisoning_features.items():\n                if values:\n                    try:\n                        combined_poison_features[key] = np.concatenate(values)\n                    except Exception as e:\n                        print(f\"Warning: Could not combine poisoning features for {key}: {str(e)}\")\n                        combined_poison_features[key] = np.zeros(len(X))\n\n            # Generate synthetic poisoning samples\n            poisoned_samples = self._generate_poisoning_samples(X_scaled, binary_labels)\n\n            print(\"\\nDataset Processing Complete:\")\n            print(f\"- Total samples: {len(y)}\")\n            print(f\"- Feature dimensions: {X_scaled.shape[1]}\")\n            print(f\"- Poisoning features extracted: {list(combined_poison_features.keys())}\")\n\n            return X_scaled, binary_labels, multi_labels, {\n                'poisoning_features': combined_poison_features,\n                'poisoned_samples': poisoned_samples,\n                'validation_stats': self._validate_data_quality(X_scaled, binary_labels, multi_labels)\n            }\n\n        except Exception as e:\n            print(f\"Error processing dataset: {str(e)}\")\n            traceback.print_exc()\n            raise\n\n\n    def _validate_data_quality(self, X: np.ndarray, binary_labels: np.ndarray, multi_labels: np.ndarray) -> Dict:\n        \"\"\"Validate data quality and compute statistics\"\"\"\n        validation_stats = {\n            'feature_stats': {\n                'mean': np.mean(X, axis=0),\n                'std': np.std(X, axis=0)\n            },\n            'class_distribution': {\n                'binary': np.bincount(binary_labels),\n                'multi': np.bincount(multi_labels)\n            },\n            'missing_values': np.isnan(X).sum(),\n            'feature_correlations': np.corrcoef(X.T)\n        }\n        return validation_stats\n\n    def _generate_poisoning_samples(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"Generate synthetic poisoning samples\"\"\"\n        X_tensor = torch.FloatTensor(X)\n\n        def dummy_loss(x):\n            return torch.mean(x ** 2)\n\n        poisoned_samples = {\n            'gradient_based': self.poisoning_generator.generate_gradient_based_poisoning(\n                X_tensor, dummy_loss\n            ).numpy(),\n            'label_flipping': self.poisoning_generator.generate_label_flipping_attacks(y),\n            'backdoor': self.poisoning_generator.generate_backdoor_triggers(\n                X, np.random.normal(0, 0.1, X.shape[1])\n            ),\n            'clean_label': self.poisoning_generator.generate_clean_label_poisoning(\n                X, np.random.normal(0, 0.1, X.shape[1])\n            )\n        }\n\n        return poisoned_samples\n\n\nclass DatasetStatistics:\n    \"\"\"Track and analyze dataset statistics\"\"\"\n    def __init__(self):\n        self.stats = defaultdict(list)\n\n    def update(self, batch_stats: Dict):\n        \"\"\"Update statistics with batch information\"\"\"\n        for k, v in batch_stats.items():\n            self.stats[k].append(v)\n\n    def get_summary(self) -> Dict:\n        \"\"\"Get summary statistics\"\"\"\n        summary = {}\n        for k, v in self.stats.items():\n            if isinstance(v[0], (int, float, np.number)):\n                summary[k] = {\n                    'mean': np.mean(v),\n                    'std': np.std(v),\n                    'min': np.min(v),\n                    'max': np.max(v)\n                }\n        return summary\n\nclass BatchGenerator:\n    \"\"\"Generate training batches with augmentation\"\"\"\n    def __init__(self, X: np.ndarray, y: np.ndarray, batch_size: int,\n                 shuffle: bool = True):\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.n_samples = len(X)\n        self.indices = np.arange(self.n_samples)\n\n    def __len__(self):\n        return int(np.ceil(self.n_samples / self.batch_size))\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n        for start_idx in range(0, self.n_samples, self.batch_size):\n            end_idx = min(start_idx + self.batch_size, self.n_samples)\n            batch_indices = self.indices[start_idx:end_idx]\n\n            yield (\n                self.X[batch_indices],\n                self.y[batch_indices]\n            )\n\n","metadata":{"id":"B8MUhliccWV7","executionInfo":{"status":"ok","timestamp":1732040987167,"user_tz":-180,"elapsed":11,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## neural network Component","metadata":{"id":"IYzwmqAftpWQ"}},{"cell_type":"code","source":"# Feature Extraction\nclass FeatureExtractor(nn.Module):\n    \"\"\"Enhanced feature extraction with attention\"\"\"\n    def __init__(self, input_dim: int, hidden_dims: List[int], dropout_rate: float = 0.2):\n        super().__init__()\n        self.input_dim = input_dim\n        layers = []\n        prev_dim = input_dim\n\n        for dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, dim),\n                nn.LayerNorm(dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = dim\n\n        self.feature_layers = nn.Sequential(*layers)\n        print(f\"Feature extractor created with input dim: {input_dim}\")\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        elif x.dim() > 2:\n            x = x.view(x.size(0), -1)\n\n        return self.feature_layers(x)\n\nclass DataStabilizer:\n    \"\"\"Handle numerical stability in data processing\"\"\"\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n\n    def stabilize_array(self, arr):\n        \"\"\"Stabilize numpy array by handling zeros and infinities\"\"\"\n        # Replace infinities with large finite numbers\n        arr = np.nan_to_num(arr, nan=0.0, posinf=1e10, neginf=-1e10)\n        return arr\n\n    def safe_divide(self, numerator, denominator):\n        \"\"\"Safe division avoiding divide by zero\"\"\"\n        return numerator / (denominator + self.eps)\n\n    def normalize_features(self, features):\n        \"\"\"Normalize features with numerical stability\"\"\"\n        mean = np.mean(features, axis=0)\n        std = np.std(features, axis=0) + self.eps\n        return (features - mean) / std\n\n    def stabilize_gradients(self, tensor):\n        \"\"\"Stabilize gradients for tensor operations\"\"\"\n        if torch.is_tensor(tensor):\n            return torch.clamp(tensor, min=-1e6, max=1e6)\n        return tensor\n\n\n\n# DQN Component\nclass DQNStream(nn.Module):\n    \"\"\"DQN stream with LSTM and attention\"\"\"\n    def __init__(self, feature_dim: int, hidden_dim: int, num_actions: int,\n                 num_heads: int = 4):\n        super().__init__()\n        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\n\n        self.value_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_actions)\n        )\n\n        self.advantage_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_actions)\n        )\n\n    def forward(self, features: torch.Tensor, hidden=None) -> Tuple[torch.Tensor, Tuple]:\n        # LSTM processing\n        lstm_out, hidden = self.lstm(features, hidden)\n\n        # Self-attention\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n\n        # Combine LSTM and attention\n        combined = lstm_out + attn_out\n        last_hidden = combined[:, -1, :]\n\n        # Dueling DQN architecture\n        values = self.value_head(last_hidden)\n        advantages = self.advantage_head(last_hidden)\n\n        # Combine value and advantage\n        q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))\n\n        return q_values, hidden\n\n# Actor-Critic Component\nclass ActorCriticStream(nn.Module):\n    \"\"\"Actor-Critic stream with shared features\"\"\"\n    def __init__(self, feature_dim: int, hidden_dim: int, num_actions: int):\n        super().__init__()\n\n        # Shared layers\n        self.shared_layer = nn.Sequential(\n            nn.Linear(feature_dim, hidden_dim),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim)\n        )\n\n        # Actor (policy) network\n        self.actor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_actions),\n            nn.Softmax(dim=-1)\n        )\n\n        # Critic (value) network\n        self.critic = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        shared_features = self.shared_layer(features)\n\n        # Get policy distribution and state value\n        action_probs = self.actor(shared_features)\n        state_value = self.critic(shared_features)\n\n        return action_probs, state_value\n\n# Combined Architecture\nclass DualStreamDetector(nn.Module):\n    \"\"\"Integrated DQN and Actor-Critic architecture\"\"\"\n    def __init__(self, input_dim: int, feature_dim: int, hidden_dim: int,\n                 num_actions: int, num_heads: int = 4):\n        super().__init__()\n\n        # Components\n        self.feature_extractor = FeatureExtractor(input_dim, [hidden_dim, feature_dim])\n        self.dqn_stream = DQNStream(feature_dim, hidden_dim, num_actions, num_heads)\n        self.ac_stream = ActorCriticStream(feature_dim, hidden_dim, num_actions)\n\n        # Integration layer\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(num_actions * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_actions)\n        )\n\n    def forward(self, x: torch.Tensor, hidden=None) -> Dict[str, torch.Tensor]:\n        # Extract features\n        features = self.feature_extractor(x)\n\n        # DQN stream\n        q_values, new_hidden = self.dqn_stream(features.unsqueeze(1), hidden)\n\n        # Actor-Critic stream\n        action_probs, state_value = self.ac_stream(features)\n\n        # Combine outputs\n        combined = torch.cat([q_values, action_probs], dim=-1)\n        final_output = self.fusion_layer(combined)\n\n        return {\n            'q_values': q_values,\n            'action_probs': action_probs,\n            'state_value': state_value,\n            'final_output': final_output,\n            'hidden': new_hidden\n        }\n\n","metadata":{"id":"G5pSRuhft2dJ","executionInfo":{"status":"ok","timestamp":1732040987167,"user_tz":-180,"elapsed":10,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Poisoning Detection System (Base and Enhanced)","metadata":{"id":"AZPmBeAkt5WG"}},{"cell_type":"code","source":"class DynamicThresholdManager:\n    \"\"\"Manages dynamic thresholds for poisoning detection\"\"\"\n    def __init__(self, initial_threshold=0.5, adaptation_rate=0.01):\n        self.threshold = initial_threshold\n        self.adaptation_rate = adaptation_rate\n        self.historical_predictions = deque(maxlen=1000)\n        self.confidence_history = deque(maxlen=1000)\n\n    def update_threshold(self, current_confidence: float, prediction_correct: bool):\n        self.historical_predictions.append(prediction_correct)\n        self.confidence_history.append(current_confidence)\n\n        recent_accuracy = np.mean(self.historical_predictions)\n        confidence_variance = np.std(self.confidence_history)\n\n        if recent_accuracy < 0.9:\n            self.threshold += self.adaptation_rate * (1 - recent_accuracy)\n        else:\n            self.threshold -= self.adaptation_rate * confidence_variance\n\n        self.threshold = np.clip(self.threshold, 0.3, 0.9)\n        return self.threshold\n\n## GRADUAL POISONING DETECTOR\n\ndef sigmoid(x):\n    \"\"\"Numpy implementation of sigmoid function\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nclass GradualPoisoningDetector:\n    def __init__(self, window_size=100):  # Add this\n        self.window_size = window_size\n        self.feature_history = deque(maxlen=window_size)\n        self.distribution_history = deque(maxlen=window_size)\n\n    def analyze_gradual_changes(self, current_features: np.ndarray) -> Dict[str, float]:\n        \"\"\"Analyze gradual changes with numerical stability\"\"\"\n        try:\n            self.feature_history.append(current_features)\n\n            if len(self.feature_history) < 2:\n                return {\n                    'gradual_poison_probability': 0.0,\n                    'change_consistency': 0.0,\n                    'change_trend': 0.0\n                }\n\n            # Calculate distribution with eps for numerical stability\n            eps = 1e-8\n            current_dist = np.histogram(current_features, bins=20)[0] + eps\n            self.distribution_history.append(current_dist)\n\n            if len(self.distribution_history) >= 2:\n                # Use stable calculation methods\n                distribution_changes = np.diff([dist for dist in self.distribution_history], axis=0)\n\n                # Add small epsilon to avoid division by zero\n                abs_changes = np.abs(distribution_changes) + eps\n                mean_change = np.mean(abs_changes)\n                std_change = np.std(abs_changes) + eps\n\n                # Calculate gradual score with safety checks\n                gradual_score = np.clip(\n                    mean_change * std_change * len(self.distribution_history),\n                    -100, 100\n                )\n\n                # Calculate trend safely\n                time_points = np.arange(len(distribution_changes))\n                if len(time_points) > 1:\n                    try:\n                        avg_changes = np.mean(distribution_changes, axis=1)\n                        trend = np.polyfit(time_points, avg_changes, 1)[0]\n                    except:\n                        trend = 0.0\n                else:\n                    trend = 0.0\n\n                return {\n                    'gradual_poison_probability': float(sigmoid(gradual_score)),\n                    'change_consistency': float(mean_change),\n                    'change_trend': float(trend)\n                }\n\n            return {\n                'gradual_poison_probability': 0.0,\n                'change_consistency': 0.0,\n                'change_trend': 0.0\n            }\n\n        except Exception as e:\n            print(f\"Warning: Error in gradual change analysis: {str(e)}\")\n            return {\n                'gradual_poison_probability': 0.0,\n                'change_consistency': 0.0,\n                'change_trend': 0.0\n            }\n\n\n    def reset(self):\n        \"\"\"Reset detector state\"\"\"\n        self.feature_history.clear()\n        self.distribution_history.clear()\n\n    def get_window_stats(self) -> Dict[str, float]:\n        \"\"\"Get statistics about the current detection window\"\"\"\n        return {\n            'window_size': len(self.feature_history),\n            'max_window': self.window_size,\n            'distribution_samples': len(self.distribution_history)\n        }\n\n\n## DATA AUGUMENTATION\n\nclass DataAugmentation:\n    \"\"\"Data augmentation techniques for poisoning detection\"\"\"\n    def __init__(self, noise_std=0.01, swap_prob=0.1):\n        self.noise_std = noise_std\n        self.swap_prob = swap_prob\n\n    def augment(self, data: torch.Tensor) -> torch.Tensor:\n        augmented = data.clone()\n\n        # Add Gaussian noise\n        if random.random() < self.swap_prob:\n            noise = torch.randn_like(augmented) * self.noise_std\n            augmented += noise\n\n        # Feature permutation\n        if random.random() < self.swap_prob:\n            idx = torch.randperm(augmented.size(1))\n            augmented = augmented[:, idx]\n\n        return augmented\n\n\n# Metrics Tracking: \n\nclass PoisoningDetectionMetrics:\n    def __init__(self, label_handler=None):\n        self.detection_history = []\n        self.distribution_stats = []\n        self.dqn_metrics = defaultdict(list)\n        self.ac_metrics = defaultdict(list)\n        self.label_handler = label_handler\n        self.metrics_dict = {}\n        self.last_update_time = time.time()\n        self.update_interval = 1.0\n        # Add these attributes\n        self.true_labels = None\n        self.predictions = None\n\n    def update_metrics(self, features, prediction, true_label, dqn_values=None, ac_probs=None):\n        # Store labels and predictions\n        if self.true_labels is None:\n            self.true_labels = []\n            self.predictions = []\n        self.true_labels.append(true_label)\n        self.predictions.append(prediction)\n\n        try:\n            # Store detection results with feature statistics\n            self.detection_history.append({\n                'prediction': prediction,\n                'true_label': true_label,\n                'feature_stats': {\n                    'mean': np.mean(features, axis=0),\n                    'std': np.std(features, axis=0),\n                    'kurtosis': scipy.stats.kurtosis(features, axis=0),\n                    'skewness': scipy.stats.skew(features, axis=0)\n                }\n            })\n\n            # Track DQN metrics\n            if dqn_values is not None:\n                self.dqn_metrics['q_values'].append(np.mean(dqn_values))\n                self.dqn_metrics['q_std'].append(np.std(dqn_values))\n\n            # Track Actor-Critic metrics\n            if ac_probs is not None:\n                self.ac_metrics['policy_entropy'].append(\n                    -np.sum(ac_probs * np.log(ac_probs + 1e-10))\n                )\n\n            # Periodic computation of metrics\n            current_time = time.time()\n            if current_time - self.last_update_time > self.update_interval:\n                self.compute_metrics()\n                self.last_update_time = current_time\n\n        except Exception as e:\n            print(f\"Error updating metrics: {str(e)}\")\n\n    def compute_metrics(self) -> Dict[str, float]:\n        \"\"\"Compute comprehensive metrics with error handling\"\"\"\n        try:\n            if not self.detection_history:\n                return {}\n\n            # Extract predictions and labels\n            predictions = [d['prediction'] for d in self.detection_history]\n            true_labels = [d['true_label'] for d in self.detection_history]\n\n            # Convert predictions to indices if needed\n            pred_indices = []\n            for pred in predictions:\n                if isinstance(pred, np.ndarray) and pred.ndim > 0:\n                    pred_indices.append(np.argmax(pred))\n                else:\n                    pred_indices.append(pred)\n\n            # Calculate basic metrics\n            metrics = {\n                'accuracy': np.mean([p == t for p, t in zip(pred_indices, true_labels)]),\n                'detection_confidence': np.mean([\n                    p.max() if isinstance(p, np.ndarray) and p.ndim > 0 else p\n                    for p in predictions\n                ]),\n                'false_positive_rate': self._compute_fpr(pred_indices, true_labels)\n            }\n\n            # Add DQN metrics\n            if self.dqn_metrics:\n                metrics.update({\n                    'avg_q_value': np.mean(self.dqn_metrics['q_values']),\n                    'q_value_std': np.mean(self.dqn_metrics['q_std'])\n                })\n\n            # Add Actor-Critic metrics\n            if self.ac_metrics:\n                metrics.update({\n                    'policy_entropy': np.mean(self.ac_metrics['policy_entropy'])\n                })\n\n            # Store computed metrics\n            self.metrics_dict = metrics\n            return metrics\n\n        except Exception as e:\n            print(f\"Error computing metrics: {str(e)}\")\n            return {\n                'accuracy': 0.0,\n                'detection_confidence': 0.0,\n                'false_positive_rate': 0.0\n            }\n\n    def _compute_fpr(self, predictions: List[float], true_labels: List[int]) -> float:\n        \"\"\"Compute False Positive Rate with error handling\"\"\"\n        try:\n            fp = sum(1 for p, t in zip(predictions, true_labels) if p == 1 and t == 0)\n            tn = sum(1 for p, t in zip(predictions, true_labels) if p == 0 and t == 0)\n            return fp / (fp + tn) if (fp + tn) > 0 else 0.0\n        except Exception as e:\n            print(f\"Error computing FPR: {str(e)}\")\n            return 0.0\n\n    def items(self):\n        \"\"\"Make metrics iterable\"\"\"\n        if not self.metrics_dict:\n            self.compute_metrics()\n        return self.metrics_dict.items()\n\n    def __getitem__(self, key):\n        \"\"\"Allow dictionary-like access\"\"\"\n        if not self.metrics_dict:\n            self.compute_metrics()\n        return self.metrics_dict[key]\n\n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive summary of all metrics\"\"\"\n        if not self.metrics_dict:\n            self.compute_metrics()\n        return {\n            'detection_metrics': self.metrics_dict,\n            'history_length': len(self.detection_history),\n            'distribution_stats': len(self.distribution_stats),\n            'dqn_metrics': dict(self.dqn_metrics),\n            'ac_metrics': dict(self.ac_metrics)\n        }\n\n    \n# Enhance Metrics Calculations\n\nclass EnhancedMetrics:\n    \"\"\"Advanced performance metrics calculation\"\"\"\n    def __init__(self):\n        self.metrics = defaultdict(dict)\n        \n    def calculate_metrics(self, y_true, y_pred, probabilities):\n        \"\"\"Calculate comprehensive set of performance metrics\"\"\"\n        metrics = {\n            'confusion_matrix': confusion_matrix(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred, average='weighted'),\n            'recall': recall_score(y_true, y_pred, average='weighted'),\n            'f1': f1_score(y_true, y_pred, average='weighted'),\n            'matthews_corr': matthews_corrcoef(y_true, y_pred),\n            'cohen_kappa': cohen_kappa_score(y_true, y_pred),\n            \n            # Detection and False Alarm Rates\n            'detection_rate': np.sum((y_pred == 1) & (y_true == 1)) / np.sum(y_true == 1),\n            'false_alarm_rate': np.sum((y_pred == 1) & (y_true == 0)) / np.sum(y_true == 0),\n            \n            # Additional error rates\n            'false_positive_rate': np.sum((y_pred == 1) & (y_true == 0)) / np.sum(y_true == 0),\n            'false_negative_rate': np.sum((y_pred == 0) & (y_true == 1)) / np.sum(y_true == 1),\n            \n            # Probability-based metrics\n            'auc_roc': roc_auc_score(y_true, probabilities[:, 1]),\n            'average_precision': average_precision_score(y_true, probabilities[:, 1]),\n            'brier_score': brier_score_loss(y_true, probabilities[:, 1])\n        }\n        \n        # Add threshold-based metrics\n        thresholds = np.arange(0.1, 1.0, 0.1)\n        for thresh in thresholds:\n            y_pred_thresh = (probabilities[:, 1] >= thresh).astype(int)\n            metrics[f'precision_at_{thresh:.1f}'] = precision_score(y_true, y_pred_thresh)\n            metrics[f'recall_at_{thresh:.1f}'] = recall_score(y_true, y_pred_thresh)\n            metrics[f'detection_rate_at_{thresh:.1f}'] = np.sum((y_pred_thresh == 1) & (y_true == 1)) / np.sum(y_true == 1)\n            metrics[f'false_alarm_rate_at_{thresh:.1f}'] = np.sum((y_pred_thresh == 1) & (y_true == 0)) / np.sum(y_true == 0)\n        \n        return metrics\n\n    def per_attack_metrics(self, y_true, y_pred, attack_mapping):\n        \"\"\"Calculate metrics for each attack type\"\"\"\n        metrics = {}\n        for attack_id, attack_name in attack_mapping.items():\n            mask = y_true == attack_id\n            if np.any(mask):\n                y_true_masked = y_true[mask]\n                y_pred_masked = y_pred[mask]\n                \n                metrics[attack_name] = {\n                    'precision': precision_score(y_true_masked, y_pred_masked, average='binary'),\n                    'recall': recall_score(y_true_masked, y_pred_masked, average='binary'),\n                    'f1': f1_score(y_true_masked, y_pred_masked, average='binary'),\n                    'detection_rate': np.sum((y_pred_masked == 1) & (y_true_masked == 1)) / np.sum(y_true_masked == 1),\n                    'false_alarm_rate': np.sum((y_pred_masked == 1) & (y_true_masked == 0)) / np.sum(y_true_masked == 0),\n                    'samples': int(np.sum(mask))\n                }\n        return metrics\n\n    def get_overall_summary(self):\n        \"\"\"Get summary of all computed metrics\"\"\"\n        summary = {}\n        for dataset, metrics in self.metrics.items():\n            summary[dataset] = {\n                'avg_detection_rate': np.mean([m['detection_rate'] for m in metrics.values()]),\n                'avg_false_alarm_rate': np.mean([m['false_alarm_rate'] for m in metrics.values()]),\n                'weighted_f1': np.mean([m['f1'] for m in metrics.values()]),\n                'total_samples': sum(m['samples'] for m in metrics.values())\n            }\n        return summary\n    \n    ","metadata":{"id":"QLjWna19uaz0","executionInfo":{"status":"ok","timestamp":1732040987167,"user_tz":-180,"elapsed":10,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Detection Syatem","metadata":{}},{"cell_type":"code","source":"# Base Detection System\n\nclass PoisoningDetectionSystem:\n    \"\"\"Base class for poisoning detection\"\"\"\n    def __init__(self, input_dim: int, config: ModelConfig = None):\n        if config is None:\n            config = ModelConfig()\n\n        self.config = config\n        self.device = config.device\n\n        # Initialize model\n        self.model = DualStreamDetector(\n            input_dim=input_dim,\n            feature_dim=config.feature_dim,\n            hidden_dim=config.hidden_dim,\n            num_actions=config.num_actions\n        ).to(self.device)\n\n        # Optimizer and AMP scaler\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=config.learning_rate\n        )\n        self.scaler = torch.cuda.amp.GradScaler() if config.use_amp and self.device.type == \"cuda\" else None\n\n        # Experience replay\n        self.replay_buffer = deque(maxlen=config.replay_buffer_size)\n        self.sequence_length = config.sequence_length\n\n        # Metrics\n        self.training_metrics = defaultdict(list)\n\n    def preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        \"\"\"Preprocess state for model input\"\"\"\n        try:\n            if isinstance(state, np.ndarray):\n                if state.ndim == 1:\n                    state = state.reshape(1, -1)\n                state_tensor = torch.from_numpy(state).float()\n            else:\n                state_tensor = state.float()\n                if state_tensor.dim() == 1:\n                    state_tensor = state_tensor.unsqueeze(0)\n\n            return state_tensor.to(self.device)\n\n        except Exception as e:\n            print(f\"Error in preprocess_state: {str(e)}\")\n            raise\n\n    def detect(self, state: np.ndarray, evaluate: bool = False) -> Dict[str, np.ndarray]:\n        \"\"\"Detect poisoning attacks\"\"\"\n        with torch.no_grad() if evaluate else torch.enable_grad():\n            processed_state = self.preprocess_state(state)\n            model_output = self.model(processed_state)\n\n            # Get predictions\n            q_values = model_output['q_values']\n            action_probs = model_output['action_probs']\n            detection_prob = F.softmax(model_output['final_output'], dim=-1)\n\n            # Convert to numpy\n            return {\n                'is_poisoning': detection_prob.cpu().numpy(),\n                'q_values': q_values.cpu().numpy(),\n                'action_probs': action_probs.cpu().numpy(),\n                'confidence': model_output['state_value'].cpu().numpy()\n            }\n\nclass PoisoningLoss(nn.Module):\n    def __init__(self, dqn_weight=0.4, policy_weight=0.3, value_weight=0.3, eps=1e-8):\n        super().__init__()\n        self.dqn_weight = dqn_weight\n        self.policy_weight = policy_weight\n        self.value_weight = value_weight\n        self.eps = eps\n\n        self.dqn_criterion = nn.SmoothL1Loss()\n        self.policy_criterion = nn.CrossEntropyLoss()\n        self.value_criterion = nn.MSELoss()\n\n    def forward(self, model_output, targets):\n        # Add numerical stability to outputs\n        q_values = model_output['q_values'].clamp(min=-100, max=100)\n        action_probs = F.softmax(model_output['action_probs'], dim=-1)\n        action_probs = torch.clamp(action_probs, min=self.eps, max=1.0)\n\n        # DQN loss with gradient scaling\n        dqn_loss = self.dqn_criterion(q_values, targets['q_targets'])\n        dqn_loss = torch.where(torch.isfinite(dqn_loss), dqn_loss, torch.zeros_like(dqn_loss))\n\n        # Policy loss with stable log\n        policy_loss = self.policy_criterion(\n            action_probs,\n            targets['actions']\n        )\n\n        # Value loss with bounded predictions\n        value_pred = model_output['state_value'].view(-1).clamp(min=-100, max=100)\n        value_target = targets['returns'].view(-1).clamp(min=-100, max=100)\n        value_loss = self.value_criterion(value_pred, value_target)\n\n        # Detection loss with stable probabilities\n        detection_probs = F.softmax(model_output['final_output'], dim=-1)\n        detection_probs = torch.clamp(detection_probs, min=self.eps, max=1.0)\n        detection_loss = self.policy_criterion(\n            detection_probs,\n            targets['labels']\n        )\n\n        # Combine losses with stability checks\n        total_loss = (\n            self.dqn_weight * torch.nan_to_num(dqn_loss) +\n            self.policy_weight * torch.nan_to_num(policy_loss + detection_loss) +\n            self.value_weight * torch.nan_to_num(value_loss)\n        )\n\n        return {\n            'total_loss': total_loss,\n            'dqn_loss': dqn_loss.item(),\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'detection_loss': detection_loss.item()\n        }\n\n\n\nclass PoisoningDataAugmentation:\n    \"\"\"Advanced data augmentation techniques specifically for poisoning detection\"\"\"\n    def __init__(self,\n                 noise_std=0.01,\n                 feature_swap_prob=0.1,\n                 feature_scale_range=(0.95, 1.05),\n                 temporal_shift_prob=0.1,\n                 max_shift=3):\n        self.noise_std = noise_std\n        self.feature_swap_prob = feature_swap_prob\n        self.feature_scale_range = feature_scale_range\n        self.temporal_shift_prob = temporal_shift_prob\n        self.max_shift = max_shift\n\n    def augment(self, data: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply various augmentation techniques to the input data\n\n        Args:\n            data: Input tensor of shape (batch_size, feature_dim) or (batch_size, sequence_length, feature_dim)\n\n        Returns:\n            Augmented tensor of the same shape\n        \"\"\"\n        augmented = data.clone()\n\n        # Add Gaussian noise\n        if random.random() < self.feature_swap_prob:\n            noise = torch.randn_like(augmented) * self.noise_std\n            augmented += noise\n\n        # Random feature scaling\n        if random.random() < self.feature_swap_prob:\n            scale_factors = torch.FloatTensor(augmented.shape[-1]).uniform_(*self.feature_scale_range)\n            if augmented.dim() == 3:  # Sequential data\n                scale_factors = scale_factors.unsqueeze(0).unsqueeze(0)\n            else:  # Single timestep data\n                scale_factors = scale_factors.unsqueeze(0)\n            augmented *= scale_factors.to(augmented.device)\n\n        # Feature permutation\n        if random.random() < self.feature_swap_prob:\n            feat_idx = torch.randperm(augmented.shape[-1])\n            if augmented.dim() == 3:\n                augmented = augmented[:, :, feat_idx]\n            else:\n                augmented = augmented[:, feat_idx]\n\n        # Temporal shift for sequential data\n        if augmented.dim() == 3 and random.random() < self.temporal_shift_prob:\n            shift = random.randint(-self.max_shift, self.max_shift)\n            augmented = torch.roll(augmented, shifts=shift, dims=1)\n\n        # Ensure values stay within reasonable bounds\n        augmented = torch.clamp(augmented, min=-10, max=10)\n\n        return augmented\n\n    def augment_batch(self, data: torch.Tensor, labels: torch.Tensor = None) -> tuple:\n        \"\"\"\n        Augment a batch of data with optional label preservation\n\n        Args:\n            data: Input tensor\n            labels: Optional label tensor\n\n        Returns:\n            Tuple of (augmented_data, labels)\n        \"\"\"\n        augmented_data = self.augment(data)\n\n        if labels is not None:\n            return augmented_data, labels\n        return augmented_data\n\n    @staticmethod\n    def mix_samples(data: torch.Tensor, labels: torch.Tensor, alpha: float = 0.2) -> tuple:\n        \"\"\"\n        Implement mixup augmentation for robust learning\n\n        Args:\n            data: Input tensor\n            labels: Label tensor\n            alpha: Mixup interpolation strength\n\n        Returns:\n            Tuple of (mixed_data, mixed_labels)\n        \"\"\"\n        if alpha > 0:\n            lam = np.random.beta(alpha, alpha)\n        else:\n            lam = 1\n\n        batch_size = data.size(0)\n        index = torch.randperm(batch_size).to(data.device)\n\n        mixed_data = lam * data + (1 - lam) * data[index]\n        mixed_labels = lam * labels + (1 - lam) * labels[index]\n\n        return mixed_data, mixed_labels\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Enhanced Detection System","metadata":{}},{"cell_type":"code","source":"# ENHANCED DETECTION SYSTEM\n\nclass EnhancedPoisoningDetectionSystem(PoisoningDetectionSystem):\n    def __init__(self, input_dim: int, config: ModelConfig = None, label_handler: LabelHandler = None):\n        super().__init__(input_dim, config)\n\nclass EnhancedPoisoningDetectionSystem(PoisoningDetectionSystem):\n    def __init__(self, input_dim: int, config: ModelConfig = None, label_handler: LabelHandler = None):\n        super().__init__(input_dim, config)\n        \n        self.device = config.device if config else DeviceManager.initialize_device()[0]\n        self.model = self.model.to(self.device)\n        self.label_handler = label_handler\n\n        # Initialize components\n        self.label_handler = label_handler\n        self.metrics_tracker = PoisoningDetectionMetrics(label_handler)\n        self.threshold_manager = DynamicThresholdManager()\n        self.gradual_detector = GradualPoisoningDetector()\n        self.pattern_memory = deque(maxlen=1000)\n\n        # Add missing components that caused errors\n        self.data_stabilizer = DataStabilizer()\n        self.criterion = PoisoningLoss()  # Initialize loss function\n        self.data_augmentation = PoisoningDataAugmentation()  # Initialize data augmentation\n\n        # Modify optimizer\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=1e-4,\n            eps=1e-8  # Increased epsilon for optimizer stability\n        )\n\n        # Print initialization info\n        print(\"\\nEnhanced Detection System Initialized:\")\n        print(f\"- Input dimension: {input_dim}\")\n        print(f\"- Number of attack types: {len(label_handler.attack_types) if label_handler else 'N/A'}\")\n        print(f\"- Binary classification: Normal vs Attack\")\n        print(f\"- Using label handler: {label_handler is not None}\")\n        print(f\"- Device: {self.device}\")\n\n        if self.device.type == \"cuda\":\n            print(f\"- GPU Memory Usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n\n\n\n    # Add this method to EnhancedPoisoningDetectionSystem\n    def preprocess_batch(self, states, binary_labels, multi_labels):\n        \"\"\"Preprocess batch data with stability checks\"\"\"\n        # Stabilize states\n        states = self.data_stabilizer.stabilize_array(states)\n\n        # Convert to tensors with proper dtype\n        states_tensor = torch.FloatTensor(states).to(self.device)\n        binary_labels_tensor = torch.LongTensor(binary_labels).to(self.device)\n\n        # Gradient scaling for large values\n        if states_tensor.abs().max() > 1e3:\n            states_tensor = F.normalize(states_tensor, dim=1)\n\n        return states_tensor, binary_labels_tensor\n\n        # Loss function\n        self.criterion = PoisoningLoss()\n\n        # Data augmentation\n        self.data_augmentation = PoisoningDataAugmentation()\n\n        # Print initialization info\n        print(\"\\nEnhanced Detection System Initialized:\")\n        print(f\"- Input dimension: {input_dim}\")\n        print(f\"- Number of attack types: {len(label_handler.attack_types) if label_handler else 'N/A'}\")\n        print(f\"- Binary classification: Normal vs Attack\")\n        print(f\"- Using label handler: {label_handler is not None}\")\n        print(f\"- Device: {self.device}\")\n\n        if self.device.type == \"cuda\":\n            print(f\"- GPU Memory Usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n\n\n    def _check_label_handler(self):\n        \"\"\"Verify label handler is available when needed\"\"\"\n        if self.label_handler is None:\n            raise ValueError(\"Label handler is required for multi-class attack analysis\")\n        return True\n\n    def get_attack_info(self, attack_id: int) -> Dict:\n        \"\"\"Get information about a specific attack type\"\"\"\n        self._check_label_handler()\n        return self.label_handler.get_attack_info(attack_id)\n\n\n    def detect(self, state: np.ndarray, evaluate: bool = False,\n              labels: np.ndarray = None) -> Dict[str, np.ndarray]:\n        \"\"\"Enhanced detection with multiple analysis streams\"\"\"\n        with torch.no_grad() if evaluate else torch.enable_grad():\n\n            with torch.amp.autocast('cuda') if self.scaler else contextlib.nullcontext():\n                # Process state\n                processed_state = self.preprocess_state(state)\n                if not evaluate and random.random() < 0.3:\n                    processed_state = self.data_augmentation.augment(processed_state)\n\n                # Model forward pass\n                model_output = self.model(processed_state)\n                combined_output = model_output['final_output']\n                detection_prob = F.softmax(combined_output, dim=-1)\n\n                # Get numpy state for gradual analysis\n                numpy_state = state.cpu().numpy() if torch.is_tensor(state) else state\n\n                try:\n                    # Analyze gradual changes with error handling\n                    gradual_analysis = self.gradual_detector.analyze_gradual_changes(numpy_state)\n                except Exception as e:\n                    print(f\"Warning: Error in gradual analysis: {str(e)}\")\n                    gradual_analysis = {\n                        'gradual_poison_probability': 0.0,\n                        'change_consistency': 0.0,\n                        'change_trend': 0.0\n                    }\n\n                # Update threshold\n                current_threshold = self.threshold_manager.update_threshold(\n                    detection_prob.detach().mean().item(),\n                    detection_prob.detach().argmax().item() == 1\n                )\n\n                # Combine detections\n                enhanced_prob = (\n                    detection_prob.detach().cpu().numpy() * self.config.dqn_weight +\n                    gradual_analysis['gradual_poison_probability'] * self.config.ac_weight\n                )\n\n                # Update pattern memory\n                self.pattern_memory.append({\n                    'features': state.detach().cpu().numpy() if torch.is_tensor(state) else state,\n                    'basic_detection': detection_prob.detach().cpu().numpy(),\n                    'gradual_score': gradual_analysis['gradual_poison_probability']\n                })\n\n                sequence_analysis = self._analyze_sequential_patterns()\n\n                return {\n                    'is_poisoning': enhanced_prob,\n                    'q_values': model_output['q_values'].detach().cpu().numpy(),\n                    'action_probs': model_output['action_probs'].detach().cpu().numpy(),\n                    'confidence': model_output['state_value'].detach().cpu().numpy(),\n                    'gradual_metrics': gradual_analysis,\n                    'sequence_metrics': sequence_analysis,\n                    'threshold': current_threshold,\n                    'detection_metrics': self.metrics_tracker.compute_metrics()\n                }\n\n    def _analyze_sequential_patterns(self) -> Dict[str, float]:\n        \"\"\"Analyze temporal patterns in detection history\"\"\"\n        try:\n            if len(self.pattern_memory) < 2:\n                return {\n                    'sequence_score': 0.0,\n                    'pattern_consistency': 0.0,\n                    'temporal_correlation': 0.0\n                }\n\n            # Get recent patterns and ensure they're the same shape\n            recent_patterns = list(self.pattern_memory)[-10:]\n            features_list = []\n\n            # Handle variable-sized features\n            base_shape = None\n            for pattern in recent_patterns:\n                features = pattern['features']\n                if isinstance(features, torch.Tensor):\n                    features = features.cpu().numpy()\n\n                # If this is first valid shape, use it as base\n                if base_shape is None and features is not None:\n                    base_shape = features.shape\n\n                # Only include features matching base shape\n                if base_shape is not None and features is not None and features.shape == base_shape:\n                    features_list.append(features)\n\n            # If we don't have enough valid patterns, return default values\n            if len(features_list) < 2:\n                return {\n                    'sequence_score': 0.0,\n                    'pattern_consistency': 0.0,\n                    'temporal_correlation': 0.0\n                }\n\n            # Calculate metrics only on valid patterns\n            try:\n                # Convert to numpy array and calculate differences\n                features_array = np.stack(features_list)\n                feature_evolution = np.diff(features_array, axis=0)\n\n                # Calculate pattern metrics\n                pattern_consistency = np.mean(np.abs(feature_evolution), axis=0)\n\n                # Get detection scores\n                detection_scores = [\n                    float(np.mean(p['basic_detection']))\n                    for p in recent_patterns[-len(features_list):]\n                ]\n\n                gradual_scores = [\n                    float(p['gradual_score'])\n                    for p in recent_patterns[-len(features_list):]\n                ]\n\n                # Calculate correlation if we have enough samples\n                if len(detection_scores) > 1:\n                    temporal_correlation = np.corrcoef(\n                        detection_scores,\n                        gradual_scores\n                    )[0, 1]\n                    if np.isnan(temporal_correlation):\n                        temporal_correlation = 0.0\n                else:\n                    temporal_correlation = 0.0\n\n                sequence_score = float(sigmoid(temporal_correlation * np.mean(pattern_consistency)))\n\n                return {\n                    'sequence_score': sequence_score,\n                    'pattern_consistency': float(np.mean(pattern_consistency)),\n                    'temporal_correlation': float(temporal_correlation)\n                }\n\n            except Exception as e:\n                print(f\"Warning: Error in sequence analysis calculations: {str(e)}\")\n                return {\n                    'sequence_score': 0.0,\n                    'pattern_consistency': 0.0,\n                    'temporal_correlation': 0.0\n                }\n\n        except Exception as e:\n            print(f\"Warning: Error in sequence pattern analysis: {str(e)}\")\n            return {\n                'sequence_score': 0.0,\n                'pattern_consistency': 0.0,\n                'temporal_correlation': 0.0\n            }\n\n\n    def train(self, batch_size: int) -> Dict[str, float]:\n        \"\"\"Train the model using experiences from the replay buffer\"\"\"\n        try:\n            if len(self.replay_buffer) < batch_size:\n                return {'status': 'insufficient_samples'}\n\n            # Sample and prepare batch\n            indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)\n            batch = [self.replay_buffer[i] for i in indices]\n\n            # Unpack and move to device\n            states, binary_labels, multi_labels, predictions, next_states, dones = zip(*batch)\n\n            states = torch.FloatTensor(np.array(states)).to(self.device)\n            binary_labels = torch.LongTensor(np.array(binary_labels)).to(self.device)\n            next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n            dones = torch.FloatTensor(np.array(dones)).to(self.device)\n\n\n            # Validate tensor shapes\n            if states.dim() != 2 or next_states.dim() != 2:\n                raise ValueError(f\"Invalid state tensor dimensions: states={states.shape}, next_states={next_states.shape}\")\n            if binary_labels.dim() != 1:\n                raise ValueError(f\"Invalid labels tensor dimension: {binary_labels.shape}\")\n\n        except Exception as e:\n            print(f\"Error converting to tensors: {str(e)}\")\n            print(f\"States shape: {np.array(states).shape if isinstance(states, (list, np.ndarray)) else 'invalid'}\")\n            print(f\"Labels shape: {np.array(binary_labels).shape if isinstance(binary_labels, (list, np.ndarray)) else 'invalid'}\")\n            return {'error': 'tensor_conversion', 'details': str(e)}\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Compute outputs and loss\n        try:\n          with torch.amp.autocast('cuda') if self.scaler else contextlib.nullcontext():\n            # Forward pass\n              outputs = self.model(states)\n              next_outputs = self.model(next_states)\n\n                    # Prepare targets for DQN\n              next_q_values = next_outputs['q_values'].detach()\n              q_targets = outputs['q_values'].clone().detach()\n              for i in range(batch_size):\n                  if not dones[i]:\n                     q_targets[i, binary_labels[i]] = self.config.gamma * next_q_values[i].max()\n\n                    # Prepare targets for actor-critic\n              returns = torch.zeros_like(outputs['state_value'])\n              for i in range(batch_size):\n                  returns[i] = outputs['state_value'][i] + \\\n                              (1 - dones[i]) * self.config.gamma * next_outputs['state_value'][i].detach()\n\n                    # Compute loss\n              targets = {\n                  'q_targets': q_targets,\n                  'actions': binary_labels,\n                  'returns': returns,\n                  'labels': binary_labels\n              }\n\n              loss_dict = self.criterion(outputs, targets)\n              total_loss = loss_dict['total_loss']\n\n                    # Check for invalid loss values\n              if torch.isnan(total_loss) or torch.isinf(total_loss):\n                raise ValueError(f\"Invalid loss value: {total_loss.item()}\")\n\n        except Exception as e:\n            print(f\"Error in forward pass or loss computation: {str(e)}\")\n            print(f\"Model outputs shape: {outputs['q_values'].shape if 'q_values' in outputs else 'invalid'}\")\n            return {'error': 'forward_pass', 'details': str(e)}\n\n              # Backward pass with AMP if available\n        try:\n            if self.scaler:\n              self.scaler.scale(total_loss).backward()\n              self.scaler.step(self.optimizer)\n              self.scaler.update()\n            else:\n                total_loss.backward()\n                      # Clip gradients\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n                self.optimizer.step()\n        except Exception as e:\n                  print(f\"Error in backward pass or optimization: {str(e)}\")\n                  return {'error': 'backward_pass', 'details': str(e)}\n\n              # Return metrics\n        try:\n            metrics = {\n                 'status': 'success',\n                 'total_loss': total_loss.item(),\n                 'dqn_loss': loss_dict['dqn_loss'],\n                 'policy_loss': loss_dict['policy_loss'],\n                 'value_loss': loss_dict['value_loss'],\n                 'detection_loss': loss_dict['detection_loss']\n            }\n            return metrics\n\n        except Exception as e:\n            print(f\"Error computing metrics: {str(e)}\")\n            return {'error': 'metrics_computation', 'details': str(e)}\n\n        except Exception as e:\n            print(f\"Unexpected error during training step: {str(e)}\")\n            traceback.print_exc()  # Print full traceback for debugging\n            return {\n                  'error': 'unexpected',\n                  'details': str(e),\n                  'traceback': traceback.format_exc()\n              }\n\n\n    def _compute_td_error(self, state_values, next_state_values, rewards, dones):\n        \"\"\"Compute TD error for value function updates\"\"\"\n        target_values = rewards + (1 - dones) * self.config.gamma * next_state_values\n        td_error = target_values - state_values\n        return td_error\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi - Dataset Generator","metadata":{"id":"GXSc9hVk1UzJ"}},{"cell_type":"code","source":"class MultiDatasetGenerator(Sequence):\n    \"\"\"Generate batches from multiple datasets with iteration control\"\"\"\n    def __init__(self, datasets: Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]],\n                 batch_size=256, shuffle=True, num_workers=0):\n        # Initialize dataset parameters\n        self.datasets = datasets\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.num_workers = num_workers\n\n        # Initialize indices for each dataset\n        self.indices = {k: np.arange(len(v[0])) for k, v in datasets.items()}\n\n        # Calculate weights for dataset sampling\n        total_samples = sum(len(v[0]) for v in datasets.values())\n        self.dataset_weights = {k: len(v[0])/total_samples for k, v in datasets.items()}\n\n        # Initialize iteration control\n        self.current_epoch_iterations = 0\n        self.max_epoch_iterations = 1000  # Maximum iterations per epoch\n\n        print(f\"Initialized MultiDatasetGenerator with {len(datasets)} datasets\")\n        for k, v in datasets.items():\n            print(f\"- {k}: {len(v[0])} samples\")\n\n    def __iter__(self):\n        \"\"\"Reset iteration state and shuffle if needed\"\"\"\n        self.current_epoch_iterations = 0\n        if self.shuffle:\n            for k in self.indices:\n                np.random.shuffle(self.indices[k])\n        return self\n\n    def __next__(self):\n        \"\"\"Get next batch with iteration limit check\"\"\"\n        if self.current_epoch_iterations >= self.max_epoch_iterations:\n            raise StopIteration\n\n        self.current_epoch_iterations += 1\n\n        # Select dataset and get batch\n        chosen_dataset = np.random.choice(\n            list(self.datasets.keys()),\n            p=list(self.dataset_weights.values())\n        )\n\n        X, binary_labels, multi_labels = self.datasets[chosen_dataset]\n        indices = self.indices[chosen_dataset]\n\n        # Calculate batch indices\n        start_idx = (self.current_epoch_iterations * self.batch_size) % len(indices)\n        batch_indices = indices[start_idx:start_idx + self.batch_size]\n\n        # Return batch data\n        return (\n            X[batch_indices],\n            binary_labels[batch_indices],\n            multi_labels[batch_indices]\n        )\n\n    def __getitem__(self, index):\n        \"\"\"Get specific batch by index\"\"\"\n        # Ensure index is within bounds\n        if index >= self.max_epoch_iterations:\n            raise IndexError(\"Batch index out of range\")\n\n        chosen_dataset = np.random.choice(\n            list(self.datasets.keys()),\n            p=list(self.dataset_weights.values())\n        )\n\n        X, binary_labels, multi_labels = self.datasets[chosen_dataset]\n        indices = self.indices[chosen_dataset]\n\n        start_idx = (index * self.batch_size) % len(indices)\n        batch_indices = indices[start_idx:start_idx + self.batch_size]\n\n        return (\n            X[batch_indices],\n            binary_labels[batch_indices],\n            multi_labels[batch_indices]\n        )\n\n    def __len__(self):\n        \"\"\"Returns precise number of batches per epoch\"\"\"\n        return min(\n            self.max_epoch_iterations,\n            int(np.ceil(sum(len(v[0]) for v in self.datasets.values()) / self.batch_size))\n        )\n\n    def reset(self):\n        \"\"\"Reset iteration state\"\"\"\n        self.current_epoch_iterations = 0\n        if self.shuffle:\n            for k in self.indices:\n                np.random.shuffle(self.indices[k])\n\n    def get_progress(self):\n        \"\"\"Get training progress information\"\"\"\n        return {\n            'current_iteration': self.current_epoch_iterations,\n            'max_iterations': self.max_epoch_iterations,\n            'progress': self.current_epoch_iterations / self.max_epoch_iterations\n        }\n","metadata":{"id":"-lbzUvsC1X7D","executionInfo":{"status":"ok","timestamp":1732040987168,"user_tz":-180,"elapsed":10,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Pipeline","metadata":{"id":"AUq4M5tFuhrE"}},{"cell_type":"code","source":"## EARLY STOPPING\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        self.min_improvement = 1e-4\n        self.max_epochs_without_improvement = 20  # Increased from 10\n        self.epochs_without_improvement = 0\n\n        \n    def __call__(self, val_score, model):\n        if self.best_score is None:\n            self.best_score = val_score\n            self.best_weights = model.state_dict()\n        elif val_score < self.best_score + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        else:\n            self.best_score = val_score\n            self.best_weights = model.state_dict()\n            self.counter = 0\n        return False\n    \n    def _get_model_weights(self, model) -> dict:\n        \"\"\"Get a deep copy of model weights\"\"\"\n        return {\n            name: param.cpu().clone().detach()\n            for name, param in model.state_dict().items()\n        }\n\n    def restore_weights(self, model) -> None:\n        \"\"\"Restore model to best weights\"\"\"\n        if self.restore_best_weights and self.best_weights is not None:\n            model.load_state_dict(self.best_weights)\n\n    def reset(self) -> None:\n        \"\"\"Reset early stopping state\"\"\"\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        self.best_weights = None\n\n    def get_best_score(self) -> float:\n        \"\"\"Return the best score achieved\"\"\"\n        return self.best_score if self.best_score is not None else float('-inf')\n\n    def is_best_epoch(self, val_score: float) -> bool:\n        \"\"\"Check if current epoch achieved best score\"\"\"\n        return self.best_score is None or val_score > self.best_score + self.min_delta\n\n","metadata":{"id":"yfawHal4vC10","executionInfo":{"status":"ok","timestamp":1732040987168,"user_tz":-180,"elapsed":9,"user":{"displayName":"Roger Nick Anaedevha","userId":"11403452854921994388"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single DataSet Trainer","metadata":{}},{"cell_type":"code","source":"## SINGLE DATASET TRAINER\n\nclass SingleDatasetTrainer:\n    \"\"\"Enhanced trainer with comprehensive poisoning detection capabilities\"\"\"\n    def __init__(self, config: ModelConfig, dataset_type: str):\n        self.config = config\n        self.dataset_type = dataset_type.lower()\n        \n        # Initialize device with TPU support\n        self.device = config.device\n        print(f\"\\nInitializing trainer with device: {self.device}\")\n\n        # Initialize components with enhanced error handling\n        print(f\"\\nInitializing loader for {self.dataset_type.upper()}\")\n        self.loader = EnhancedDatasetLoader(\n            dataset_type=self.dataset_type, \n            config=self.config\n        )\n        self.memory_monitor = MemoryMonitor()\n        self.metrics_tracker = PoisoningDetectionMetrics(\n            label_handler=self.loader.label_handler\n        )\n\n        # Training setup with proper directory handling\n        self.writer = SummaryWriter(f'logs/{self.dataset_type}')\n        self.checkpoint_dir = os.path.join(config.checkpoint_dir, self.dataset_type)\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        \n        # Initialize poisoning-specific components\n        self.poisoning_stats = defaultdict(list)\n        self.detection_history = []\n\n    def train_on_dataset(self, file_path: str):\n        \"\"\"Complete training process with enhanced poisoning detection\"\"\"\n        try:\n            print(f\"\\nStarting training process for {self.dataset_type.upper()}\")\n\n            # Verify file exists and has correct format\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n            \n            if not file_path.endswith('.csv'):\n                print(\"Warning: Expected CSV file format\")\n\n            # Load and process dataset with poisoning features\n            X, binary_labels, multi_labels, additional_info = self.loader.load_and_process_dataset(file_path)\n\n            # Log dataset statistics\n            print(\"\\nDataset Statistics:\")\n            print(f\"- Total samples: {len(X)}\")\n            print(f\"- Feature dimensions: {X.shape[1]}\")\n            print(f\"- Poisoning features extracted: {list(additional_info['poisoning_features'].keys())}\")\n\n            print(\"\\nSplitting data into train and validation sets...\")\n            try:\n                # Ensure array types with validation\n                X = np.array(X, dtype=np.float32)\n                binary_labels = np.array(binary_labels, dtype=np.int32)\n                \n                # Validate data before splitting\n                if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n                    print(\"Warning: Data contains NaN or Inf values, cleaning...\")\n                    X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n\n                # Stratified split with poisoning preservation\n                X_train, X_val, y_train, y_val = train_test_split(\n                    X, binary_labels,\n                    test_size=0.2,\n                    random_state=42,\n                    stratify=binary_labels\n                )\n                \n                # Verify split maintains poisoning distribution\n                train_poison_ratio = np.mean(y_train)\n                val_poison_ratio = np.mean(y_val)\n                print(f\"Train set poisoning ratio: {train_poison_ratio:.3f}\")\n                print(f\"Validation set poisoning ratio: {val_poison_ratio:.3f}\")\n                \n                print(f\"Train set size: {len(X_train)}, Validation set size: {len(X_val)}\")\n\n            except Exception as e:\n                print(f\"Error in data splitting: {str(e)}\")\n                raise\n\n            # Initialize detection system with poisoning capabilities\n            detection_system = EnhancedPoisoningDetectionSystem(\n                input_dim=X.shape[1],\n                config=self.config,\n                label_handler=self.loader.label_handler\n            )\n            print(\"\\nInitialized detection system\")\n\n            # Create data generators with poisoning awareness\n            train_generator = MultiDatasetGenerator(\n                {self.dataset_type: (X_train, y_train, y_train)},\n                batch_size=self.config.batch_size,\n                shuffle=True  # Important for poisoning detection\n            )\n\n            val_generator = MultiDatasetGenerator(\n                {self.dataset_type: (X_val, y_val, y_val)},\n                batch_size=self.config.batch_size,\n                shuffle=False  # Keep validation set ordered\n            )\n            print(\"Created data generators\")\n\n            # Setup comprehensive training pipeline\n            pipeline = ComprehensiveTrainingPipeline(\n                detection_system=detection_system,\n                data_generator=train_generator,\n                val_generator=val_generator,\n                config=self.config,\n                label_handler=self.loader.label_handler\n            )\n\n            # Train model with poisoning detection\n            print(f\"\\nStarting training on {self.dataset_type.upper()}...\")\n            training_success = pipeline.train()\n\n            if not training_success:\n                print(\"Warning: Training completed with some errors\")\n\n            # Save results with enhanced poisoning information\n            self._save_results(detection_system, additional_info)\n\n            # Compute final poisoning statistics\n            final_stats = self._compute_poisoning_stats(detection_system, X_val, y_val)\n            print(\"\\nFinal Poisoning Detection Statistics:\")\n            for metric, value in final_stats.items():\n                print(f\"- {metric}: {value:.4f}\")\n\n            return detection_system, self.metrics_tracker\n\n        except Exception as e:\n            print(f\"Error in training process: {str(e)}\")\n            traceback.print_exc()\n            raise\n\n    def _save_results(self, detection_system: EnhancedPoisoningDetectionSystem, additional_info: Dict):\n        \"\"\"Save results with comprehensive poisoning information\"\"\"\n        try:\n            results_path = os.path.join(self.checkpoint_dir, f'{self.dataset_type}_results.pt')\n            \n            # Ensure directory exists\n            os.makedirs(os.path.dirname(results_path), exist_ok=True)\n\n            # Convert metrics to serializable format\n            metrics_dict = self.metrics_tracker.compute_metrics()\n            \n            # Prepare poisoning features\n            poisoning_info = {\n                'features': additional_info['poisoning_features'],\n                'detection_stats': self.poisoning_stats,\n                'validation_metrics': additional_info['validation_stats']\n            }\n\n            # Save everything with proper error handling\n            save_dict = {\n                'model_state': detection_system.model.state_dict(),\n                'config': {k: v for k, v in self.config.__dict__.items() \n                          if not k.startswith('_')},\n                'metrics': metrics_dict,\n                'poisoning_info': poisoning_info\n            }\n\n            # Convert numpy arrays to lists for serialization\n            save_dict = self._prepare_for_saving(save_dict)\n\n            try:\n                torch.save(save_dict, results_path)\n                print(f\"\\nResults saved to {results_path}\")\n            except Exception as e:\n                # Fallback to NPZ format if torch.save fails\n                np_path = results_path + '.npz'\n                np.savez(np_path, **save_dict)\n                print(f\"\\nResults saved as NPZ to {np_path}\")\n\n        except Exception as e:\n            print(f\"Error saving results: {str(e)}\")\n            raise\n\n    def _prepare_for_saving(self, data: Dict) -> Dict:\n        \"\"\"Prepare data for saving by converting numpy arrays to lists\"\"\"\n        if isinstance(data, np.ndarray):\n            return data.tolist()\n        elif isinstance(data, dict):\n            return {k: self._prepare_for_saving(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [self._prepare_for_saving(item) for item in data]\n        return data\n\n    def _compute_poisoning_stats(self, detection_system, X_val, y_val) -> Dict[str, float]:\n        \"\"\"Compute comprehensive poisoning detection statistics\"\"\"\n        try:\n            with torch.no_grad():\n                # Convert to tensors\n                X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n                \n                # Get predictions\n                outputs = detection_system.detect(X_val_tensor)\n                predictions = outputs['is_poisoning']\n                \n                # Compute metrics\n                return {\n                    'detection_accuracy': np.mean(predictions.argmax(axis=1) == y_val),\n                    'detection_confidence': np.mean(np.max(predictions, axis=1)),\n                    'false_positive_rate': np.mean((predictions.argmax(axis=1) == 1) & (y_val == 0)),\n                    'false_negative_rate': np.mean((predictions.argmax(axis=1) == 0) & (y_val == 1))\n                }\n        except Exception as e:\n            print(f\"Error computing poisoning stats: {str(e)}\")\n            return {}\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compregensive Training Pipeline","metadata":{}},{"cell_type":"code","source":"## COMPREHENSIVE TRAINING PIPELINE\n\nclass ComprehensiveTrainingPipeline:\n    \"\"\"Advanced training pipeline with monitoring and evaluation\"\"\"\n    def __init__(self, detection_system: EnhancedPoisoningDetectionSystem,\n                 data_generator: MultiDatasetGenerator,\n                 val_generator: MultiDatasetGenerator,\n                 config: ModelConfig,\n                 label_handler: LabelHandler):\n        # Core components (keep existing)\n        self.detection_system = detection_system\n        self.data_generator = data_generator\n        self.val_generator = val_generator\n        self.config = config\n        self.label_handler = label_handler\n\n        # Training parameters (keep existing)\n        self.batch_size = config.batch_size\n        self.num_epochs = config.num_epochs\n        self.checkpoint_dir = config.checkpoint_dir\n        self.log_dir = config.log_dir\n\n        # Initialize trackers (keep existing)\n        self._global_step = 0\n        self._epoch = 0\n        self.best_metrics = {\n            'accuracy': 0,\n            'f1_score': 0,\n            'unknown_detection_rate': 0,\n            'per_attack_f1': defaultdict(float)\n        }\n\n        # Setup components (new method)\n        self._setup_components()\n\n    def preprocess_batch(self, states, binary_labels, multi_labels):\n        \"\"\"Preprocess batch data with stability checks\"\"\"\n        try:\n            # Get data stabilizer from detection system\n            data_stabilizer = self.detection_system.data_stabilizer\n\n            # Stabilize states\n            states = data_stabilizer.stabilize_array(states)\n\n            # Convert to tensors with proper dtype\n            states_tensor = torch.FloatTensor(states).to(self.detection_system.device)\n            binary_labels_tensor = torch.LongTensor(binary_labels).to(self.detection_system.device)\n\n            # Gradient scaling for large values\n            if states_tensor.abs().max() > 1e3:\n                states_tensor = F.normalize(states_tensor, dim=1)\n\n            return states_tensor, binary_labels_tensor\n\n        except Exception as e:\n            print(f\"Error in batch preprocessing: {str(e)}\")\n            print(f\"States shape: {states.shape if isinstance(states, np.ndarray) else 'invalid'}\")\n            print(f\"Labels shape: {binary_labels.shape if isinstance(binary_labels, np.ndarray) else 'invalid'}\")\n            raise\n\n\n\n    def _setup_components(self):\n        \"\"\"Initialize training components while preserving existing functionality\"\"\"\n        try:\n            # Create directories\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n            os.makedirs(self.log_dir, exist_ok=True)\n\n            # Initialize monitoring (preserve existing)\n            self.writer = SummaryWriter(self.log_dir)\n            self.memory_monitor = MemoryMonitor()\n            self.early_stopping = EarlyStopping(\n                patience=self.config.patience,\n                min_delta=self.config.min_delta\n            )\n\n            # Initialize attack-specific monitoring\n            self.attack_metrics = {\n                attack_id: defaultdict(list)\n                for attack_id in self.label_handler.attack_types\n            }\n\n            # Performance tracking (integrates with existing metrics)\n            self.train_losses = []\n            self.val_losses = []\n            self.performance_history = defaultdict(list)\n            self.progress_monitor = ProgressMonitor(self.log_dir)\n\n            # Loss function (preserve existing criterion)\n            self.criterion = PoisoningLoss()\n\n            print(f\"\\nPipeline initialized:\")\n            print(f\"- Batch size: {self.batch_size}\")\n            print(f\"- Epochs: {self.num_epochs}\")\n            print(f\"- Checkpoints: {self.checkpoint_dir}\")\n            print(f\"- Logs: {self.log_dir}\")\n            print(f\"- Monitoring {len(self.label_handler.attack_types)} attack types\")\n\n        except Exception as e:\n            print(f\"Error setting up training components: {str(e)}\")\n            raise\n            \n    \n    def train(self):\n        \"\"\"Execute training loop with safe TPU handling and enhanced data processing\"\"\"\n        try:\n            # Get device from detection system\n            device = self.detection_system.device\n            print(f\"\\nStarting training on device: {device}\")\n\n            # Safe TPU check and loader creation\n            use_tpu = is_tpu_device(device)\n            if use_tpu:\n                try:\n                    import torch_xla.distributed.parallel_loader as pl\n                    loader = pl.ParallelLoader(self.data_generator, [device]).per_device_loader(device)\n                except Exception as e:\n                    print(f\"Warning: TPU parallel loading failed, using default loader: {str(e)}\")\n                    loader = self.data_generator\n            else:\n                loader = self.data_generator\n\n            for epoch in range(self.num_epochs):\n                print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n                epoch_metrics = defaultdict(list)\n\n                # Training loop with progress tracking\n                with tqdm(total=len(loader), desc=\"Training\") as pbar:\n                    self.detection_system.model.train()\n\n                    for batch_idx, (states, binary_labels, multi_labels) in enumerate(loader):\n                        try:\n                            # Convert numpy arrays to tensors if needed\n                            if isinstance(states, np.ndarray):\n                                states = torch.from_numpy(states).float()\n                            if isinstance(binary_labels, np.ndarray):\n                                binary_labels = torch.from_numpy(binary_labels).long()\n\n                            # Move data to device in single operation\n                            states = states.to(device, non_blocking=True)\n                            binary_labels = binary_labels.to(device, non_blocking=True)\n\n                            # Apply data augmentation if configured\n                            if hasattr(self.detection_system, 'data_augmentation') and random.random() < 0.3:\n                                states = self.detection_system.data_augmentation.augment(states)\n\n                            # Forward pass with automatic mixed precision if enabled\n                            with torch.cuda.amp.autocast() if self.config.use_amp else contextlib.nullcontext():\n                                # Forward pass\n                                self.detection_system.optimizer.zero_grad()\n                                outputs = self.detection_system.model(states)\n\n                                # Prepare DQN targets\n                                q_values = outputs['q_values']\n                                action_probs = outputs['action_probs']\n                                state_value = outputs['state_value']\n\n                                # Compute targets for both DQN and Actor-Critic\n                                targets = {\n                                    'q_targets': q_values.clone().detach(),\n                                    'actions': binary_labels,\n                                    'returns': state_value.clone().detach(),\n                                    'labels': binary_labels\n                                }\n\n                                # Compute combined loss\n                                loss_dict = self.criterion(outputs, targets)\n                                total_loss = loss_dict['total_loss']\n\n                                # Check for loss validity\n                                if torch.isnan(total_loss) or torch.isinf(total_loss):\n                                    print(f\"Warning: Invalid loss value detected in batch {batch_idx}\")\n                                    continue\n\n                            # Backward pass with gradient scaling if using AMP\n                            if self.config.use_amp and hasattr(self.detection_system, 'scaler'):\n                                self.detection_system.scaler.scale(total_loss).backward()\n                                self.detection_system.scaler.step(self.detection_system.optimizer)\n                                self.detection_system.scaler.update()\n                            else:\n                                total_loss.backward()\n                                # Clip gradients for stability\n                                torch.nn.utils.clip_grad_norm_(\n                                    self.detection_system.model.parameters(), \n                                    self.config.gradient_clip\n                                )\n\n                                # Optimizer step with TPU/GPU handling\n                                if use_tpu:\n                                    xm.optimizer_step(self.detection_system.optimizer)\n                                    xm.mark_step()\n                                else:\n                                    self.detection_system.optimizer.step()\n\n                            # Update metrics\n                            metrics = {\n                                'loss': total_loss.item(),\n                                'dqn_loss': loss_dict['dqn_loss'],\n                                'policy_loss': loss_dict['policy_loss'],\n                                'value_loss': loss_dict['value_loss'],\n                                'detection_loss': loss_dict['detection_loss'],\n                                'q_value_mean': q_values.mean().item(),\n                                'q_value_std': q_values.std().item(),\n                                'policy_entropy': -(action_probs * torch.log(action_probs + 1e-10)).sum(1).mean().item()\n                            }\n\n                            # Update epoch metrics\n                            for k, v in metrics.items():\n                                epoch_metrics[k].append(v)\n\n                            # Update detection metrics\n                            with torch.no_grad():\n                                predictions = outputs['final_output'].argmax(dim=1)\n                                accuracy = (predictions == binary_labels).float().mean().item()\n                                epoch_metrics['accuracy'].append(accuracy)\n\n                            # Update progress bar\n                            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n                            pbar.set_postfix(avg_metrics)\n                            pbar.update(1)\n\n                            # Memory management\n                            if self.memory_monitor.check_memory():\n                                print(\"\\nHigh memory usage detected\")\n                                if torch.cuda.is_available():\n                                    torch.cuda.empty_cache()\n                                break\n\n                        except Exception as e:\n                            print(f\"Error in batch {batch_idx}: {str(e)}\")\n                            continue\n\n                    # Epoch end processing\n                    avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n                    print(f\"\\nEpoch {epoch + 1} Summary:\")\n                    for k, v in avg_metrics.items():\n                        print(f\"- {k}: {v:.4f}\")\n\n                    # Validation check\n                    if self.val_generator and (epoch + 1) % 5 == 0:\n                        val_metrics = self._evaluate(epoch)\n\n                        # Update best metrics\n                        self._update_best_metrics(val_metrics, epoch)\n\n                        # Early stopping check\n                        if self.early_stopping(val_metrics.get('accuracy', 0), self.detection_system.model):\n                            print(\"\\nEarly stopping triggered\")\n                            break\n\n                    # Save checkpoint if needed\n                    if (epoch + 1) % 10 == 0:\n                        self._save_checkpoint(f'epoch_{epoch + 1}')\n\n                    # Cleanup\n                    if use_tpu:\n                        xm.mark_step()\n                    else:\n                        torch.cuda.empty_cache()\n\n            print(\"\\nTraining completed successfully\")\n            return True\n\n        except Exception as e:\n            print(f\"Training error: {str(e)}\")\n            traceback.print_exc()\n            return False\n\n\n    \n    def _train_batch(self, states, binary_labels, multi_labels):\n        \"\"\"Optimized training batch for TPU\"\"\"\n        try:\n            # Move data to TPU and preprocess in a single step\n            states_tensor = xm.mesh_reduce('states', states, lambda x: torch.FloatTensor(x).to(xm.xla_device()))\n            binary_labels_tensor = xm.mesh_reduce('labels', binary_labels, lambda x: torch.LongTensor(x).to(xm.xla_device()))\n\n            # Forward pass optimization\n            with torch.autocast(device_type=self.device.type):\n                detection_output = self.detection_system.detect(states_tensor, labels=binary_labels_tensor)\n\n                # Batch processing for replay buffer\n                batch_experiences = list(zip(\n                    states, \n                    binary_labels,\n                    multi_labels,\n                    detection_output['is_poisoning'],\n                    states,\n                    [True] * len(states)\n                ))\n\n                # Extend replay buffer efficiently\n                self.detection_system.replay_buffer.extend(batch_experiences)\n\n                # Train if enough samples\n                if len(self.detection_system.replay_buffer) >= self.batch_size:\n                    batch_metrics = self.detection_system.train(self.batch_size)\n                    xm.mark_step()  # Explicit TPU sync point\n                    return batch_metrics\n\n            return None\n\n        except Exception as e:\n            print(f\"Error in batch training: {str(e)}\")\n            return None\n    \n    \n\n    def _update_best_metrics(self, val_metrics: Dict[str, float], epoch: int):\n        \"\"\"Update best metrics if current results are better\"\"\"\n        try:\n            for metric_name, value in val_metrics.items():\n                if isinstance(value, (int, float)):\n                    if metric_name not in self.best_metrics or value > self.best_metrics[metric_name]:\n                        self.best_metrics[metric_name] = value\n                        print(f\"New best {metric_name}: {value:.4f}\")\n        except Exception as e:\n            print(f\"Warning: Error updating best metrics: {str(e)}\")\n\n    def _log_metrics(self, metrics: Dict[str, float]):\n        \"\"\"Log metrics to tensorboard and update history\n\n        Args:\n            metrics: Dictionary containing metric names and values\n        \"\"\"\n        try:\n            # Log to tensorboard\n            for name, value in metrics.items():\n                self.writer.add_scalar(\n                    f'metrics/{name}',\n                    value,\n                    self._global_step\n                )\n\n                # Update history\n                self.performance_history[name].append(value)\n\n            # Update global step\n            self._global_step += 1\n\n        except Exception as e:\n            print(f\"Warning: Error logging metrics: {str(e)}\")\n\n    def _log_attack_metrics(self, attack_id: int, predictions: np.ndarray, labels: np.ndarray):\n        \"\"\"Log metrics for specific attack type\"\"\"\n        try:\n            attack_info = self.label_handler.get_attack_info(attack_id)\n            # Convert predictions to class indices if needed\n            if len(predictions.shape) > 1:\n                pred_indices = predictions.argmax(axis=1)\n            else:\n                pred_indices = predictions\n\n            metrics = {\n                'precision': precision_score(labels, pred_indices, average='binary'),\n                'recall': recall_score(labels, pred_indices, average='binary'),\n                'f1': f1_score(labels, pred_indices, average='binary'),\n                'accuracy': accuracy_score(labels, pred_indices)\n            }\n\n            # Log to tensorboard\n            for name, value in metrics.items():\n                self.writer.add_scalar(\n                    f'attack_metrics/{attack_info[\"attack_name\"]}/{name}',\n                    value,\n                    self._global_step\n                )\n\n                # Store in attack metrics\n                self.attack_metrics[attack_id][name].append(value)\n\n        except Exception as e:\n            print(f\"Warning: Error logging attack metrics: {str(e)}\")\n\n    def _log_batch_metrics(self, batch_metrics: Dict[str, float]):\n        \"\"\"Log batch-level training metrics\"\"\"\n        try:\n            # Add batch metrics to history\n            for name, value in batch_metrics.items():\n                if name == 'total_loss':\n                    self.train_losses.append(value)\n                self.writer.add_scalar(f'batch/{name}', value, self._global_step)\n\n        except Exception as e:\n            print(f\"Warning: Error logging batch metrics: {str(e)}\")\n\n    def _log_epoch_metrics(self, epoch: int, metrics: Dict[str, float]):\n        \"\"\"Log epoch-level metrics\"\"\"\n        try:\n            # Compute epoch averages\n            epoch_metrics = {}\n            for name, values in metrics.items():\n                if values:  # Check if list is not empty\n                    epoch_metrics[f'epoch_{name}'] = np.mean(values)\n\n            # Log to tensorboard\n            for name, value in epoch_metrics.items():\n                self.writer.add_scalar(f'epoch/{name}', value, epoch)\n\n            return epoch_metrics\n\n        except Exception as e:\n            print(f\"Warning: Error logging epoch metrics: {str(e)}\")\n            return {}\n    \n    \n    def _evaluate(self, epoch: int) -> Dict[str, float]:\n        \"\"\"Enhanced evaluation with comprehensive metrics and proper device handling\"\"\"\n        self.detection_system.model.eval()\n        device = self.detection_system.device\n        all_metrics = {}\n        per_attack_metrics = defaultdict(list)\n\n        try:\n            with torch.no_grad():\n                # Get validation data with proper handling\n                eval_data, eval_binary_labels, eval_multi_labels = next(iter(self.val_generator))\n\n                # Convert numpy arrays to tensors if needed\n                if isinstance(eval_data, np.ndarray):\n                    eval_data = torch.from_numpy(eval_data).float()\n                if isinstance(eval_binary_labels, np.ndarray):\n                    eval_binary_labels = torch.from_numpy(eval_binary_labels).long()\n                if isinstance(eval_multi_labels, np.ndarray):\n                    eval_multi_labels = torch.from_numpy(eval_multi_labels).long()\n\n                # Move to device\n                eval_data = eval_data.to(device)\n                eval_binary_labels = eval_binary_labels.to(device)\n                eval_multi_labels = eval_multi_labels.to(device)\n\n                # Model prediction\n                outputs = self.detection_system.model(eval_data)\n                predictions = torch.argmax(outputs['final_output'], dim=1)\n\n                # Calculate basic metrics\n                accuracy = (predictions == eval_binary_labels).float().mean().item()\n                predictions_np = predictions.cpu().numpy()\n                labels_np = eval_binary_labels.cpu().numpy()\n\n                metrics = {\n                    'accuracy': accuracy,\n                    'precision': precision_score(labels_np, predictions_np, average='binary'),\n                    'recall': recall_score(labels_np, predictions_np, average='binary'),\n                    'f1': f1_score(labels_np, predictions_np, average='binary'),\n                    'avg_q_value': outputs['q_values'].mean().item(),\n                    'policy_entropy': -(outputs['action_probs'] * \n                                      torch.log(outputs['action_probs'] + 1e-10)).sum(1).mean().item()\n                }\n\n                # Per-attack metrics\n                if self.label_handler:\n                    for attack_id in self.label_handler.attack_types:\n                        attack_mask = eval_multi_labels == attack_id\n                        if torch.any(attack_mask):\n                            attack_preds = predictions[attack_mask]\n                            attack_labels = eval_binary_labels[attack_mask]\n\n                            attack_accuracy = (attack_preds == attack_labels).float().mean().item()\n                            attack_name = self.label_handler.get_attack_info(attack_id)['attack_name']\n                            per_attack_metrics[attack_name] = {\n                                'accuracy': attack_accuracy,\n                                'count': int(attack_mask.sum().item())\n                            }\n\n                # Uncertainty metrics\n                detection_probs = F.softmax(outputs['final_output'], dim=1)\n                entropy = -(detection_probs * torch.log(detection_probs + 1e-10)).sum(dim=1)\n                metrics['prediction_entropy'] = entropy.mean().item()\n                metrics['detection_confidence'] = detection_probs.max(dim=1)[0].mean().item()\n\n                # Combine all metrics\n                all_metrics.update(metrics)\n                all_metrics['per_attack'] = dict(per_attack_metrics)\n\n                # Log metrics\n                self._log_evaluation_metrics(all_metrics, epoch)\n\n                return all_metrics\n\n        except Exception as e:\n            print(f\"Error during evaluation: {str(e)}\")\n            traceback.print_exc()\n            return {'accuracy': 0.0, 'error': str(e)}\n\n\n    def _calculate_metrics(self, model, data, labels) -> Dict[str, float]:\n        \"\"\"Calculate metrics with TPU optimization\"\"\"\n        try:\n            device = self.detection_system.device\n\n            # Forward pass\n            outputs = model.model(data)\n            predictions = torch.argmax(outputs['final_output'], dim=1)\n\n            # Calculate metrics on device\n            accuracy = (predictions == labels).float().mean()\n\n            # Move metrics to CPU for numpy operations\n            if isinstance(device, torch_xla.xla_model.XlaDevice):\n                accuracy = xm.mesh_reduce('accuracy', accuracy, lambda x: x.item())\n                predictions = xm.mesh_reduce('predictions', predictions, lambda x: x.cpu())\n                labels = xm.mesh_reduce('labels', labels, lambda x: x.cpu())\n            else:\n                accuracy = accuracy.item()\n                predictions = predictions.cpu()\n                labels = labels.cpu()\n\n            # Calculate additional metrics\n            metrics = {\n                'accuracy': accuracy,\n                'precision': precision_score(labels, predictions, average='binary'),\n                'recall': recall_score(labels, predictions, average='binary'),\n                'f1': f1_score(labels, predictions, average='binary')\n            }\n\n            return metrics\n\n        except Exception as e:\n            print(f\"Error calculating metrics: {str(e)}\")\n            return {'accuracy': 0.0, 'error': str(e)}\n\n    def _test_unknown_attacks(self, data) -> Dict[str, float]:\n        \"\"\"Test unknown attack detection with TPU support\"\"\"\n        try:\n            device = self.detection_system.device\n\n            # Forward pass\n            outputs = self.detection_system.model(data)\n            detection_probs = F.softmax(outputs['final_output'], dim=1)\n\n            # Calculate uncertainty metrics\n            entropy = -(detection_probs * torch.log(detection_probs + 1e-10)).sum(dim=1)\n\n            if isinstance(device, torch_xla.xla_model.XlaDevice):\n                entropy = xm.mesh_reduce('entropy', entropy, lambda x: x.mean().item())\n            else:\n                entropy = entropy.mean().item()\n\n            return {\n                'unknown_detection_entropy': entropy,\n                'unknown_detection_confidence': 1.0 - entropy\n            }\n\n        except Exception as e:\n            print(f\"Error in unknown attack detection: {str(e)}\")\n            return {\n                'unknown_detection_entropy': 0.0,\n                'unknown_detection_confidence': 0.0\n            }\n\n\n\n    def _log_per_attack_metrics(self, per_attack_metrics: Dict, epoch: int):\n        \"\"\"Log per-attack metrics to tensorboard\"\"\"\n        for attack_name, metrics in per_attack_metrics.items():\n            for metric_name, value in metrics.items():\n                self.writer.add_scalar(\n                    f'per_attack/{attack_name}/{metric_name}',\n                    value,\n                    epoch\n                )\n\n    def _save_checkpoint(self, identifier: str):\n        \"\"\"Save model checkpoint with additional metrics\"\"\"\n        path = os.path.join(\n            self.checkpoint_dir,\n            f'checkpoint_{identifier}.pt'\n        )\n\n        checkpoint = {\n            'epoch': self._epoch,\n            'model_state_dict': self.detection_system.model.state_dict(),\n            'optimizer_state_dict': self.detection_system.optimizer.state_dict(),\n            'metrics': self.best_metrics,\n            'global_step': self._global_step,\n            'label_mapping': self.label_handler.label_mapping,  # Save label information\n            'attack_types': self.label_handler.attack_types\n        }\n\n        torch.save(checkpoint, path)\n        print(f\"\\nCheckpoint saved: {path}\")\n\n    def _handle_oom_error(self):\n        \"\"\"Handle out of memory error\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        self.batch_size = max(32, self.batch_size // 2)\n        print(f\"\\nReducing batch size to: {self.batch_size}\")\n\n    def _check_memory(self):\n        \"\"\"Check memory usage\"\"\"\n        if torch.cuda.is_available():\n            memory_used = torch.cuda.memory_allocated()/1e9\n            if memory_used > 0.9 * torch.cuda.get_device_properties(0).total_memory/1e9:\n                print(f\"\\nHigh GPU memory usage ({memory_used:.2f}GB)\")\n                self._handle_oom_error()\n\n    def _log_training_progress(self, epoch: int, batch_idx: int, metrics: Dict):\n        \"\"\"Log training progress\"\"\"\n        try:\n            # Log to tensorboard\n            step = epoch * len(self.data_generator) + batch_idx\n            for name, value in metrics.items():\n                if isinstance(value, (int, float, np.number)):\n                    self.writer.add_scalar(f'training/{name}', value, step)\n\n            # Update global step\n            self._global_step = step\n\n        except Exception as e:\n            print(f\"Warning: Error logging progress: {str(e)}\")\n\n    def _handle_epoch_completion(self, epoch: int, epoch_metrics: Dict):\n        \"\"\"Handle end of epoch procedures\"\"\"\n        try:\n            # Calculate average metrics\n            avg_metrics = {}\n            for k, v in epoch_metrics.items():\n                if isinstance(v, (list, np.ndarray)) and len(v) > 0:\n                    if all(isinstance(x, (int, float, np.number)) for x in v):\n                        avg_metrics[k] = float(np.mean(v))\n\n            # Print epoch summary\n            print(f\"\\nEpoch {epoch + 1} Complete:\")\n            print(\"-\" * 50)\n            print(\"Training Metrics:\")\n            for name, value in avg_metrics.items():\n                print(f\"- {name}: {value:.4f}\")\n\n            # Log to tensorboard\n            for name, value in avg_metrics.items():\n                self.writer.add_scalar(f'epoch/{name}', value, epoch)\n\n            # Validation if available\n            if self.val_generator and (epoch + 1) % 5 == 0:\n                print(\"\\nRunning Validation...\")\n                val_metrics = self._evaluate(epoch)\n                self._update_best_metrics(val_metrics, epoch)\n            \n            # Self_progress_monitor\n            self.progress_monitor.update(epoch, avg_metrics)\n            if (epoch + 1) % 5 == 0:  # Plot every 5 epochs\n                self.progress_monitor.plot_training_progress()\n            \n            # Memory management\n            if self.memory_monitor.check_memory():\n                self._handle_oom_error()\n\n            print(\"\\nCurrent Best Metrics:\")\n            for metric, value in self.best_metrics.items():\n                if isinstance(value, (int, float)):\n                    print(f\"- Best {metric}: {value:.4f}\")\n\n        except Exception as e:\n            print(f\"Warning: Error in epoch completion handling: {str(e)}\")\n            traceback.print_exc()\n\n\n    def _check_convergence(self, current_loss: float, last_loss: float,\n                          plateau_counter: int, min_loss_change: float) -> Tuple[int, bool]:\n        \"\"\"Check if training has converged\"\"\"\n        if abs(current_loss - last_loss) < min_loss_change:\n            plateau_counter += 1\n        else:\n            plateau_counter = 0\n\n        return plateau_counter, plateau_counter >= self.config.patience\n\n    def _should_stop_training(self, iteration_counter: int, max_iterations: int,\n                            avg_loss: float, plateau_counter: int) -> bool:\n        \"\"\"Check if training should stop\"\"\"\n        if iteration_counter >= max_iterations:\n            print(f\"\\nReached maximum iterations ({max_iterations})\")\n            return True\n\n        if avg_loss < 1e-4:\n            print(f\"\\nReached minimum loss threshold\")\n            return True\n\n        if plateau_counter >= self.config.patience:\n            print(f\"\\nLoss plateau reached\")\n            return True\n\n        return False\n    \n    def _log_evaluation_metrics(self, metrics: Dict, epoch: int):\n        \"\"\"Log evaluation metrics properly\"\"\"\n        for metric_name, value in metrics.items():\n            if isinstance(value, (int, float)):\n                self.writer.add_scalar(f'eval/{metric_name}', value, epoch)\n                print(f\"{metric_name}: {value:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Processor","metadata":{}},{"cell_type":"code","source":"# Initialize\nconfig = ModelConfig()  # Will automatically detect and setup TPU\n\nclass DatasetProcessor:\n    \"\"\"Handles dataset processing and model training\"\"\"\n    def __init__(self, config: ModelConfig):\n        self.config = config\n        self.all_models = {}\n        self.all_metrics = {}\n\n    def process_single_dataset(self, dataset_type: str, data_path: str):\n        \"\"\"Process a single dataset\"\"\"\n        try:\n            # Load dataset using EnhancedDatasetLoader\n            loader = EnhancedDatasetLoader(dataset_type, self.config)\n            X, binary_labels, multi_labels, additional_info = loader.load_and_process_dataset(data_path)\n            print(f\"\\nDataset loaded - Shape: {X.shape}\")\n\n            # Initialize components\n            metrics_tracker = PoisoningDetectionMetrics(loader.label_handler)\n            print(\"Created metrics tracker\")\n\n            # Create train and validation generators\n            split_idx = int(0.8 * len(X))\n            X_train, X_val = X[:split_idx], X[split_idx:]\n            binary_train, binary_val = binary_labels[:split_idx], binary_labels[split_idx:]\n            multi_train, multi_val = multi_labels[:split_idx], multi_labels[split_idx:]\n\n            train_generator = MultiDatasetGenerator(\n                {dataset_type: (X_train, binary_train, multi_train)},\n                batch_size=self.config.batch_size,\n                shuffle=True,\n                num_workers=self.config.num_workers\n            )\n            print(\"Created data generator\")\n\n            val_generator = MultiDatasetGenerator(\n                {dataset_type: (X_val, binary_val, multi_val)},\n                batch_size=self.config.batch_size,\n                shuffle=False,\n                num_workers=self.config.num_workers\n            )\n            print(\"Created validation generator\")\n\n            # Initialize detection system\n            detection_system = EnhancedPoisoningDetectionSystem(\n                input_dim=X.shape[1],\n                config=self.config,\n                label_handler=loader.label_handler\n            )\n            print(\"Initialized detection system\")\n\n            # Setup training pipeline\n            pipeline = ComprehensiveTrainingPipeline(\n                detection_system=detection_system,\n                data_generator=train_generator,\n                val_generator=val_generator,\n                config=self.config,\n                label_handler=loader.label_handler\n            )\n\n            # Train\n            print(\"\\nStarting training...\")\n            pipeline.train()\n\n            # Save additional info from enhanced processing\n            self._save_additional_info(dataset_type, additional_info)\n\n            return detection_system, metrics_tracker\n\n        except Exception as e:\n            print(f\"Error processing dataset: {str(e)}\")\n            traceback.print_exc()\n            return None, None\n\n    def _save_additional_info(self, dataset_type: str, additional_info: Dict):\n        \"\"\"Save additional processing information\"\"\"\n        save_path = os.path.join(\n            self.config.checkpoint_dir,\n            f'{dataset_type}_additional_info.pt'\n        )\n        torch.save(additional_info, save_path)\n        print(f\"\\nSaved additional processing info to {save_path}\")\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1FFc5L-_OodHnSswXN2I15mAnpzO7YIiV"},"id":"042xtxM5vfni","outputId":"3c52be10-dbf6-4726-f7dd-23d6a1860261","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results Evaluation","metadata":{}},{"cell_type":"code","source":"class ResultsAnalyzer:\n    \"\"\"Analyze and format detection results per dataset\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'cic': defaultdict(dict),\n            'ton': defaultdict(dict),\n            'cse': defaultdict(dict)\n        }\n        \n    def compute_per_dataset_metrics(self, dataset_type: str, predictions: np.ndarray, \n                                  true_labels: np.ndarray, attack_types: Dict[int, str]):\n        \"\"\"Compute comprehensive metrics for each attack type\"\"\"\n        results = {}\n        \n        # Per-attack metrics\n        for attack_id, attack_name in attack_types.items():\n            mask = true_labels == attack_id\n            if np.any(mask):\n                results[attack_name] = {\n                    'accuracy': accuracy_score(true_labels[mask], predictions[mask]),\n                    'precision': precision_score(true_labels[mask], predictions[mask], average='weighted'),\n                    'recall': recall_score(true_labels[mask], predictions[mask], average='weighted'),\n                    'f1': f1_score(true_labels[mask], predictions[mask], average='weighted'),\n                    'samples': int(np.sum(mask))\n                }\n        \n        # Overall metrics\n        results['overall'] = {\n            'accuracy': accuracy_score(true_labels, predictions),\n            'macro_precision': precision_score(true_labels, predictions, average='macro'),\n            'macro_recall': recall_score(true_labels, predictions, average='macro'),\n            'macro_f1': f1_score(true_labels, predictions, average='macro'),\n            'weighted_f1': f1_score(true_labels, predictions, average='weighted')\n        }\n        \n        self.metrics[dataset_type] = results\n        return results\n\n    def generate_results_table(self) -> pd.DataFrame:\n        \"\"\"Generate tabular results for all datasets\"\"\"\n        all_results = []\n        \n        for dataset in ['cic', 'ton', 'cse']:\n            dataset_metrics = self.metrics[dataset]\n            \n            for attack, metrics in dataset_metrics.items():\n                if attack != 'overall':\n                    row = {\n                        'Dataset': dataset.upper(),\n                        'Attack Type': attack,\n                        'Accuracy': metrics.get('accuracy', 0) * 100,\n                        'Precision': metrics.get('precision', 0) * 100,\n                        'Recall': metrics.get('recall', 0) * 100,\n                        'F1-Score': metrics.get('f1', 0) * 100,\n                        'Samples': metrics.get('samples', 0)\n                    }\n                    all_results.append(row)\n                    \n        return pd.DataFrame(all_results)\n\n    def print_summary(self):\n        \"\"\"Print comprehensive summary of results\"\"\"\n        print(\"\\nDetection Results Summary\")\n        print(\"=\" * 80)\n        \n        for dataset in ['cic', 'ton', 'cse']:\n            print(f\"\\n{dataset.upper()} Dataset Results:\")\n            print(\"-\" * 40)\n            \n            if dataset in self.metrics and 'overall' in self.metrics[dataset]:\n                overall = self.metrics[dataset]['overall']\n                print(f\"Overall Accuracy: {overall['accuracy']*100:.2f}%\")\n                print(f\"Macro F1-Score: {overall['macro_f1']*100:.2f}%\")\n                print(f\"Weighted F1-Score: {overall['weighted_f1']*100:.2f}%\")\n                \n                print(\"\\nPer-Attack Performance:\")\n                for attack, metrics in self.metrics[dataset].items():\n                    if attack != 'overall':\n                        print(f\"\\n{attack}:\")\n                        print(f\"- Accuracy: {metrics['accuracy']*100:.2f}%\")\n                        print(f\"- F1-Score: {metrics['f1']*100:.2f}%\")\n                        print(f\"- Samples: {metrics['samples']}\")\n            else:\n                print(\"No results available\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Progress Monitor","metadata":{}},{"cell_type":"code","source":"class ProgressMonitor:\n    def __init__(self, save_dir='visualizations'):\n        self.metrics_history = defaultdict(list)\n        self.save_dir = save_dir\n        os.makedirs(save_dir, exist_ok=True)\n        \n    def update(self, epoch, metrics):\n        for k, v in metrics.items():\n            self.metrics_history[k].append(v)\n            \n    def plot_training_progress(self):\n        plt.figure(figsize=(15, 5))\n        for metric in ['accuracy', 'loss', 'f1']:\n            if metric in self.metrics_history:\n                plt.plot(self.metrics_history[metric], label=metric)\n        plt.legend()\n        plt.title('Training Progress')\n        plt.savefig(f'{self.save_dir}/training_progress.png')\n        plt.close()\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results Visualization","metadata":{}},{"cell_type":"code","source":"class ResultsVisualizer:\n    \"\"\"Visualization tools for detection results\"\"\"\n    def __init__(self, save_dir='visualizations'):\n        self.save_dir = save_dir\n        os.makedirs(save_dir, exist_ok=True)\n        # Use a built-in matplotlib style instead of 'seaborn'\n        plt.style.use('seaborn-v0_8-darkgrid')  # or simply use 'default'\n        \n        # Set default plot parameters\n        plt.rcParams['figure.figsize'] = (12, 8)\n        plt.rcParams['axes.grid'] = True\n        plt.rcParams['font.size'] = 10\n        \n    def plot_confusion_matrices(self, results, dataset_type):\n        \"\"\"Plot confusion matrices for each attack type\"\"\"\n        try:\n            plt.figure(figsize=(15, 5))\n            if dataset_type in results and 'metrics' in results[dataset_type]:\n                metrics = results[dataset_type]['metrics']\n                if hasattr(metrics, 'confusion_matrix'):\n                    sns.heatmap(metrics.confusion_matrix, annot=True, fmt='d',\n                               xticklabels=['Normal', 'Attack'],\n                               yticklabels=['Normal', 'Attack'],\n                               cmap='Blues')  # Specify colormap explicitly\n                    plt.title(f'{dataset_type.upper()} Confusion Matrix')\n                    plt.tight_layout()\n                    plt.savefig(f'{self.save_dir}/{dataset_type}_confusion_matrix.png')\n                    plt.close()\n        except Exception as e:\n            print(f\"Error plotting confusion matrix: {str(e)}\")\n        \n    def plot_roc_curves(self, results, dataset_type):\n        \"\"\"Plot ROC curves for each dataset\"\"\"\n        try:\n            plt.figure(figsize=(10, 6))\n            if dataset_type in results and 'metrics' in results[dataset_type]:\n                metrics = results[dataset_type]['metrics']\n                if hasattr(metrics, 'fpr') and hasattr(metrics, 'tpr'):\n                    plt.plot(metrics.fpr, metrics.tpr, \n                            label=f'{dataset_type} (AUC = {metrics.roc_auc:.2f})')\n                    plt.plot([0, 1], [0, 1], 'k--')  # Add diagonal line\n                    plt.xlabel('False Positive Rate')\n                    plt.ylabel('True Positive Rate')\n                    plt.title(f'{dataset_type.upper()} ROC Curve')\n                    plt.legend()\n                    plt.grid(True)\n                    plt.savefig(f'{self.save_dir}/{dataset_type}_roc_curve.png')\n                    plt.close()\n        except Exception as e:\n            print(f\"Error plotting ROC curve: {str(e)}\")\n\n    def plot_attack_distribution(self, results, dataset_type):\n        \"\"\"Plot distribution of attacks for each dataset\"\"\"\n        try:\n            if dataset_type in results and 'metrics' in results[dataset_type]:\n                metrics = results[dataset_type]['metrics']\n                if hasattr(metrics, 'true_labels'):\n                    plt.figure(figsize=(15, 8))\n                    unique_labels, counts = np.unique(metrics.true_labels, return_counts=True)\n                    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n                    plt.bar(unique_labels, counts, color=colors)\n                    plt.title(f'Attack Distribution - {dataset_type.upper()}')\n                    plt.xlabel('Attack Types')\n                    plt.ylabel('Number of Samples')\n                    plt.xticks(rotation=45)\n                    plt.grid(True, axis='y')\n                    plt.tight_layout()\n                    plt.savefig(f'{self.save_dir}/{dataset_type}_attack_distribution.png')\n                    plt.close()\n        except Exception as e:\n            print(f\"Error plotting attack distribution: {str(e)}\")\n\n    def plot_performance_comparison(self, results):\n        \"\"\"Plot performance comparison across datasets\"\"\"\n        try:\n            plt.figure(figsize=(12, 6))\n            metrics_data = []\n            for dataset_type, result in results.items():\n                if 'metrics' in result:\n                    metrics = result['metrics']\n                    if hasattr(metrics, 'accuracy'):\n                        metrics_data.append({\n                            'Dataset': dataset_type.upper(),\n                            'Accuracy': metrics.accuracy,\n                            'Precision': metrics.precision,\n                            'Recall': metrics.recall,\n                            'F1-Score': metrics.f1,\n                            'Detection Rate': metrics.detection_rate,\n                            'False Alarm Rate': metrics.false_alarm_rate\n                        })\n            \n            if metrics_data:\n                df = pd.DataFrame(metrics_data)\n                ax = df.set_index('Dataset').plot(kind='bar', \n                                                width=0.8,\n                                                colormap='viridis')\n                plt.title('Performance Metrics Comparison')\n                plt.ylabel('Score')\n                plt.xlabel('Dataset')\n                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n                plt.grid(True, axis='y')\n                plt.tight_layout()\n                plt.savefig(f'{self.save_dir}/performance_comparison.png',\n                          bbox_inches='tight')\n                plt.close()\n        except Exception as e:\n            print(f\"Error plotting performance comparison: {str(e)}\")\n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Execution 1","metadata":{}},{"cell_type":"code","source":"def tpu_training_wrapper(func):\n    \"\"\"Wrapper to handle TPU execution modes with proper device management\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            if xm.xrt_world_size() > 1:\n                # Multi-core TPU execution\n                def _mp_fn(index, *args):\n                    device = xm.xla_device()\n                    results = func(*args)\n                    xm.mark_step()\n                    return results\n                \n                return xmp.spawn(_mp_fn, args=args, nprocs=8)\n            else:\n                # Single core execution\n                device = xm.xla_device()\n                results = func(*args)\n                xm.mark_step()\n                return results\n        except Exception as e:\n            print(f\"Error in TPU execution: {str(e)}\")\n            traceback.print_exc()\n            return None\n    return wrapper\n\n# Add TPU device check helper at the top\ndef is_tpu_device(device) -> bool:\n    \"\"\"Safely check if device is TPU with enhanced error handling\"\"\"\n    try:\n        import torch_xla\n        import torch_xla.core.xla_model as xm\n        if isinstance(device, str):\n            return device.startswith('xla')\n        return hasattr(device, 'type') and device.type.startswith('xla')\n    except (ImportError, AttributeError) as e:\n        print(f\"TPU check failed: {str(e)}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error in TPU check: {str(e)}\")\n        return False\n\ndef main_single_dataset():\n    \"\"\"Main function to process datasets one at a time with TPU support\"\"\"\n    try:\n        # Setup with TPU awareness and metrics\n        import torch_xla.core.xla_model as xm\n        import torch_xla.debug.metrics as met\n        \n        device = xm.xla_device()  # Get TPU device directly\n        config = ModelConfig()\n        \n        if not setup_system():\n            raise RuntimeError(\"System setup failed\")\n\n        # Dataset configurations\n        datasets = {\n            'cic': {'path': '/kaggle/input/poisoning-i/CIC_IoT_M3.csv', 'description': 'CIC-IDS Dataset'},\n            'ton': {'path': '/kaggle/input/poisoning-i/UNSW_TON_IoT.csv', 'description': 'TON-IoT Dataset'},\n            'cse': {'path': '/kaggle/input/poisoning-i/CSE-CIC_2018.csv', 'description': 'CSE-CIC Dataset'}\n        }\n\n        # Initialize analyzers and visualizers\n        results_analyzer = ResultsAnalyzer()\n        attack_mapper = AttackTypeMapper()\n        enhanced_metrics = EnhancedMetrics()\n        visualizer = ResultsVisualizer()\n        \n        # Process one dataset at a time with TPU handling\n        results = {}\n        for dataset_type, info in datasets.items():\n            print(f\"\\n{'='*50}\")\n            print(f\"Processing {dataset_type.upper()} Dataset\")\n            print(f\"{'='*50}\")\n            \n            try:\n                trainer = SingleDatasetTrainer(config, dataset_type)\n                model, metrics = trainer.train_on_dataset(info['path'])\n                \n                results[dataset_type] = {\n                    'model': model,\n                    'metrics': metrics\n                }\n\n                # Generate metrics and visualizations for this dataset\n                attack_mapping = AttackTypeMapper.get_mappings()[dataset_type]\n                \n                # Compute comprehensive metrics\n                per_attack_results = enhanced_metrics.per_attack_metrics(\n                    metrics.true_labels,\n                    metrics.predictions,\n                    attack_mapping\n                )\n                \n                # Generate visualizations\n                visualizer.plot_confusion_matrices(results, dataset_type)\n                visualizer.plot_roc_curves(results, dataset_type)\n                visualizer.plot_attack_distribution(results, dataset_type)\n                \n                # Compute per-dataset metrics\n                dataset_results = results_analyzer.compute_per_dataset_metrics(\n                    dataset_type,\n                    metrics.predictions,\n                    metrics.true_labels,\n                    trainer.loader.label_handler.attack_types\n                )\n                \n                print(f\"\\nCompleted processing {dataset_type.upper()}\")\n                print(\"Cleaning up memory...\")\n                gc.collect()\n                xm.mark_step()  # Ensure TPU operations are completed\n                \n            except Exception as e:\n                print(f\"Error processing {dataset_type} dataset: {str(e)}\")\n                traceback.print_exc()\n                continue\n\n        # Generate and display overall results\n        results_table = results_analyzer.generate_results_table()\n        print(\"\\nDetailed Results Table:\")\n        print(results_table.to_string())\n        results_analyzer.print_summary()\n        \n        # Generate overall performance comparison\n        visualizer.plot_performance_comparison(results)\n\n        return results\n\n    except Exception as e:\n        print(f\"Error in main execution: {str(e)}\")\n        traceback.print_exc()\n        return None\n    \n    # Generate and display results\n    results_table = results_analyzer.generate_results_table()\n    print(\"\\nDetailed Results Table:\")\n    print(results_table.to_string())\n    results_analyzer.print_summary() \n    \n    attack_mapper = AttackTypeMapper()\n    enhanced_metrics = EnhancedMetrics()\n    visualizer = ResultsVisualizer()\n    \n    # In main_single_dataset(), after processing results:\n    for dataset_type, result in results.items():\n        attack_mapping = AttackTypeMapper.get_mappings()[dataset_type]\n        per_attack_results = enhanced_metrics.per_attack_metrics(\n            result['metrics'].true_labels,\n            result['metrics'].predictions,\n            attack_mapping\n        )\n        visualizer.plot_attack_distribution(results, dataset_type)\n    \n\nif __name__ == \"__main__\":\n    try:\n        # First ensure TPU is available\n        print(\"Checking TPU availability...\")\n        import torch_xla\n        import torch_xla.core.xla_model as xm\n        import torch_xla.debug.metrics as met\n        \n        if torch_xla is None:\n            raise ImportError(\"torch_xla not available\")\n        \n        print(f\"TPU devices found: {xm.xrt_world_size()}\")\n        \n        # Run with TPU support\n        results = main_single_dataset()\n        \n        # Print training summary\n        if results:\n            print(\"\\nTraining Summary:\")\n            for dataset_type, result in results.items():\n                print(f\"\\n{dataset_type.upper()} Results:\")\n                if result['metrics']:\n                    for metric_name, value in result['metrics'].items():\n                        if isinstance(value, (int, float)):\n                            print(f\"- {metric_name}: {value:.4f}\")\n        \n        # Print TPU metrics safely\n        try:\n            if xm.xrt_world_size() > 0:\n                print(\"\\nTPU Metrics:\")\n                print(met.metrics_report())\n        except Exception as e:\n            print(f\"Note: Could not generate TPU metrics report: {str(e)}\")\n            \n    except Exception as e:\n        print(f\"Failed to initialize training: {str(e)}\")\n        traceback.print_exc()\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n","metadata":{"id":"ONoygyUIceec"}}]}